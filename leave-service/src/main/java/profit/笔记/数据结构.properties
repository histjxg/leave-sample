02|如何抓住重点，系统高效地学习数据结构与算法?
    数据结构：
        广义：
            指一组数据的存储结构。
            例子：
                1。图书馆储藏书籍为了方便查找，图书管理员一般会将书籍分门别类进行“存储”
                2。按照一定规律编号，就是书籍这种“数据”的存储结构。
        狭义：
            队列、栈、堆等
        目的：
            如何让代码更省存储空间
    算法：
        广义：
            操作数据的一组方法
            例子：
                查找一本书，
                    方法一：
                        可以一本一本地找
                    方法二：
                        也可以先根据书籍类别的编号，是人文，还是科学、计算机，来定位书架，然后再依次查找
        狭义：
            二分查找、动态规划等
        目的：
            如何让代码运行得更快
    关系：
        概念：
            1。数据结构和算法是相辅相成的。数据结构是为算法服务的
            2。算法要作用在特定的数据结构之上。
            3。数据结构是静态的，它只是组织数据的一种方式。如果不在它的基础上操作、构建算法，孤立存在的数据结构就是没用的。
        例子一：
            数组具有随机访问的特点，常用的二分查找算法需要用数组来存储数据
        例子二：
            链表这种数据结构，二分查找算法就无法工作了，因为链表并不支持随机访问
    学习方法：
        来历--》自身的特点---》适合解决的问题---》实际的应用场景
        类似：背景-》特点--》工作能力-》工作经验

03|复杂度分析(上):如何分析、统计算法的执行效率和资源消耗?
    背景：
        1。算法和数据结构本身解决的是“省”和“快”的问题，即如何让代码运行得更快，如何让代码更省存储空间
        2。执行效率是算法一个非常重要的考量指标
    问题：
        如何衡量算法代码的执行效率？
    方法：
        时间、空间复杂度分析呢
            问题：
                1。我把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。
                2。为什么还要做时间、空间复杂度分析呢?
            原因：
                1。测试结果非常依赖测试环境
                    例子：
                        同样一段代码，在不同的处理器上处理的结果是不一样的
                2。测试结果受数据规模的影响很大
                    例子：
                        同一个排序算法，待排序数据的有序度不一样，排序的执行时间就会有很大的差别
    大O复杂度表示法：
        公式：
            T(n)=O(f(n))
                解释：
                    1。T(n)，它表示代码执行的时间
                    2。n表示数据规模的大小
                    3。f(n)表示每行代码执行的次数总和。
                        原因：
                        这是一个公式，所以用f(n)来表示
                    4。公式中的O，表示代码的执行时间T(n)与f(n)表达式成正比
                例子一：
                    T(n) = O(2n+2)
                例子二：
                    T(n) = O(2n2+2n+3)
                注意：
                    1。当n很大时，你可以把它想象成10000、100000。
                    2。公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略。
                    例子一：T(n) = O(n); 例子二：T(n) = O(n2)
        概念：
            1。并不具体表示代码真正的执行时间
            2。表示代码执行时间随数据规模增长的变化趋势
            3。也叫作渐进时间复杂度(asymptotic time complexity)，简称时间复杂度。
        时间复杂度分析
            方法一：
                只关注循环执行次数最多的一段代码
                参考：
                    profit.jikeshijian.shujujiegou.TimecomplexityMostNum03.cal01
            方法二：
                加法法则:总复杂度等于量级最大的那段代码的复杂度
                参考：
                    profit.jikeshijian.shujujiegou.TimecomplexityMostNum03.cal02
            方法三：
                乘法法则:嵌套代码的复杂度等于嵌套内外代码复杂度的乘积
                参考：
                    profit.jikeshijian.shujujiegou.TimecomplexityMostNum03.cal03
            几种常见的多项式时间复杂度。
                1。O(1)
                    情形一：
                        只要代码的执行时间不随n的增大而增长，这样代码的时间复杂度我们都记作O(1)
                    情形二：
                        一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码
                    例子：
                        int i = 8;
                        int j = 6;
                        int sum = i + j;
                2。O(logn)、O(nlogn)
                    O(logn)例子：
                        i=1;
                        while (i <= n)  {
                            i=i*2
                        }
                        分析：
                            2^x=n
                        所以
                            x=log2n，这段代码的 时间复杂度就是O(log2n)。
                        注意：
                            实际上，不管是以2为底、以3为底，还是以10为底，我们可以把所有对数阶的时间复杂度都记为O(logn)。
                        原因：
                            O(log3n) = O(C * log2n)
                            在对数阶时间复杂度的表示方法里，我们忽略对数的“底”， 统一表示为O(logn)。
                    O(nlogn)例子：
                        如果一段代码的时间复杂度是O(logn)，我们循环执行n遍，时间复 杂度就是O(nlogn)了
                3。O(m+n)、O(m*n)
                    例子：
                        参考：profit.jikeshijian.shujujiegou.TimecomplexityMostNum03.cal04
    空间复杂度分析：
        概念：
            渐进空间复杂度(asymptotic space complexity)，表示算法的存储空间与数据规模之间的增长关系。
        例子：
            参考：
                profit.jikeshijian.shujujiegou.TimecomplexityMostNum03.print
        几种常见的多项式空间复杂度。
            1。O(1)
            2。O(n)
            3。O(n^2)
            4。O(logn)
            5.O(nlogn)

04|复杂度分析(下):浅析最好、最坏、平均、均摊时间复杂度
    1。最好、最坏情况时间复杂度
        例子：
            参考：
                profit.jikeshijian.shujujiegou.TimecomplexityMostNum03.find01
                profit.jikeshijian.shujujiegou.TimecomplexityMostNum03.find02
        最好是时间复杂度：
            概念：
                在最理想的情况下，执行这段代码的时间复杂度
            例子：
                在最理想的情况下，要查找的变量x正好是数组的第一个元素
        最坏情况时间复杂度
            概念：
                在最糟糕的情况下，执行这段代码的时间复杂度
            例子：
                刚举的那个例子，如果数组中没有要查找的变量x，我们需要把整个数组都遍历一遍才行

    2。平均情况时间复杂度
        例子：
            要查找的变量x在数组中的位置，有n+1种情况:在数组的0~n-1位置中和不在数组中
        分析
            1。把每种情况下，查找需要遍历的元素个数累加起来
            2。然后再除以n+1，就可以得到需要遍历的元素个数的平均值
                (1+2+3+···+n+n)/(n+1)=(n*(n+3))/(2*(n+1))
            3.要查找的变量x，要么在数组里，要么就不在数组里
            4.这两种情况对应的概率统计起来很麻烦，为了方便你理解
            5.我们假设在数组中与不在数组中的概率都为1/2
            6.要查找的数据出现在0~n-1这n个位置的概率也是一样的，为1/n
            7.根据概率乘法法则，要查找的数据出现在0~n-1中任意位置的概率就 是1/(2n)。
        平均复杂度计算公式为：
            1*1/(2n)+2*1/(2n)+3*1/(2n)+····+n*1/(2n)+n*1/n=(3n+1)/4
        推论：
            1。这个值就是概率论中的加权平均值，也叫作期望值
            2。平均时间复杂度的全称应该叫加权平均时间复杂度或者期望时间复杂度。
            3。加权平均值为(3n+1)/4。用大O表示法来表示，去掉系数和常量，这段代码的加权平均时间复杂度仍然是O(n)。
        扩展：
            1。在大多数情况下，我们并不需要区分最好、最坏、平均情况时间复杂度三种情况。
            2。只有同一块代码在不同的情况下，时间复杂度有量级的差距，我们才会使用这三种复杂度表示法来区分。

    3。均摊时间复杂度
        概念：
            通过摊还分析得到的时间复杂度
        例子：
            参考：
                profit.jikeshijian.shujujiegou.TimecomplexityMostNum03.insert
        场景：
            1。均摊时间复杂度和摊还分析应用场景比较特殊，所以我们并不会经常用到
            2。对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低
            3。只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，
            4。我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上

05|数组:为什么很多编程语言中数组都从0开始编号?
    数组：
        概念：
            1。是一种线性表数据结构
            2。它用一组连续的内存空间，来存储一组具有相同类型的数据。
        关键词：
            1。线性表
                概念：
                    数据排成像一条线一样的结构
                特点：
                    数据最多只有前和后两个方向
                例子：
                    数组，链表、队列，栈
            2。随机访问
                原因：
                    连续的内存空间和相同类型的数据
                问题：
                    数组是如何实现根据下标随机访问数组元素的吗?
                回答：
                    首先通过下面的寻址公式，计算出该元素存储的内存地址:
                        a[i]_address = base_address + i * data_type_size
                        说明：
                            1。计算机给数组a[10]，分配了一块连续内存空间1000~1039，
                            2。内存块的 首地址为base_address = 1000。
                            3。其中data_type_size表示数组中每个元素的大
        缺点：
            在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。
            问题：
                为什么会导致低效?
            分析：
                插入操作
                    1。设数组的长度为n，现在，如果我们需要将一个数据插入到数组中的第k个位置。
                    2。为了把第k个位置腾出来，给新来的数据，我们需要将第k~n这部分的元素都顺序地往后挪一位。
                        情形一：
                            如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为O(1)
                        情形二：
                            如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是O(n)
                            平均情况时间复杂度为(1+2+...n)/n=O(n)。
                            原因：
                                我们在每个位置插入元素的概率是一样的
                        情形三：
                            如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移k之后的数据
                        情形四：
                            1。如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合
                            2。数组只是被当作一个存储数据的集合
                            3。在这种情况下，如果要将某个数组插入到第k个位置，为了避免大规模的数据搬移
                            4。直接将第k位的数据搬移到数组元素的最后，把新的元素直接放入第k个位置。
                        优点：
                            利用这种技巧，在特定场景下，在第K个位置插入一个元素的时间复杂度为O(1)
                删除操作：
                    情形一：
                        我们要删除第k个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。
                    情形二：
                        如果删除数组末尾的数据，则最好情况时间复杂度为O(1);
                    情形三：
                        如果删除开头的数据，则最坏情况时间复杂度为O(n);平均情况时间复杂度也为O(n)。
                    情形四：
                        如果我们将多次删除操作集中在一起执行
                        问题：
                            删除的效率是不是会提高很多呢?
                        案例：
                            数组a[10]中存储了8个元素:a，b，c，d，e，f，g，h。现在，我们要依次删除a，b，c三个元素。
                            分析：
                                1。为了避免d，e，f，g，h这几个数据会被搬移三次，我们可以先记录下已经删除的数据。
                                2。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除
                                3。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作
                            优点：
                                大大减少了删除操作导致的数据搬移
                            应用场景：
                                JVM的标记清除垃圾回收算法的核心思想

    警惕数组的访问越界问题
        int[] a = new int[3];
        a[3] =10;
        把数组越界检查的工作丢给程序员来做，像Java本身就会做越界检查。会抛出ArrayIndexOutOfBoundsException异常；

    问题：
        容器能否完全替代数组?
    容器：
        例子：
            1。ArrayList最大的优势就是可以将很多数组操作的细节封装起来
            如：
                比如前面提到的数组插入、删除数据时需要搬移其他数据等
        优点：
            支持扩容
                数组缺点：
                    1。数组本身在定义的时候需要预先指定大小，因为需要分配连续的内存空间
                    2。如果我们申请了大小为10的数组，当第11个数据需要存储到数组中时
                    3。我们就需要重新分配一块更大的空间，将原来的数据复制过去，然后再将新的数据插入。
                容器优点：
                    1。如果使用ArrayList，我们就完全不需要关心底层的扩容逻辑，ArrayList已经帮我们实现好了
                    2。每次存储空间不够的时候，它都会将空间自动扩容为1.5倍大小。
                容器缺点：
                    扩容操作涉及内存申请和数据搬移，是比较耗时的
                    方法：
                        如果事先能确定需要存储的数据大小，最好在创建ArrayList的时候事先指定数据大小。
    扩展：
        1。Java ArrayList无法存储基本类型，比如int、long，需要封装为Integer、Long类
            而Autoboxing、Unboxing则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。
        2。如果数据大小事先已知，并且对数据的操作非常简单，用不到ArrayList提供的大部分方法，也可以直接使用数组。
        3。当要表示多维数组时，用数组往往会更加直观。比如Object[][] array;而用容器的话则需要这样定义:ArrayList<ArrayList > array。

06|链表(上):如何实现LRU缓存淘汰算法?
    缓存：
        优点：
            是一种提高数据读取性能的技术
        应用：
            CPU缓存、数据库缓存、浏览器缓存等等。
            问题：
                缓存的大小有限，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留?
            策略：
                1。先进先出策略FIFO(First In，First Out)
                2。最少使用策略LFU(Least Frequently Used)
                3。最近最少使用策略LRU(Least Recently Used)
    数组：
        特点：
            数组需要一块连续的内存空间来存储，对内存的要求比较高
        案例：
            如果我们申请一个100MB大小的数组
            分析：
                1。当内存中没有连续的、足够大的存储空间时
                2。即便内存的剩余总可用空间大于100MB，仍然会申请失败。

    链表：
        概念：
            1.不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用。
            2.我们把内存块称为链表的“结点”
            3.为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址
        例子：
            申请的是100MB大小的链表，根本不会有问题。
        分类：
            1。单链表:
                概念：
                    1.其中有两个结点是比较特殊的，它们分别是第一个结点和最后一个结点。
                    2.我们习惯性地把第一个结点叫作头结点，把最后一个结点叫作尾结点。头结点用来记录链表的基地址。
                    3.有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是:指针不是指向下一个结点
                    4.而是指向一个空地址NULL，表示这是链表上最后一个结点。
                删除或更新：
                    插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，在链表中插入和删除一个数据是非常快速的。
                    原因：
                        链表的存储空间本身就不是连续的。
                查询：
                    1。链表要想随机访问第K个元素，就没有数组那么高效了。
                    2。无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址
                    3。而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。
                    原因：
                        链表中的数据并非连续存储的，所以无法像数组那样
            2。双向链表:
                概念：
                    支持两个方向，每个结点不止有一个后继指针next指向后面的结点，还有一个前驱指针prev指向前面的结点。
                缺点：
                    如果存储同样多的数据，双向链表要比单链表占用更多的内存空间
                删除：
                    情形一：
                        1。不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比
                        2。直到找到值等于给定值的结点，然后再通过我前面讲的指针操作将其删除。
                        分析：
                            1。单纯的删除操作时间复杂度是O(1)
                            2。遍历查找的时间是主要的耗时点，对应的时间复杂度为O(n)
                            3。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为O(n)。
                    情形二：
                        1。我们已经找到了要删除的结点，但是删除某个结点q需要知道其前驱结点
                        2。而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点
                        3。我们还是要从头结点开始遍历链表，直到p->next=q，说明p是q的前驱结点。
                        优点：
                            单链表删除操作需要O(n)的时间复杂度，而双向链表只需要在O(1)的时间复杂度内就搞定了!
                插入：
                    情形一：
                        1。如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势
                        优点：
                            2。双向链表可以在O(1)时间复杂度搞定，而单向链表需要O(n)的时间复杂度。
                查询：
                    情形一：
                        对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些
                    原因：
                        1。我们可以记录上次查找的位置p，每次查询时
                        2。根据要查找的值与p的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。
                应用：
                    LinkedHashMap这个容器。如果你深入研究LinkedHashMap的实现原理，就会发现其中就用到了双向链表这种数据结构。
                扩展：
                    情形一：
                        1。当内存空间充足的时候，如果我们更加追求代码的执行速度
                        2。我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构
                    情形二：
                        1。如果内存比较紧缺，比如代码跑在手机或者单片机上
                        2。这个时候，就要反过来用时间换空间的设计思路。

            3。循环链表:
                概念：
                    双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。
                特点：
                    1。它跟单链表唯一的区别就在尾结点
                    2。循环链表的尾结点指针是指向链表的头结点。
                    3。它像一个环一样首尾相连，所以叫作“循环”链表。
                优点：
                    当要处理的数据具有环型结构特点时，就特别适合采用循环链表
            扩展：
                1。对于执行较慢的程序，可以通过消耗更多的内存(空间换时间)来进行优化
                2。而消耗过多内存的程序，可以通过消耗更多的时间(时间换空间)来降低内存的消耗

    链表VS数组性能大比拼
        思路：
            1。数组和链表的对比，并不能局限于时间复杂度。
            2。在实际的软件开发中，不能仅仅利用复杂度分析就决定使用那个数据结构来存储数据
        数组：
            特点：
                简单易用，在实现上使用的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，所以访问效率更高
            缺点：
                大小固定，一经声明就要占用整块连续内存空间
                情形一：
                    如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足(out of memory)”
                情形二：
                    如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时

        链表：
            在内存中并不是连续存储，所以对cpu缓存不友好，没办法有效预读
            优点：
                链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。
            问题
                Java中的ArrayList容器，也可以支持动态扩容啊?
                回答：
                    1。当我们往支持动态扩容的数组中插入一个数据时，如果数组中没有空闲空间了
                    2。就会申请一个更大的空间，将数据拷贝过去，而数据拷贝的操作是非常耗时的
                案例：
                    1。如果我们用ArrayList存储了了1GB大小的数据，这个时候已经没有空闲空间了，当我们再插入数据的时候，
                    2。ArrayList会申请一个1.5GB大小的存储空间，并且把原来那1GB的数据拷贝到新申请的空间上
        扩展：
            如果你的代码对内存的使用非常苛刻，那数组就更适合你
            原因：
                1。链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍
                2。对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片
                3。如果是Java语言，就有可能会导致频繁的GC(Garbage Collection，垃圾回收)。

07|链表(下):如何轻松写出正确的链表代码?
    技巧一:理解指针或引用的含义
        概念：
            1。将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针
            2。指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。
        例子一：
            p->next=q：p结点中的next指针存储了q结点的内存地址。
        例子二：
            p->next=p->next->next。这行代码表示，p结点的next指针存储了p结点的下下一个结点的内存地址。
    技巧二:警惕指针丢失和内存泄漏
        例子一：
            结点a和相邻的的结点b之间插入结点x，假设当前指针p指向结点a
            p->next = x; // 将p的next指针指向x结点;
            x->next = p->next; // 将x的结点的next指针指向b结点;
            分析：
                1。p->next指针在完成第一步操作之后，已经不再指向结点b了，而是指向结点x。
                2。第2行代码相当于将x赋值给x->next，自己指向自己
            现象：
                因此，整个链表也就断成了两半，从结点b往后的所有结点都无法访问到了。
            方法：
                顺序换一下
    技巧三:利用哨兵简化实现难度
        插入：
            情形一：
                在结点p后面插入一个新的结点
                new_node->next = p->next;
                p->next = new_node;
            情形二：
                插入第一个结点
                    if (head == null){
                        head = n;
                    }
        删除：
            情形一：
                删除结点p的后继结点
                p->next = p->next->next;
            情形二：
                如果我们要删除链表中的最后一个结点
                if (head-next == null){
                    head = null;
                }
        分析：
            针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理
            缺点：
                代码实现起来就会很繁琐，不简洁，而且也容易因为考虑不全而出错
            思考：
                1。还记得如何表示一个空链表吗?head=null表示链表中没有结点了
                2。其中head表示头结点指针，指向链表中的第一个结点。
            方法：
                1。在任何时候，不管链表是不是空，head指针都会一直指向这个哨兵结点
                    1。带头链表：有哨兵结点的链表
                    2。不带头链表：没有哨兵结点的链表
        扩展：
            哨兵简化编程难度的技巧，在很多代码实现中都有用到
            应用：
                插入排序、归并排序、动态规划等
            例子一：
                参考：
                    profit.jikeshijian.shujujiegou.TimecomplexityMostNum03.findShaobing01
            例子二：
                profit.jikeshijian.shujujiegou.TimecomplexityMostNum03.findShaobing02
            问题：
                在字符串a很长的时候，比如几万、几十万，你觉得哪段代码运行得更快点呢?
                答案：
                    例子二
                    原因：
                        1。两段代码中执行次数最多就是while循环那一部分。
                        2。第二段代码中，我们通过一个哨兵a[n-1] = key，成功省掉了一个比较语句i<n，不要小看这一条语句
                        3。当累积执行万次、几十万次时，累积的时间就很明显了。
            注意：
                例子二只是为了举例说明哨兵的作用，下面的代码可读性太差了。

    技巧四:重点留意边界条件处理
        背景：
            1。代码在一些边界或者异常情况下，最容易产生Bug
            2。要实现没有Bug的链表代码，一定要在编写的过程中以及编写完成之后
            3。检查边界条件是否考虑全面，以及代码在边界条件下是否能正确运行。
        检查链表代码是否正确的边界条件
            1。如果链表为空时，代码是否能正常工作?
            2。如果链表只包含一个结点时，代码是否能正常工作?
            3。如果链表只包含两个结点时，代码是否能正常工作?
            4。代码逻辑在处理头结点和尾结点的时候，是否能正常工作?

    技巧五:举例画图，辅助思考
        例子：
            1。往单链表中插入一个数据这样一个操作
            2。我一般都是把各种情况都举一个例子，画出插入前和插入后的链表变化
        优点：
            看图写代码，比较简单

    技巧六:多写多练，没有捷径
        1。单链表反转
        2。链表中环的检测
        3。两个有序的链表合并
        4。删除链表倒数第n个结点
        5。求链表的中间结点

08|栈:如何实现浏览器的前进和后退功能?
    案例：
        1。当你依次访问完一串页面a-b-c之后，点击浏览器的后退按钮，就可以查看之前浏览过的页面b和a
        2。当你后退到页面a，点击前进按钮，就可以重新查看页面b和c
        3。但是，如果你后退到页面b后，点击了新的页面d，那就无法再通过前进、后退功能查看页面c了。
        问题：
            如何实现这个功能；
        方法：
            1。使用两个栈，X和Y，我们把首次浏览的页面依次压入栈X，当点击后退按钮时，再依次从栈X中出栈，并将出栈的数据依次放入栈Y
            2。当我们点击前进按钮时，我们依次从栈Y中取出数据，放入栈X中
            3。当栈X中没有数据时，那就说明没有页面可以继续后退浏览了。
            4。当栈Y中没有数据，那就说明没有页面可以点击前进按钮浏览了。
    栈：
        特点：
            1。后进者先出，先进者后出
            2。一种操作受限的线性表，只允许在一端插入和删除数据
        类似：
            1。一摞叠在一起的盘子。我们平时放盘子的时候，都是从下往上一个一个放;
            2。取的时候，我们也是从上往下一个一个地依次取，不能从中间任意抽出
        思考：
            相比数组和链表，栈带给我的只有限制，并没有任何优势
            问题：
                那我直接使用数组或者链表不就好了吗?为什么还要用这个“操作受限”的“栈”呢?
                回答：
                    1。从功能上来说，数组或链表确实可以替代栈，但你要知道，特定的数据结构是对特定场景的抽象
                    2。数组或链表暴露了太多的操作接口，操作上的确灵活自由，但使用时就比较不可控，自然也就更容易出错。
        顺序栈：用数组实现的栈
            参考：profit.jikeshijian.shujujiegou.stack.ArrayStack08
            分析：
                1。基于数组实现的栈，是一个固定大小的栈，在初始化栈时需要事先指定栈的大小
                2。当栈满之后，就无法再往栈里添加数据了
                3。尽管链式栈的大小不受限，但要存储next指针，内存消耗相对较多。
                问题：
                    如何基于数组实现一个可以支持动态扩容的栈呢?，
                    参考：
                        1。动态扩容的数组，当数组空间不够时，我们就重新申请一块更大的内存
                        2。将原来数组中数据统统拷贝过去。
                方法：
                    1。底层依赖一个支持动态扩容的数组就可以啦
                    2。当栈满了之后，我们就申请一个更大的数组，将原来的数据搬移到新数组中
            出栈：
                不会涉及内存的重新申请和数据的搬移，所以出栈的时间复杂度仍然是O(1)
            入栈：
                情形一：
                    当栈中有空闲空间时，入栈操作的时间复杂度为O(1)。
                情形二：
                    当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了O(n)。
            扩展：
                对于入栈操作来说，最好情况时间复杂度是O(1)，最坏情况时间复杂度是O(n)。
                平均时间复杂度：
                    方法：摊还分析法，有在个别时刻才会退化为O(n)，所以把耗时多的入栈操作的时间均摊到其他入栈操作上，平均情况下的耗时就接近O(1)。
        链式栈：用链表实现的栈
        应用：
            1。栈在函数调用中的应用
                函数掉用栈：
                    概念：
                        1。操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构,用来存储函数调用时的临时变量。
                        2。每进入一个函数，就会将临时变量作为一个栈帧入栈
                        3。当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈
                    例子：
                        参考：
                            profit.jikeshijian.shujujiegou.stack.FunctionStack08.main
            2。栈在表达式求值中的应用
                案例：
                    34+13*9+44-12/3
                原理：
                    1。编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。
                    2。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈;
                    3。当遇到运算符，就与运算符栈的栈顶元素进行比较。
                        1。如果比运算符栈顶元素的优先级高，就将当前运算符压入栈
                        2。如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符
                            1。从操作数栈的栈顶取2个操作数
                            2。然后进行计算，再把计算完的结果压入操作数栈，继续比较。
            3。栈在括号匹配中的应用
                背景：
                    1。我们假设表达式中只包含三种括号，圆括号()、方括号[]和花括号{}，并且它们可以任意嵌套。
                    2。比如，{[{}]}或[{()}([])]等都为合法格式，而{[}()]或[({)]为不合法的格式。
                问题：
                    我现在给你一个包含三种括号的表达式字符串，如何检查它是否合法呢?
                方法：
                    栈来解决
                流程：
                    1。我们用栈来保存未匹配的左括号，从左到右依次扫描字符串
                    2。当扫描到左括号时，则将其压入栈中;当扫描到右括号时，从栈顶取出一个左括号。
                    3。如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串
                    4。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。

09|队列:队列在线程池等有限资源池中的应用
    背景：
        栈只支持两个基本操作:入栈push()和出栈pop()。
    队列：
        概念：
            1。先进者先出
            2。支持在队尾插入元素，在队头删除元素
        类似：
            想象成排队买票，先来的先买，后来的人只能站末尾，不允许插队
        操作：
            入队enqueue()，放一个数据到 队列尾部;
            出队dequeue()，从队列头部取一个元素。
        顺序队列：
            概念：
                用数组实现的队列叫作顺序队列，
            参考：
                profit.jikeshijian.shujujiegou.queue.ArrayQueue09
            缺点：
                1。随着不停地进行入队、出队操作，head和tail都会持续往后移动。
                2。当tail移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据了
            问题：
                如何解决呢？
                思路：
                    1。在数组那一节，我们也遇到过类似的问题，就是数组的删除操作会导致数组中的数据不连续
                    2。用数据搬移!但是，每次进行出队操作都相当于删除数组下标为0的数据
                    3。要搬移整个队列中的数据，这样出队操作的时间复杂度就会从原来的O(1)变为O(n)
                方法：
                    参考：
                    profit.jikeshijian.shujujiegou.queue.ArrayQueue09.enqueue02
            应用：
                1。基于数组实现的有界队列(bounded queue)，队列的大小有限，所以线程池中排队的请求超过队列大小时
                2。接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。
            注意：
                1。设置一个合理的队列大小，也是非常有讲究的
                2。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能。
        链式队列：
            概念：
                用链表实现的队列叫作链式队列
            数据结构：
                需要两个指针:head指针和tail指针。它们分别指向链表的第一个结点和最后一个结点
            代码实现：
                profit.jikeshijian.shujujiegou.queue.QueueBasedOnLinkedList
            应用：
                1。可以实现一个支持无限排队的无界队列(unbounded queue)
                2。针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的。
            缺点：
                可能会导致过多的请求排队等待，请求处理的响应时间过长
        循环队列
            背景：
                用数组来实现队列的时候，在tail==n时，会有数据搬移操作
            缺点：
                入队操作性能就会受到影响
            问题：
                有没有办法能够避免数据搬移呢?
            概念：
                把首尾相连，扳成了一个环。
            最关键：
                确定好队空和队满的判定条件
                    1。队空判定条件：head == tail
                    2。队满判定条件：(tail+1)%n=head
            代码实现：
                参考：profit.jikeshijian.shujujiegou.queue.CircularQueue09
        阻塞队列：
            概念：
                1。在队列基础上增加了阻塞操作
                2。就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回
                3。如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。
        并发队列：
            背景：
                在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题
            问题：
                如何实现一个线程安全的队列呢?
            方法：
                并发队列
                    概念：
                        线程安全的队列我们叫作并发队列
                    实现思路：
                        直接的实现方式是直接在enqueue()、dequeue()方法上加锁
                        缺点：
                            锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作
                        优化:
                            基于数组的循环队列，利用CAS原子操作，可以实现非常高效的并发队列
    问题：
        线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理?
        处理策略：
            策略一：
                是非阻塞的处理方式，直接拒绝任务请求
            策略二：
                是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。
                问题：
                    那如何存储排队的请求呢?
                方法：
                    希望公平地处理每个排队的请求，先进者先服务，所以队列这种数据结构很适合来存储排队请求

10|递归:如何用三行代码找到“最终推荐人”?
    递归：
        应用：
            DFS深度优先搜索、前中后序二叉树遍历等等
        思路：
            递归求解问题的分解过程
                递：去的过程
                归：回来的过程
        案例：
            去电影院，不知道自己坐第几排，可以用递归的思想？
            代码如下：
                int f(int n) {
                    if (n == 1) {return 1};
                    return f(n-1) + 1;
                }
        三个条件：
            1。一个问题的解可以分解为几个子问题的解
                子问题：
                    概念：
                        数据规模更小的问题
                    例子：
                        1。前面讲的电影院的例子，你要知道，“自己在哪一排”的问题
                        2。可以分解为“前一排的人在哪一排”这样一个子问题。
            2。这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样
                例子：
                    电影院那个例子，你求解“自己在哪一排”的思路，和前面一排人求解“自己在哪一排”的思路，是一模一样的。
            3。存在递归终止条件
                原因：
                    1。把问题分解为子问题，把子问题再分解为子子问题
                    2。一层一层分解下去，不能存在无限循环，这就需要有终止条件
                例子：
                    1。电影院的例子，第一排的人不需要再继续询问任何人
                    2。就知道自己在哪一排，也就是f(1)=1，这就是递归的终止条件。
        递归代码：
            最关键：
                1。写出递推公式
                2。找到终止条件
                3。剩下将递推公式转化为代码就很简单了。
            案例：
                假如这里有n个台阶，每次你可以跨1个台阶或者2个台阶，请问走这n个台阶有多少种走法?
                现象：
                    1。如果有7个台阶，你可以2，2，2，1这样子上去
                    2。也可以1，2，1，1，2这样子上去，总之走法有很多
                问题：
                    如何用编程求得总共有多少种走法呢?
                分析：
                    1。找到递推公式：
                        1。根据第一步的走法把所有走法分为两类，第一类是第一步走了1个台阶，另一类是第一步走了2个台阶
                        2。所以n个台阶的走法就等于先走1阶后，n-1个台阶的走法加上先走2阶后，n-2个台阶的走法
                            公式为：f(n) = f(n-1)+f(n-2)
                    2。终止条件
                        1。当有一个台阶时，我们不需要再继续递归，就只有一种走法
                        2。f(1)=1
                        问题：
                            这个递归终止条件足够吗?
                            回答：
                                可以用n=2，n=3这样比较小的数试验一下
                        3。推出：f(1)=1，f(2)=2
                    3。递推公式和终止条件放到一起：
                        f(1) = 1;
                        f(2) = 2;
                        f(n) = f(n-1)+f(n-2)
                    4。代码编写
                        int f(int n) {
                            if (n == 1) return 1; 
                            if (n == 2) return 2;
                            return  f(n-1) + f(n-2);
                        }
                         代码关键：
                            1。找到如何将大问题分解为小问题的规律，并且基于此写出递推公式
                            2。然后再推敲终止条件
                            3。最后将递推公式和终止条件翻译成代码。
        注意：
            1。对于递归代码，这种试图想清楚整个递和归过程的做法，实际上是进入了一个思维误区
            2。要原因就是自己给自己制造了这种理解障碍
            问题：
                那正确的思维方式应该是怎样的呢?
            案例：
                1。如果一个问题A可以分解为若干子问题B、C、D，你可以假设子问题B、C、D已经解决，在此基础上思考如何解决问题
                2。而且，你只需要思考问题A与子问题B、C、D两层之间的关系即可，不需要一层一层往下思考子问题与子子问题
                3。子子问题与子子子问题之间的关系。屏蔽掉递归细节，这样子理解起来就简单多了。
            方法：
                1。编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式
                2。不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤。
        问题：
            1。递归代码要警惕堆栈溢出
                案例一：
                    函数调用栈：
                        1。函数调用会使用栈来保存临时变量
                        2。每调用一个函数，都会将临时变量封装为栈帧压入内存栈，等函数执行完成返回时，才出栈
                        3。系统栈或者虚拟机栈空间一般都不大。
                        4。如果递归求解的数据规模很大，调用层次很深，一直压入栈，就会有堆栈溢出的风险。
                案例二：
                    1。电影院的例子，如果我们将系统栈或者JVM堆栈大小设置为1KB
                    2。在求解f(19999)时便会出现如下堆栈报错:
                        Exception in thread "main" java.lang.StackOverflowError
                问题：
                    如何避免出现堆栈溢出呢?
                    方法：
                        限制递归调用的最大深度的方式来解决这个问题
                    具体流程：
                        递归调用超过一定深度(比如1000)之后，我们就不继续往下再递归了，直接返回报错。
                    例子：
                        还是电影院那个例子，我们可以改造成下面这样子，就可以避免堆栈溢出了
                        int depth = 0;
                        int f(int n) {
                            ++depth;
                            if (depth > 1000) throw exception;
                            if (n == 1) return 1;
                            return f(n-1)+1;
                        }
                    注意：
                        1。因为最大允许的递归深度跟当前线程剩余的栈空间大小有关，事先无法计算
                        2。如果实时计算，代码过于复杂，就会影响代码的可读性。
                        3。如果最大深度比较小，比如10、50，就可以用这种方法，否则这种方法并不是很实用。
            2。递归代码要警惕重复计算
                案例：
                    想要计算f(5)，需要先计算f(4)和f(3)，而计算f(4)还需要计算f(3)
                    问题：
                        f(3)就被计算了很多次，这就是重复计算问题。
                思路：
                    1。为了避免重复计算，我们可以通过一个数据结构(比如散列表)来保存已经求解过的f(k)。
                    2。当递归调用到f(k)时，先看下是否已经求解过了
                        1。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。
                        2。否，正常求解
                优化代码：
                    public int f(int n) {
                        if (n == 1) return 1;
                        if (n == 2) return 2;
                    // hasSolvedList可以理解成一个Map，key是n，value是f(n)
                    if (hasSolvedList.containsKey(n)) {
                      return hasSovledList.get(n);
                      }
                    int ret = f(n-1) + f(n-2);
                    hasSovledList.put(n, ret);
                      return ret;
                    }
            3。时间效率上
                递归代码里多了很多函数调用，当这些函数调用的数量较大时，就会积聚成一个可观的时间成本
            4。空间复杂度上
                1。递归调用一次就会在内存栈中保存一次现场数据
                2。在分析递归代码空间复杂度时，需要额外考虑这部分的开销
                例子：
                    前面讲到的电影院递归代码，空间复杂度并不是O(1)，而是O(n)。
        优点：
            1。递归代码的表达力很强
            2。写起来非常简洁
        缺点：
            1。空间复杂度高、有堆栈溢出的风险
            2。存在重复计算
            3。过多的函数调用会耗时较多等问题
        怎么将递归代码改写为非递归代码?
            例子一：
                int f(int n) {
                int ret = 1;
                for (int i = 2; i <= n; ++i) {
                  ret = ret +1;
                  }
                return ret;
                }
            例子二：
                int f(int n) {
                    if (n == 1) return 1;
                    if (n == 2) return 2;
                    int ret = 0;
                    int pre = 2;
                    int prepre = 1;
                    for (int i = 3; i <= n; ++i) {
                        ret = pre + prepre;
                        prepre = pre;
                        pre = ret;
                    }
                    return ret;
                }
            问题：
                是不是所有的递归代码都可以改为这种迭代循环的非递归写法呢?
                答案：
                    是的
                    原因：
                        1。递归本身就是借助栈来实现的，只不过我们使用的栈是系统或者虚拟机本身提供的，我们没有感知罢了
                        2。如果我们自己在内存堆上实现栈，手动模拟入栈、出栈过程
                        3。这样任何递归代码都可以改写成看上去不是递归代码的样子。
        思考题：
            如何找到“最终推荐人”?
            解决方案：
                long findRootReferrerId(long actorId) {
                    Long referrerId = select referrer_id from [table] where actor_id = actorId; 
                    if (referrerId == null)return actorId;
                    return findRootReferrerId(referrerId);
                }
            分析：
                1。用三行代码就能搞定了，不过在实际项目中，上面的代码并不能工作，为什么呢?
                2。这里面有两个问题。
            问题一：
                如果递归很深，可能会有堆栈溢出的问题。
                方法：
                    用限制递归深度来解决
            问题二：
                如果数据库里存在脏数据，我们还需要处理由此产生的无限递归问题
                方法一：
                    用限制递归深度来解决
                方法二：
                    自动检测A-B-C-A这种“环”的存在。
                    问题：
                        如何来检测环的存在呢?
            例子：
                1。比如demo环境下数据库中，测试工程师为了方便测试，会人为地插入一些数据，就会出现脏数据
                2。如果A的推荐人是B，B的推荐人是C，C的推荐人是A，这样就会发生死循环。
11|排序(上):为什么插入排序比冒泡排序更受欢迎?
    按时间复杂度分类：
        com/suixingpay/profit/document/数据结构/图片/第11讲按时间复杂度算法分类.png
        问题：
            1。插入排序和冒泡排序的时间复杂度相同，都是O(n2)
            2。在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢?
    如何分析排序算法：
        1。最好情况、最坏情况、平均情况时间复杂度
            过程：
                1。在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。
                2。还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。
            原因：
                1。有些排序算法会区分，为了好对比，所以我们最好都做一下区分
                2。对于要排序的数据，有的接近有序，有的完全无序
                    有序度不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。
        2。时间复杂度的系数、常数、低阶
            概念：
                在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。
            原因：
                1。时间复杂度反应的是数据规模n很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶
                2。实际的软件开发中，我们排序的可能是10个、100个、1000个这样规模很小的数据
        3。比较次数和交换(或移动)次数
            概念：
                在分析排序算法的执行效率的时候，应该把比较次数和交换(或移动)次数也考虑进去。
    排序算法的内存消耗
        背景：
            算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外
        扩展：
            针对排序算法的空间复杂度，我们还引入了一个新的概念，原地排序(Sorted in place)
                原地排序算法：
                    特指空间复杂度是O(1)的排序算法
    排序算法的稳定性
        背景：
            1。仅仅用执行效率和内存消耗来衡量排序算法的好坏是不够的。
            2。还有一个重要的度量指标，稳定性。
        稳定性：
            概念：
                如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。
            例子：
                比如我们有一组数据2，9，3，4，8，3，按照大小排序之后就是2，3，3，4，8，9。
                现象：
                    1。这组数据里有两个3。经过某种排序算法排序之后，如果两个3的前后顺序没有改变，那我们就把这种排序算法叫作稳定的排序算法
                    2。如果前后顺序发生变化，那对应的排序算法就叫作不稳定的排序算法。
                问题：
                    两个3哪个在前，哪个在后有什么关系啊，稳不稳定又有什么关系呢?为什么要考察排序算法的稳定性呢?
                    原因：
                        1。很多数据结构和算法课程，在讲排序的时候，都是用整数来举例
                        2。但在真正软件开发中，我们要排序的往往不是单纯的整数，而是一组对象，我们需要按照对象的某个key来排序。
                    例子：
                        1。现在要给电商交易系统中的“订单”排序。订单有两个属性，一个是下单时间，另一个是订单金额
                        2。如果我们现在有10万条订单数据，我们希望按照金额从小到大对订单数据排序。
                        3。对于金额相同的订单，我们希望按照下单时间从早到晚有序
                    问题：
                        对于这样一个排序需求，我们怎么来做呢?
                    方法一：
                        1。我们先按照金额对订单数据进行排序，然后，再遍历排序之后的订单数据
                        2。对于每个金额相同的小区间再按照下单时间排序
                    方法二：
                        1。我们先按照下单时间给订单排序，注意是按照下单时间，不是金额
                        2。排序完成之后，我们用稳定排序算法，按照订单金额重新排序
                        3。两遍排序之后，我们得到的订单数据就是按照金额从小到大排序，金额相同的订单按照下单时间从早到晚排序的。
                        优点：
                            1。可以保持金额相同的两个对象，在排序之后的前后顺序不变。
                            2。第一次排序之后，所有的订单按照下单时间从早到晚有序了
                            3。在第二次排序中，我们用的是稳定的排序算法，
    冒泡排序：
        参考：
            profit.jikeshijian.shujujiegou.sorts.Sorts11.bubbleSort
        概念：
            1。只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。
            2。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复n次，就完成了n个数据的排序工作。
        参考：
            profit.jikeshijian.shujujiegou.sorts.Sorts11.insertionSort
        问题一：
            冒泡排序是原地排序算法吗?
            答案：
                冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为O(1)，是一个原地排序算法。
        问题二：
            冒泡排序是稳定的排序算法吗?
            答案：
                1。在冒泡排序中，只有交换才可以改变两个元素的前后顺序。
                2。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候
                3。我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。
        问题三：
            冒泡排序的时间复杂度是多少?
            答案：
                1。最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以最好情况时间复杂度是O(n)。
                2。最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行n次冒泡操作，所以最坏情况时间复杂度为O(n2)。
                3。平均情况下的时间复杂是多少呢?
                    背景：
                        1。对于包含n个数据的数组，这n个数据就有n!种排列方式。
                        2。不同的排列方式，冒泡排序执行的时间肯定是不同的
                    例子：
                        其中一个要进行6次冒泡，而另一个只需要4次
                    思路一：
                        如果用概率论方法定量分析平均时间复杂度，涉及的数学推理和计算就会很复杂
                    思路二：
                        通过“有序度”和“逆序度”这两个概念来进行分析。
                        有序度：
                            概念：
                                数组中具有有序关系的元素对的个数
                                有序元素对用数学表达式表示就是这样:
                                    a[i] <= a[j], 如果i < j。
                            例子一：
                                对于一个倒序排列的数组，比如6，5，4，3，2，1，有序度是0；
                            例子二：
                                对于一个有序的数组，比如1，2，3，4，5，6有序度就是n*(n-1)/2也就是15。
                        满有序度：
                            完全有序的数组的有序度
                        逆序度：
                            概念：
                                正好跟有序度相反(默认从小到大为有序)
                            逆序元素对：
                                a[i] > a[j], 如果i < j。
                            公式：
                                逆序度=满有序度-有序度
                            扩展：
                                排序的过程就是一种增加有序度，减少逆序度的过程，最后达到满有序度，就说明排序完成了。
                                例子：
                                    要排序的数组的初始状态是4，5，6，3，2，1 ，其中，有序元素对有(4，5) (4，6)(5，6)，所以有序度是3
                                    当n=6， 所以排序完成之后终态的满有序度为n*(n-1)/2=15。
                                分析：
                                    1。冒泡排序包含两个操作原子，每交换一次，有序度增加1。不管算法怎么改进，交换次数总是确定的，即为有序度。
                                    2。也就是n*(n-1)/2–初始有序度。此例中就是15–3=12，要进行12次交换操作。
                                问题
                                    对于包含n个数据的数组进行冒泡排序，平均交换次数是多少呢?
                                    情形一：
                                        最坏情况下，初始状态的有序度是0，所以要进行n*(n-1)/2次交换。
                                    情形二：
                                        最好情况下，初始状态的有序 度是n*(n-1)/2，就不需要进行交换
                                    推出：
                                        我们可以取个中间值n*(n-1)/4，来表示初始有序度既不是很高也不是很低的平均情况
                                        1。平均情况下，需要n*(n-1)/4次交换操作，比较操作肯定要比交换操作多
                                        2。而复杂度的上限是O(n2)，所以平均情况下的时间复杂度就是O(n2)。
    插入排序(Insertion Sort)
        参考：
            profit.jikeshijian.shujujiegou.sorts.Sorts11.insertionSort
        问题：
            一个有序的数组，我们往里面添加一个新的数据后，如何继续保持数据有序呢?
            方法：
                只要遍历数组，找到数据应该插入的位置将其插入即可。
            分析：
                1。这是一个动态排序的过程，即动态地往有序集合中添加数据，我们可以通过这种方法保持集合中的数据一直有序
                2。而对于一组静态数据，我们也可以借鉴上面讲的插入方法，来进行排序，于是就有了插入排序算法。
        原理：
            1。我们将数组中的数据分为两个区间，已排序区间和未排序区间。
            2。初始已排序区间只有一个元素，就是数组的第一个元素ha
            3。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序
            4。重复这个过程，直到未排序区间中元素为空，算法结束
        比较和移动：
            比较：
                当我们需要将一个数据a插入到已排序区间时，需要拿a与已排序区间的元素依次比较大小，找到合适的插入位置
                注意：
                    对于不同的查找插入点方法(从头到尾、从尾到头)，元素的比较次数是有区别的、

            移动：
                找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位，这样才能腾出位置给元素插入
                注意：
                    对于一个给定的初始序列，移动操作的次数总是固定的，就等于逆序度
                原因：
                    1。满有序度是n*(n-1)/2=15，初始序列的有序度是5，所以逆序度是10
                    2。插入排序中，数据移动的个数总和也等于10=3+3+4。
                    参考：
                        com/suixingpay/profit/document/数据结构/图片/第11讲插入排序的逆序度.png
        问题一：
            插入排序是原地排序算法吗?
            答案：
                插入排序算法的运行并不需要额外的存储空间，空间复杂度是O(1)，这是一个原地排序算法
        问题二：
            插入排序是稳定的排序算法吗?
            答案：
                1。对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面
                2。这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。
        问题三：
            插入排序的时间复杂度是多少?
            情形一：
                1。如果要排序的数据已经是有序的，我们并不需要搬移任何数据
                2。如果我们从尾到头在有序数据组里面查找插入位置，每次只需要比较一个数据就能确定插入的位置
                3。这种情况下，最好是时间复杂度为O(n)。注意，这里是从尾到头遍历已经有序的数据。
            情形二：
                1。如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据
                2。所以需要移动大量的数据，所以最坏情况时间复杂度为O(n2)。
            情形三：
                1。在数组中插入一个数据的平均时间复杂度是o(n),对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，
                2。执行n次插入操作，所以平均时间复杂度为O(n2)。



    选择排序：
        参考：
            profit.jikeshijian.shujujiegou.sorts.Sorts11.selectionSort
        原理：
            1。分已排序区间和未排序区间
            2.选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。
        问题一：
            选择排序是原地排序吗？
                答案：
                    是一种原地排序算法
        问题二：
            选择排序是稳定的排序算法吗?
            答案：
                不是。
            例子：
                1。比如5，8，5，2，9这样一组数据，使用选择排序算法来排序的话
                2。第一次找到最小元素2，与第一个5交换位置，那第一个5和中间的5顺序就变了
        问题三：
            选择排序的时间复杂度是多少？
            答案
            选择排序的最好情况时间复杂度、最坏情况和平均情况时间复杂度都为O(n2)。
    思考题：
        冒泡排序和插入排序的时间复杂度都是O(n2)，都是原地排序算法，为什么插入排序要比冒泡排序更受欢迎呢?
        背景：
            1。冒泡排序不管怎么优化，元素交换的次数是一个固定值，是原始数据的逆序度
            2。插入排序是同样的，不管怎么优化，元素移动的次数也等于原始数据的逆序度。
        原因：
            冒泡排序的数据交换要比插入排序的数据移动要复杂，冒泡排序需要3个赋值操作，而插入排序只需要1个
            冒泡排序中数据的交换操作:
                if (a[j] > a[j + 1]) {
                    int tmp = a[j];
                    a[j] = a[j + 1];
                    a[j + 1] = tmp;
                    // 此次冒泡有数据交换
                    flag = true;
                }
            插入排序的数据移动操作：
                if (a[j] > value) {
                    a[j + 1] = a[j];
                } else {
                    break;
                }
    分析：
        1。把执行一个赋值语句的时间粗略地计为单位时间(unit_time)
        2。然后分别用冒泡排序和插入排序对同一个逆序度是K的数组进行排序
        3。冒泡排序，需要K次交换操作，每次需要3个赋值语句，所以交换操作总耗时就是3*K单位时间。
        4。插入排序中数据移动操作只需要K个单位时间。

12|排序(下):如何用快排思想在O(n)内查找第K大元素?
    归并排序
        原理：
            1。使用的就是分治思想。
            2。将一个大问题分解成小的子问题来解决。小的子问题解决了，大问题也就解决了
            扩展：
                1。分治思想跟我们前面讲的递归思想很像，分治算法一般都是用递归来实现的
                2。分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突。
        代码实现
            参考：
                profit.jikeshijian.shujujiegou.sorts.MergeSort12.mergeSort
        性能分析：
            问题一：
                归并排序是稳定的排序算法吗?
            分析：
                归并排序稳不稳定关键要看merge()函数，也就是两个有序子数组合并成一个有序数组的那部分代码。
            过程：
                1。在合并的过程中，如果A[p...q]和A[q+1...r]之间有值相同的元素
                2。那我们可以像伪代码中那样，先把A[p...q]中的元素放入tmp数组
                3。保证了值相同的元素，在合并前后的先后顺序不变。所以，归并排序是一个稳定的排序算法。
            问题二：
                归并排序的时间复杂度是多少?
                思路：
                    如何求递归的时间复杂度
                    参考
                        分析过程参考第12讲文章
                归并排序的时间复杂度任何情况下都是O(nlogn)，看起来非常优秀
            问题三：
                归并排序的空间复杂度是多少?
                概念：
                    归并排序的空间复杂度是O(n)。
                    归并排序不是原地排序算法。
                原因：
                    1。归并排序的合并函数，在合并两个有序数组为一个有序数组时，需要借助额外的存储空间。
                注意：
                    1。递归代码的空间复杂度并不能像时间复杂度那样累加。
                    2。尽管每次合并操作都需要申请额外的内存空间，但在合 并完成之后，临时开辟的内存空间就被释放掉了
                    3。在任意时刻，CPU只会有一个函数在执行，也就只会有一个临时的内存空间在使用。
                    4。临时内存空间最大也不会超过n个数据的大小，所以空间复杂度是O(n)。
    快速排序
        概念：
            快排利用的也是分治思想
        参考：
            profit.jikeshijian.shujujiegou.sorts.QuickSort12.quickSort
        快速排序不是一个稳定排序
            例子：
                1。分区的过程涉及交换操作，如果数组中有两个相同的元素，比如序列6，8，7，6，3，5，9，4
                2。在经过第一次分区操作之后，两个6的相对先后顺序就会改变
        问题：
            快排和归并用的都是分治思想，递推公式和递归代码也非常相似，那它们的区别在哪里呢?
            答案：
                1。归并排序的处理过程是由下到上的，先处理子问题，然后再合并。虽然是稳定的，但不是原地排序算法
                2。快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题。非稳定，但是是原地排序算法
        性能分析：
            时间复杂度：
                T(n)在大部分情况下的时间复杂度都可以做到O(nlogn)，只有在极端情况下，才会退化到O(n2)
            分析参考：
                第12讲

13|线性排序:如何根据年龄给100万用户数据排序?
    桶排序(Bucket sort)
        原理：
            1。将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序
            2。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。
        时间复杂度：
            流程：
                1。如果要排序的数据有n个，我们把它们均匀地划分到m个桶内，每个桶里就有k=n/m个元素
                2。每个桶内部使用快速排序，时间复杂度为O(k * logk)
                3。m个桶排序的时间复杂度就是O(m * k * logk)，因为k=n/m，所以整个桶排序的时间复杂度就是O(n*log(n/m))
            假设：
                当桶的个数m接近数据个数n时，log(n/m)就是一个非常小的常量，这 个时候桶排序的时间复杂度接近O(n)。
        问题：
            它是不是可以替代我们之前讲的排序算法呢?
            答案：
                不能
                原因：
                    桶排序对要排序数据的要求是非常苛刻的。
                条件一：
                    1。要排序的数据需要很容易就能划分成m个桶，并且，桶与桶之间有着天然的大小顺序
                    2。每个桶内的数据都排序完之后，桶与桶之间的数据不需要再进行排序。
                条件二：
                    数据在各个桶之间的分布是比较均匀的
                        原因：
                            1。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了
                            2。在极端情况下，如果数据都被划分到一个桶里，那就退化为O(nlogn)的排序算法了。

        应用：
            适合用在外部排序中
            外部排序：
                数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。
                例子：
                    1。我们有10GB的订单数据，我们希望按订单金额(假设金额都是正整数)进行排序，但是我们的内存有限
                    2。只有几百MB，没办法一次性把10GB的数据都加载到内存中
                    问题：
                        怎么办？
                    思路：
                        1。先扫描一遍文件，看订单金额所处的数据范围。
                        2。假设经过扫描之后我们得到，订单金额最小是1元，最大是10万元
                        3。我们将所有订单根据金额划分到100个桶里，第一个桶我们存储金额在1元到1000元之内的订单
                        4。第二桶存储金额在1001元到2000元之内的订单，以此类推
                        5。每一个桶对应一个文件，并且按照金额范围的大小顺序编号命名(00，01，02...99)。
                    理想情况：
                        1。如果订单金额在1到10万之间均匀分布，那订单会被均匀划分到100个文件中
                        2。每个小文件中存储大约100MB的订单数据，我们就可以将这100个小文件依次放到内存中，用快排来排序。
                        3。等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据
                        4。并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了。
                    一般情况：
                        1。订单按照金额在1元到10万元之间并不一定是均匀分布的
                        2。10GB订单数据是无法均匀地被划分到100个文件中的
                        3。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大，没法一次性读入内存
                        问题：
                            又该怎么办呢?
                            思路：
                                针对这些划分之后还是比较大的文件，我们可以继续划分
                                例子：
                                    1。订单金额在1元到1000元之间的比较多，我们就将这个区间继续划分为10个小区间
                                    2。1元到100元，101元到200元，201元到300元...901元到1000元。
                                    3。如果划分之后，101元到200元之间的订单还是太多，无法一次性读入内存
                                    4。那就继续再划分，直到所有的文件都能读入内存为止。
        参考代码：
            profit.jikeshijian.shujujiegou.sorts.BucketSort13
    计数排序(Counting sort)
        原理：
            1。当要排序的n个数据，所处的范围并不大的时候，比如最大值是k，我们就可以把数据划分成k个桶。
            2。每个桶内的数据值都是相同的，省掉了桶内排序的时间。
        案例：
            1。我们都经历过高考，高考查分数系统你还记得吗?
            2。我们查分数的时候，系统会显示我们的成绩以及所在省的排名
            3。如果你所在的省有50万考生，如何通过成绩快速排序得出名次呢?
        思考：
            1。考生的满分是900分，最小是0分，这个数据的范围很小，所以我们可以分成901个桶，对应分数从0分到900分。
            2。根据考生的成绩，我们将这50万考生划分到这901个桶里。桶内的数据都是分数相同的考生，所以并不需要再进行排序
            3。我们只需要依次扫描每个桶，将桶内的考生依次输出到一个数组中，就实现了50万考生的排序
        为什么这个排序算法叫“计数”排序呢?“计数”的含义来自哪里呢?
            例子：
                1。假设只有8个考生，分数在0到5分之间。这8个考生的成绩我们放在一个数组A[8]中
                2。它们分别是:2，5，3，0，2，3，0，3。
            分析：
                1。考生的成绩从0到5分，我们使用大小为6的数组C[6]表示桶，其中下标对应分数
                2。不过，C[6]内存储的并不是考生，而是对应的考生个数
                图解参考第13讲
            参考：
                profit.jikeshijian.shujujiegou.sorts.CountingSort13.countingSort
            思路：
                1。我们从后到前依次扫描数组A。比如，当扫描到3时，我们可以从数组C中取出下标为3的值7
                2。到目前为止，包括自己在内，分数小于等于3的考生有7个，也就是说3是数组R中的第7个元素(也就是数组R中下标为6的位置)
                3。当3放入到数组R中后，小于等于3的元素就只剩下了6个了，
                4。所以相应的C[3]要减1，变成6。
                5。以此类推，当我们扫描到第2个分数为3的考生的时候，就会把它放入数组R中的第6个元素的位置(也就是下标为5的位置)
                6。当我们扫描完整个数组A后，数组R内的数据就是按照分数从小到大有序排列的了。
        应用：
            1。数据范围不大的场景中，如果数据范围k比要排序的数据n大很多，就不适合用计数排序了。
            2。计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。
            例子一：
                如果考生成绩精确到小数后一位，我们就需要将所有的分数都先乘以10，转化成整数，然后再放到9010个桶内
            例子二：
                如果要排序的数据中有负数，数据的范围是[-1000, 1000]，那我们就需要先对每个数据都加1000，转化成非负整数
    基数排序(Radix sort)
        参考：
            profit.jikeshijian.shujujiegou.sorts.RadixSort13.radixSort
        问题：
            假设我们有10万个手机号码，希望将这10万个手机号码从小到大排序，你有什么比较快速的排序方法呢?
        规律：
            1。假设要比较两个手机号码a，b的大小
            2。如果在前面几位中，a手机号码已经比b手机号码大了，那后面的几位就不用看了。
        思路：
            1。先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序
            2。以此类推，最后按照第一位重新排序。经过11次排序之后，手机号码就都有序了，手机号码稍微有点长
        注意：
            1。这里按照每位来排序的排序算法要是稳定的，否则这个实现思路就是不正确的
            2。如果是非稳定排序算法，那最后一次排序只会考虑最高位的大小顺序，完全不管其他位的大小关系，那么低位的排序就完全没有意义了。
        总结：
            1。需要可以分割出独立的“位”来比较，而且位之间有递进的关系
            2。如果a数据的高位比b数据大，那剩下的低位就不用比较了
            3。每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到O(n)了。
    思考题：
        如何根据年龄给100万用户排序?
        答案：
            1。根据年龄给100万用户排序，就类似按照成绩给50万考生排序。
            2。我们假设年龄的范围最小1岁，最大不超过120岁
            3。我们可以遍历这100万用户，根据年龄将其划分到这120个桶里，然后依次顺序遍历这120个桶中的元素。
14|排序优化:如何实现一个通用的、高性能的排序函数?
    如何选择合适的排序算法?
        com/suixingpay/profit/document/数据结构/图片/第14讲排序算法复杂度.png
    选择：
        情形一：
            如果对小规模数据进行排序，可以选择时间复杂度是O(n2)的算法;
        情形二：
            如果对大规模数据进行排序，时间复杂度是O(nlogn)的算法更加高效
    扩展：
        1。为了兼顾任意规模数据的排序，一般都会首选时间复杂度是O(nlogn)的排序算法来实现排序函数。
        2。时间复杂度是O(nlogn)的排序算法不止一个
            例子：
                1。归并排序、快速排序
                2。堆排序
        3。使用归并排序的情况其实并不多
            原因：
                1。归并排序并不是原地排序算法，空间复杂度是O(n)
                例子：
                    如果要排序100MB的数据，除了数据本身占用的内存之外，排序算法还要额外再占用100MB的内存空间，空间耗费就翻倍了。
        4。用快排比较多
            缺点：
                最坏情况下的时间复杂度是O(n2)，
                原因：
                    1。如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据
                    2。这种O(n2)时间复杂度出现的主要原因还是因为我们分区点选的不够合理。
                情形一：
                    被分区点分开的两个分区中，数据的数量差不多。
                情形二：
                    1。如果很粗暴地直接选择第一个或者最后一个数据作为分区点，不考虑数据的特点
                    2。肯定会出现之前讲的那样，在某些情况下，排序的最坏情况时间复杂度是O(n2)。
                    3。为了提高排序算法的性能，我们也要尽可能地让每次分区都比较平均。
            优化方法：
                1。三数取中法
                    原理：
                        1。我们从区间的首、尾、中间，分别取出一个数，然后对比大小，取这3个数的中间值作为分区点。
                        2。这样每间隔某个固定的长度，取数据出来比较，将中间值作为分区点的分区算法，肯定要比单纯取某一个数据更好
                        3。如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”
                2。随机法
                    原理：
                        随机法就是每次从要排序的区间中，随机选择一个元素作为分区点。
                    缺点：
                        不能保证每次分区点都选的比较好
                    扩展
                        1。从概率的角度来看，也不大可能会出现每次分区点都选的很差的情况
                        2。平均情况下，这样选的分区点是比较好的
                        3。时间复杂度退化为最糟糕的O(n2)的情况，出现的可能性不大。
    举例分析排序函数
        案例：
            情形一：
                数据量不大
                    qsort()会优先使用归并排序来排序输入数据
                        原因：
                            1。归并排序的空间复杂度是O(n)，所以对于小数据量的排序，比如1KB、2KB等
                            2。归并排序额外需要1KB、2KB的内存空间，这个问题不大
                        思路：
                            用空间换时间的技巧，现在计算机的内存都挺大的，我们很多时候追求的是速度
            情形二：
                数据量大的时候
                排序100MB的数据，这个时候我们再用归并排序就不合适了。
                方法；
                    要排序的数据量比较大的时候，qsort()会改为用快速排序算法来排序。
                    问题：
                        如何选择分区点：

                    答案
                        qsort()选择分区点的方法就是“三数取中法”。
            实际上：
                1。qsort()并不仅仅用到了归并排序和快速排序，它还用到了插入排序
                2。在快速排序的过程中，当要排序的区间中，元素的个数小于等于4时，qsort()就退化为插入排序
                3。不再继续用递归来做快速排序
                    原因：
                        在小规模数据面前，O(n2)时间复杂度的算法并不一定比O(nlogn)的算法执行时间长
        扩展：
            时间复杂度代表的是一个增长趋势，如果画成增长曲线图，你会发现O(n2)比O(nlogn)要陡峭，也就是说增长趋势要更猛一些。
15|二分查找(上):如何用最省内存的方式实现快速查找功能?
    二分查找：
        概念：
            针对有序数据集合的查找算法，也叫折半查找算法
        例子一：
            1。猜字游戏，我随机写一个0到99之间的数字，然后你来猜我写的是什么。
            2。我随机写一个0到99之间的数字，然后你来猜我写的是什么。
            问题：
                如何快速猜中我写的数字呢?
                方法：
                    二分法
        例子二：
            1。假设有1000条订单数据，已经按照订单金额从小到大排序
            2。每个订单金额都不同，并且最小单位是元。我们现在想知道是否存在金额等于19元的订单。
            3。如果存在，则返回订单数据，如果不存在则返回null。
            方法一：
                从第一个订单开始，一个一个遍历这1000个订单，直到找到金额等于19元的订单为止
                缺点：
                    查找会比较慢，最坏情况下，可能要遍历完这1000条记录才能找到
            方法二：
                1。利用二分思想，每次都与区间的中间数据比对大小，缩小查找区间的范围
                2。其中，low和high表示待查找区间的下标，mid表示待查找区间的中间元素下标。
                总结：
                    1。二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。
                    2。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为0
    查找速度：
        1。假设数据大小是n，每次查找后数据都会缩小为原来的一半，也就是会除以2。
        2。最坏情况下，直到查找区间被缩小为空，才停止。
        分析：
            1。这是一个等比数列。其中n/2k=1时，k的值就是总共缩小的次数
            2。而每一次缩小操作只涉及两个数据的大小比较，所以，经过了k次区间缩小操作，时间复杂度就是O(k)
            3。通过n/2k=1，我们可以求得k=log2n，所以时间复杂度就是O(logn)。
    参考：
        profit.jikeshijian.shujujiegou.sorts.BinsearchSort15.bSearch
        二分查找的递归与非递归实现
            容易出错的三个地方：
                1。循环退出条件
                    注意是low<=high，而不是low<high。
                2。mid的取值
                    注意：
                        mid=(low+high)/2这种写法是有问题的
                        原因：
                            1。如果low和high比较大的话，两者之和就有可能会溢出
                            2。改进的方法是将mid的计算方式写成low+(high- low)/2。
                            优化：
                                如果要将性能优化到极致的话，我们可以将这里的除以2操作转化成位运算low+((high-low)>>1)。
                                原因：
                                    相比除法运算来说，计算机处理位运算要快得多。
                3。low和high的更新
                    1。low=mid+1，high=mid-1
                    2。注意这里的+1和-1，如果直接写成low=mid或者high=mid，就可能会发生死循环。
                    3。比如，当high=3，low=3时，如果a[3]不等于value，就会导致一直循环不退出。
            递归代码的实现：
                参考：
                    profit.jikeshijian.shujujiegou.sorts.BinsearchSort15.bsearch02
    二分查找应用场景的局限性
        1。二分查找依赖的是顺序表结构，简单点说就是数组。
            问题：
                那二分查找能否依赖其他数据结构呢?
                答案：
                    比如链表。答案是不可以的
                    原因：
                        1。二分查找算法需要按照下标随机访问元素。
                        2。数组按照下标随机访问数据的时间复杂度是O(1)，而链表随机访问的时间复杂度是O(n)。
                        3。如果数据使用链表存储，二分查找的时间复杂就会变得很高。
            注意：
                如果你的数据是通过其他数据结构存储的，则无法应用二分查找
        2。二分查找针对的是有序数据。
            背景：
                如果数据没有序，我们需要先排序
            注意：
                情形一：
                    在插入、删除操作不频繁
                    1。如果我们针对的是一组静态的数据，没有频繁地插入、删除，我们可以进行一次排序，多次二分查找
                    2。这样排序的成本可被均摊，二分查找的边际成本就会比较低。
        3。数据量太小不适合二分查找
            概念：
                如果要处理的数据量很小，完全没有必要用二分查找，顺序遍历就足够了
            情形一：
                1。比如我们在一个大小为10的数组中查找一个元素，不管用二分查找还是顺序遍历，查找速度都差不多。
                2。只有数据量比较大的时候，二分查找的优势才会比较明显。
            情形二：
                如果数据之间的比较操作非常耗时，不管数据量大小，我都推荐使用二分查找
                例子：
                    1。数组中存储的都是长度超过300的字符串，如此长的两个字符串之间比对大小，就会非常耗时
                    2。我们需要尽可能地减少比较次数，而比较次数的减少会大大提高性能，这个时候二分查找就比顺序遍历更有优势。
        4。数据量太大也不适合二分查找
            原因：
                二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻。

            例子：
                1。比如，我们有1GB大小的数据，如果希望用数组来存储，那就需要的连续内存空间。
                分析：
                    1。即便有2GB的内存空间剩余，但是如果这剩余的2GB内存空间都是零散的
                    2。没有连续的1GB大小的内存空间，那照样无法申请一个1GB大小的数组。
                    3。二分查找是作用在数组这种数据结构之上的，所以太大的数据用数组存储就比较吃力了，也就不能用二分查找了。
    思考题：
        如何在1000万个整数中快速查找某个整数?
        分析：
            1。我们的内存限制是100MB，每个数据大小是8字节
            方法一：
                先对这1000万数据从小到大排序，然后再利用二分查找算法，就可以快速地查找想要的数据了
        猜测：
            散列表和二叉树也可以解决这个问题
            答案：
                不行；
        原因：
            1。虽然大部分情况下，用二分查找可以解决的问题，用散列表、二叉树都可以解决
            2。不管是散列表还是二叉树，都会需要比较多的额外的内存空间
            3。如果用散列表或者二叉树来存储这1000万的数据，用100MB的内存肯定是存不下的。
            4。而二分查找底层依赖的是数组，除了数据本身之外，不需要额外存储其他信息，是最省内存空间的存储方式
16|二分查找(下):如何快速定位IP对应的省份地址?
    变体一：查找第一个值等于给定值的元素
        案例：
            1。比如下面这样一个有序数组，其中，a[5]，a[6]，a[7]的值都等于8，是重复的数据。
            2。我们希望查找第一个等于8的数据，也就是下标是5的元素。
        过程：
            1。首先拿8与区间的中间值a[4]比较，8比6大，于是在下标5到9之间继续查找。
            2。下标5和9的中间位置是下标7，a[7]正好等于8，所以代码就返回了。
        分析：
            1。尽管a[7]也等于8，但它并不是我们想要找的第一个等于8的元素，因为第一个值等于8的元素是数组下标为5的元素
            2。针对这个变形问题，我们可以稍微改造一下上一节的代码。
        参考代码：
            profit.jikeshijian.shujujiegou.sorts.BinsearchSort15.bsearch04
    变体二：查找最后一个值等于给定值的元素
        问题：
            查找最后一个值等于给定值的元素，又该如何做呢?
        参考代码：
            profit.jikeshijian.shujujiegou.sorts.BinsearchSort15.bsearch05

    变体三：查找第一个大于等于给定值的元素
        案例：
            在有序数组中，查找第一个大于等于给定值的元素
        例子：
            数组中存储的这样一个序列:3，4，6，7，10。如果查找第一个大于等于5的元素，那就是6。
        参考代码：
            profit.jikeshijian.shujujiegou.sorts.BinsearchSort15.bsearch06

    变体四：查找最后一个小于等于给定值的元素
        例子：
            数组中存储了这样一组数据:3，5，6，8，9，10。最后一个小于等于7的元素就是6。
        参考代码：
            profit.jikeshijian.shujujiegou.sorts.BinsearchSort15.bsearch07
    思考题：
        如何快速定位出一个IP地址的归属地?
        思路：
            如果IP区间与归属地的对应关系不经常更新，我们可以先预处理这12万条数据，让其按照起始IP从小到大排序
        问题：
            如何来排序呢?
        答案：
            1。IP地址可以转化为32位的整型数。
            2。我们可以将起始地址，按照对应的整型值的大小关系，从小到大进行排序。
        方法：
            这个问题就可以转化为我刚讲的第四种变形问题“在有序数组中，查找最后一个小于等于某个给定值的元素”了。
        过程：
            1。当我们要查询某个IP归属地时，我们可以先通过二分查找，找到最后一个起始IP小于等于这个IP的IP区间
            2。然后，检查这个IP是否在这个IP区间内，如果在，我们就取出对应的归属地显示
            3。如果不在，就返回未查找到。
17|跳表:为什么Redis一定要用跳表来实现有序集合?
    背景：
        二分查找底层依赖的是数组随机访问的特性，所以只能用数组来实现
        问题：
            如果数据存储在链表中，就真的没法用二分查找算法了吗?
        方法：
            1。对链表稍加改造，就可以支持类似“二分”的查找算法
            2。改造之后的数据结构叫作跳表(Skip list)
    跳表：
        概念：
            动态数据结构，可以支持快速的插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树(Red-black tree)。
        扩展：
            Redis中的有序集合(Sorted Set)就是用跳表来实现的。
        背景：
            对于一个单链表来讲，即便链表中存储的数据是有序的，如果我们要想在其中查找某个数据，也只能从头到尾遍历链表
            缺点：
                查找效率就会很低，时间复杂度会很高，是O(n)。
            问题：
                那怎么来提高查找效率呢?
                思路一：
                    1。对链表建立一级“索引”，查找起来是不是就会更快一些呢?
                    2。每两个结点提取一个结点到上一级，我们把抽出来的那一级叫作索引或索引层
                    例子：
                        如果我们现在要查找某个结点，比如16。
                        流程：
                            1。我们可以先在索引层遍历，当遍历到索引层中值为13的结点时
                            2。我们发现下一个结点是17，那要查找的结点16肯定就在这两个结点之间
                            3。然后我们通过索引层结点的down指针，下降到原始链表这一层，继续遍历。
                            4。这个时候，我们只需要再遍历2个结点，就可以找到值等于16的这个结点了。
                        优点：
                            原来如果要查找16，需要遍历10个结点，现在只需要遍历7个结点。
                思路二：
                    那如果我们再加一级索引呢?效率会不会提升更多呢?
                    方法：
                        1。跟前面建立第一级索引的方式相似，我们在第一级索引的基础之上，每两个结点就抽出一个结点到第二级索引
                        2。现在我们再来查找16，只需要遍历6个结点了，需要遍历的结点数量又减少了。
        总结：
            前面讲的这种链表加多级索引的结构，就是跳表
    用跳表查询到底有多快?
        方法：
            通过时间复杂度来分析
        参考：
            在一个单链表中查询某个数据的时间复杂度是O(n)。
        问题：
            一个具有多级索引的跳表中，查询某个数据的时间复杂度是多少呢?
        思路：
            1。我把问题分解一下，先来看这样一个问题，如果链表里有n个结点，会有多少级索引呢?
            答案：
                1。每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是n/2
                2。第二级索引的结点个数大约就是n/4，
                3。第三级索引的结点个数大约就是n/8
                4。依次类推，也就是说，第k级索引的结点个数是第k-1级索引的结点个数的1/2，那第k级索引结点的个数就是n/(2k)。
        假设：
            假设索引有h级，最高级的索引有2个结点。
            分析
                1。通过上面的公式可以得到n/(2^h)=2,从而求的h=log2n-1
                2。如果包含原始链表这一层，整个跳表的高度就是log2n
                3。我们在跳表中查询某个数据的时候，如果每一层都要遍历m个结点，那在跳表中查询一个数据的时间复杂度就是O(m*logn)。
                问题：
                    那这个m的值是多少呢?
                    分析：
                        按照前面这种索引结构，我们每一级索引都最多只需要遍历3个结点，也就是说m=3
                        问题
                            为什么是3？
                            解释：
                                1。假设我们要查找的数据是x，在第k级索引中，我们遍历到y结点之后，发现x大于y，小于后面的结点z
                                2。所以我们通过y的down指针，从第k级索引下降到第k-1级索引。
                                3。在第k-1级索引中，y和z之间只有3个结点(包含y和z)，所以，我们在K-1级索引中最多只需要遍历3个结点，
                                4。依次类推，每一级索引都最多只需要遍历3个结点。
            结论：
                1。得到m=3，所以在跳表中查询任意数据的时间复杂度就是O(logn)
                2。查找的时间复杂度跟二分查找是一样的，基于单链表实现了二分查找
            缺点：
                空间换时间的设计思路。

    跳表是不是很浪费内存?
        背景：
            比起单纯的单链表，跳表需要存储多级索引，肯定要消耗更多的存储空间。
            问题：
                那到底需要消耗多少额外的存储空间呢?
                思路：
                    分析跳表的空间复杂度
            分析：
                1。假设原始链表大小为n，那第一级索引大约有n/2个结点，第二级索引大约有n/4个结点
                2。以此类推，每上升一级就减少一半，直到剩下2个结点。
                3。如果我们把每层索引的结点数写出来，就是一个等比数列。
                4。这几级索引的结点总和就是n/2+n/4+n/8...+8+4+2=n-2，跳表的空间复杂度是O(n)
                推出：
                    如果将包含n个结点的单链表构造成跳表，我们需要额外再用接近n个结点的存储空间
                问题：
                    有没有办法降低索引占用的内存空间呢?
        思路：
            前面都是每两个结点抽一个结点到上级索引
            猜测：
                如果我们每三个结点或五个结点，抽一个结点到上级索引，是不是就不用那么多索引结点了呢?
            验证：
                1。第一级索引需要大约n/3个结点，第二级索引需要大约n/9个结点。
                2。每往上一级，索引结点个数都除以3
                3。为了方便计算，我们假设最高一级的索引结点个数是1。
                4。我们把每级索引的结点个数都写下来，也是一个等比数列。
                5。通过等比数列求和公式，总的索引结点大约就是n/3+n/9+n/27+...+9+3+1=n/2。
            优点
                尽管空间复杂度还是O(n)，但比上面的每两个结点抽一个结点的索引构建方法，要减少了一半的索引结点存储空间。
        扩展：
            在软件开发中，我们不必太在意索引占用的额外空间。
            原因：
                1。原始链表中存储的有可能是很大的对象，而索引结点只需要存储关键值和几个指针
                2。并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了。
    高效的动态插入和删除
        概念：
            插入、删除操作的时间复杂度也是O(logn)。
            问题：
                如何做到O(logn)的时间复杂度的?
            插入操作思路：
                单链表
                    1。在单链表中，一旦定位好要插入的位置，插入结点的时间复杂度是很低的，就是O(1)。
                    2。这里为了保证原始链表中数据的有序性，我们需要先找到要插入的位置，这个查找操作就会比较耗时。
                    3。对于纯粹的单链表，需要遍历每个结点，来找到插入的位置
                跳表：
                    1。我们讲过查找某个结点的的时间复杂度是O(logn)
                    2。这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是O(logn)。
            删除操作思路
                1。如果这个结点在索引中也有出现，我们除了要删除原始链表中的结点，还要删除索引中的。
                    原因：
                        单链表中的删除操作需要拿到要删除结点的前驱结点，然后通过指针操作完成删除
                2。在查找要删除的结点的时候，一定要获取前驱结点。当然，如果我们用的是双向链表，就不需要考虑这个问题了。
    跳表索引动态更新
        背景：
            当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某2个索引结点之间数据非常多的情况
            现象：
                极端情况下，跳表还会退化成单链表。
        关键点：
            1。作为一种动态数据结构，我们需要某种手段来维护索引与原始链表大小之间的平衡
            2。如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降。
        方法：
            通过随机函数来维护前面提到的“平衡性”。
            扩展
                红黑树、AVL树这样平衡二叉树，你就知道它们是通过左右旋的方式保持左右子树的大小平衡
            思路：
                当我们往跳表中插入数据的时候，我们可以选择同时将这个数据插入到部分索引层中。
                问题：
                    如何选择加入哪些索引层呢?
                    流程：
                        1。我们通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值K
                        2。那我们就将这个结点添加到第一级到第K级这K级索引中。
                    注意：
                        1。随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化
    参考：
        profit.jikeshijian.shujujiegou.skiplist.SkipList17

18|散列表(上):Word文档中的单词拼写检查功能是如何实现的?
    散列思想
        散列表：
            背景：
                1。用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来
                2。可以说，如果没有数组，就没有散列表。
            例子一：
                1。假如我们有89名选手参加学校运动会。为了方便记录成绩，每个选手胸前都会贴上自己的参赛号码。
                2。这89名选手的编号依次是1到89。
                需求：
                    现在我们希望编程实现这样一个功能，通过编号快速找到对应的选手信息。你会怎么做呢?
                方法：
                    1。我们可以把这89名选手的信息放在数组里。编号为1的选手，我们放到数组中下标为1的位置;
                    2。编号为2的选手，我们放到数组中下标为2的位置
                    3。以此类推，编号为k的选手放到数组中下标为k的位置。
                    设计原因：
                        1。参赛编号跟数组下标一一对应，当我们需要查询参赛编号为x的选手的时候
                        2。只需要将下标为x的数组元素取出来就可以了，时间复杂度就是O(1)。
                    优点：
                        效率很高
                    思想：
                        散列的思想
                    分析：
                        1。参赛编号是自然数，并且与数组的下标形成一一映射
                        2。利用数组支持根据下标随机访问的时候，时间复杂度是O(1)这一特性
                        3。可以实现快速查找编号对应的选手信息。

            例子二
                1。假设校长说，参赛编号不能设置得这么简单，要加上年级、班级这些更详细的信息
                2。所以我们把编号的规则稍微修改了一下，用6位数字来表示。
                3。比如051167，其中，前两位05表示年级，中间两位11表示班级，最后两位还是原来的编号1到89
                思路：
                    1。不能直接把编号作为数组下标，
                    2。可以截取参赛编号的后两位作为数组下标，来存取选手信息数据
                    3。当通过参赛编号查询选手信息的时候，我们用同样的方法，取参赛编号的后两位，作为数组下标，来读取数组中的数据。
                分析：
                    1。参赛选手的编号我们叫作键(key)或者关键字。我们用它来标识一个选手。
                    2。把参赛编号转化为数组下标的映射方法就叫作散列函数(或“Hash函数”“哈希函数”)
                    3。而散列函数计算得到的值就叫作散列值(或“Hash值”“哈希值”)。
            规律：
                1。散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是O(1)的特性
                2。通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置
                3。按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。

        散列函数：
            概念：
                是一个函数。我们可以把它定义成hash(key)
                    说明：
                        1。key表示元素的键值
                        2。hash(key)的值表示经过散列函数计算得到的散列值。
            三个基本要求：
                1。散列函数计算得到的散列值是一个非负整数;
                    原因：
                        数组下标是从0开始的，所以散列函数生成的散列值也要是非负整数。
                2。如果key1 = key2，那hash(key1) == hash(key2);
                    原因：
                        相同的key，经过散列函数得到的散列值也应该是相同的
                3。如果key1 ≠ key2，那hash(key1) ≠ hash(key2)。
                    注意：
                        真实的情况下，要想找到一个不同的key对应的散列值都不一样的散列函数，几乎是不可能的。
                    原因：
                        数组的存储空间有限，也会加大散列冲突的概率

        散列冲突
            背景：
                再好的散列函数也无法避免散列冲突
                问题
                    如何解决散列冲突问题呢?
            方法：
                1。开放寻址法
                    思想：
                        如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入
                        问题：
                            那如何重新探测新的位置呢
                            方法一：
                                线性探测(Linear Probing)。
                                    流程：
                                        1。当我们往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了
                                        2。我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。
                                    例子：
                                        1。散列表的大小为10，在元素x插入散列表之前，已经6个元素插入到散列表中。
                                        2。x经过Hash算法之后，被散列到位置下标为7的位置，但是这个位置已经有数据了，所以就产生了冲突。
                                        3。于是我们就顺序地往后一个一个找，看有没有空闲的位置，遍历到尾部都没有找到空闲的位置
                                        4。于是我们再从表头开始找，直到找到空闲位置2，于是将其插入到这个位置。
                                    散列表查找：
                                        概念：
                                            查找元素的过程有点儿类似插入过程
                                        过程：
                                            1。通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。
                                            2。如果相等，则说明就是我们要找的元素，否则就顺序往后依次查找
                                            3。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中
                                        注意：
                                            1。在查找的时候，一旦我们通过线性探测方法，找到一个空闲位置，我们就可以认定散列表中不存在这个数据。
                                            2。如果这个空闲位置是我们后来删除的，就会导致原来的查找算法失效。
                                            现象：
                                                本来存在的数据，会被认定为不存在。
                                            问题：
                                                如何解决？
                                            方法：
                                                1。将删除的元素，特殊标记为deleted
                                                2。当线性探测查找的时候，遇到标记为deleted的空间，并不是停下来，而是继续往下探测。
                                    缺点：
                                        1。当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大
                                        2。空闲位置会越来越少，线性探测的时间就会越来越久
                                        情形一
                                            极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为O(n)。
                                        情形二：
                                            在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。
                            方法二：
                                二次探测(Quadratic probing)
                                类比
                                    线性探测
                                        1。线性探测每次探测的步长是1
                                        2。那它探测的下标序列就是hash(key)+0，hash(key)+1，hash(key)+2......
                                    二次探测：
                                        1。二次探测探测的步长就变成了原来的“二次方”
                                        2。探测的下标序列就是hash(key)+0，hash(key)+12，hash(key)+22......
                            方法三
                                双重散列(Double hashing)。
                                原理：
                                    1。不仅要使用一个散列函数
                                    2。我们使用一组散列函数hash1(key)，hash2(key)，hash3(key)......
                                    3。先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。
        装载因子：
            1。不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高
            2。为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。
            3。我们用装载因子(load factor)来表示空位的多少。
            装载因子的计算公式：
                散列表的装载因子=填入表中的元素个数/散列表的长度
                扩展
                    装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。
                4。链表法
                    原理：
                        1。在散列表中，每个“桶(bucket)”或者“槽(slot)”会对应一条链表
                        2。所有散列值相同的元素我们都放到相同槽位对应的链表中。
                    插入：
                        1。通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是O(1)。
                    查找、删除一个元素：
                        通过散列函数计算出对应的槽，然后遍历链表查找或者删除
                        时间复杂度：
                            1。这两个操作的时间复杂度跟链表的长度k成正比，也就是O(k)
                            2。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中n表示散列中数据的个数，m表示散列表中“槽”的个数。
    思考题：
        Word文档中单词拼写检查功能是如何实现的?
        分析：
            1。常用的英文单词有20万个左右，假设单词的平均长度是10个字母
            2。平均一个单词占用10个字节的内存空间，那20万英文单词大约占2MB的存储空间，就算放大10倍也就是20MB
            3。对于现在的计算机来说，这个大小完全可以放在内存里面。所以我们可以用散列表来存储整个英文单词词典。
        流程：
            1。当用户输入某个英文单词时，我们拿用户输入的单词去散列表中查找
            2。如果查到，则说明拼写正确;如果没有查到，则说明拼写可能有误，给予提示
            3。借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误。
19|散列表(中):如何打造一个工业级水平的散列表?
    背景：
        1。散列表的查询效率并不能笼统地说成是O(1)。
        2。它跟散列函数、装载因子、散列冲突等都有关系
        3。如果散列函数设计得不好，或者装载因子过高，都可能导致散列冲突发生的概率升高，查询效率下降。
    散列表碰撞攻击案例：
        极端情况下，有些恶意的攻击者，还有可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里
        如：
            1。如果散列表中有10万个数据，退化后的散列表查询的效率就下降了10万倍。
        分析：
            1。如果之前运行100次查询只需要0.1秒，那现在就需要1万秒
            2。查询操作消耗大量CPU或者线程资源，导致系统无法响应其他请求
            3。达到拒绝服务攻击(DoS)的目的

    如何设计散列函数?
        背景：
            散列函数设计的好坏，决定了散列表冲突的概率大小，也直接决定了散列表的性能。
            问题：
                什么才是好的散列函数呢?
                答案：
                    1。散列函数的设计不能太复杂
                        原因：
                            过于复杂的散列函数，势必会消耗很多计算时间，也就间接的影响到散列表的性能
                    2。散列函数生成的值要尽可能随机并且均匀分布
                        目的：
                            1。避免或者最小化散列冲突
                            2。即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况。
                    3。实际工作中，我们还需要综合考虑各种因素
                        例子：
                            关键字的长度、特点、分布、还有散列表的大小等。
        例子一：
            数据分析法
                1。学生运动会的例子，我们通过分析参赛编号的特征，把编号中的后两位作为散列值
                2。还可以用类似的散列函数处理手机号码，因为手机号码前几位重复的可能性很大
                3。但是后面几位就比较随机，我们可以取手机号的后四位作为散列值
        例子二：
            如何实现Word拼写检查功能
            设计：
                1。将单词中每个字母的ASCll码值“进位”相加，
                2。然后再跟散列表的大小求余、取模，作为散列值
                如：英文单词nice，我们转化出来的散列值就是下面这样:
                hash("nice")=(("n" - "a") * 26*26*26 + ("i" - "a")*26*26 + ("c" - "a")*26+ ("e"-"a")) / 78978
        扩展：
            直接寻址法、平方取中法、折叠法、随机数法等
    装载因子过大了怎么办?
        背景：
            装载因子越大，说明散列表中的元素越多，空闲位置越少，散列冲突的概率就越大
        现象：
            不仅插入数据的过程要多次寻址或者拉很长的链，查找的过程也会因此变得很慢。
        情形一：
            1。对于没有频繁插入和删除的静态数据集合来说，
            2。我们很容易根据数据的特点、分布等，设计出完美的、极少冲突的散列函数，因为毕竟之前数据都是已知的。
        情形二：
            1。对于动态散列表来说，数据集合是频繁变动的
            2。我们事先无法预估将要加入的数据个数，所以我们也无法事先申请一个足够大的散列表
            现象：
                1。随着数据慢慢加入，装载因子就会慢慢变大。
                2。当装载因子大到一定程度之后，散列冲突就会变得不可接受。
            问题：
                这个时候，我们该如何处理呢?
                方法：
                    动态扩容，重新申请一个更大的散列表，将数据搬移到这个新散列表中
                例子：
                    1。假设每次扩容我们都申请一个原来散列表大小两倍的空间
                    2。如果原来散列表的装载因子是0.8，那经过扩容之后，新散列表的装载因子就下降为原来的一半，变成了0.4。
                    数组扩容：
                        数据搬移操作比较简单
                    散列表扩容：
                        数据搬移操作要复杂很多
                        原因：
                            散列表的大小变了，数据的存储位置也变了，需要通过散列函数重新计算每个数据的存储位置。
                        问题：
                            对于支持动态扩容的散列表，插入操作的时间复杂度是多少呢？
                            分析：
                                1。插入一个数据，最好情况下，不需要扩容，最好时间复杂度是O(1)
                                2。最坏情况下，散列表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是O(n)。
                                3。用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是O(1)。
                            注意：
                                对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多
                                    情形一：
                                        如果我们对空间消耗非常敏感，我们可以在装载因子小于某个值之后，启动动态缩容
                                    情形二：
                                        如果我们更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了。
        注意：
            1。当散列表的装载因子超过某个阈值时，就需要进行扩容。装载因子阈值需要选择得当。
            2。如果太大，会导致冲突过多
            3。如果太小，会导致内存浪严重。
        扩展：
            1。装载因子阈值的设置要权衡时间、空间复杂度
            2。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值;
            3。相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于1。
    如何避免低效地扩容?
        背景：
            1。动态扩容的散列表插入一个数据都很快
            2。但是在特殊情况下，当装载因子已经到达阈值，需要先进行扩容，再插入数据。
            现象
                插入数据就会变得很慢，甚至会无法接受。
            例子：
                如果散列表当前大小为1GB，要想扩容为原来的两倍大小
                分析：
                    1。那就需要对1GB的数据重新计算哈希值
                    2。并且从原来的散列表搬移到新的散列表
                缺点：
                    很耗时
                    方法：
                        将扩容操作穿插在插入操作的过程中，分批完成
                        具体流程：
                            1。当装载因子触达阈值之后，我们只申请新空间
                            2。但并不将老的数据搬移到新散列表中。
                            3。当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表
                            4。每次插入一个数据到散列表，我们都重复上面的过程
                            5。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了
                        优点：
                            这样没有了集中的一次性数据搬移，插入操作就都变得很快了。
                        问题：
                            查询操作怎么做？
                        答案：
                            先从新散列表中查找，如果没有找到，再去老的散列表中查找
                        分析：
                            1。通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况
                            2。这种实现方式，任何情况下，插入一个数据的时间复杂度都是O(1)。
        参考图片：
            com/suixingpay/profit/document/数据结构/图片/第19讲避免低效的动态扩容.png
    如何选择冲突解决方法?
        1。开放寻址法
            优点：
                1。散列表中的数据都存储在数组中，可以有效地利用CPU缓存加快查询速度。
                1。这种方法实现的散列表，序列化起来比较简单
            缺点：
                1。删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据
                2。所有的数据都存储在一个数组中，冲突的代价更高
                3。使用开放寻址法解决冲突的散列表，装载因子的上限不能太大，浪费内存
            应用：
                当数据量比较小、装载因子小的时候，适合采用开放寻址法
            例子：
                Java中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。
        2。链表法
            优点：
                1。链表法对内存的利用率比开放寻址法要高
                    原因：
                        链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。
                2。链表法比起开放寻址法，对大装载因子的容忍度更高。
                    原因：
                        开放寻址法：
                            适用装载因子小于1的情况。接近1时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多
                        链表法：
                            只要散列函数的值随机均匀，即便装载因子变成10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。
            扩展：
                对链表法稍加改造，可以实现一个更加高效的散列表
                例子：
                    改造为其他高效的动态数据结构，比如跳表、红黑树
                优点：
                    1。即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是O(logn)。
                2。有效避免了前面讲到的散列碰撞攻击。
            应用：
                1。基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表
                2。比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。
    工业级散列表举例分析：
        Java中的HashMap这样一个工业级的散列表
            流程：
                1。初始大小
                    1。HashMap默认的初始大小是16，这个默认值是可以设置的
                    2。如果事先知道大概的数据量有多大，可以通过修改默认初始大小
                    优点：
                        减少动态扩容的次数，这样会大大提高HashMap的性能。
                2。装载因子和动态扩容
                    1。最大装载因子默认是0.75，当HashMap中元素个数超过0.75*capacity(capacity表示散列表的容量)的时候
                    2。就会启动扩容，每次扩容都会扩容为原来的两倍大小。
                3。散列冲突解决方法
                    HashMap底层采用链表法来解决冲突
                    原因：
                        即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况
                        现象：
                            一旦出现拉链过长，则会严重影响HashMap的性能。
                        方法：
                            在JDK1.8版本中，为了对HashMap做进一步优化，我们引入了红黑树
                            过程：
                                1。而当链表长度太长(默认超过8)时，链表就转换为红黑树
                                    优点：
                                        可以利用红黑树快速增删改查的特点，提高HashMap的性能
                                2。当红黑树结点个数少于8个的时候，又会将红黑树转化为链表
                                    原因：
                                        数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。
                4。散列函数
                    int hash(Object key) {
                        int h = key.hashCode();
                        return (h ^ (h >>> 16)) & (capitity -1); //capicity表示散列表的大小
                    }
    思考题：
        如何设计的一个工业级的散列函数?
        思考：
            1。何为一个工业级的散列表?
            2。工业级的散列表应该具有哪些特性?
        要求：
            1。支持快速的查询、插入、删除操作;
            2。内存占用合理，不能浪费过多的内存空间;
            3。性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。
        如何实现这样一个散列表呢？
            设计思路：
                1。设计一个合适的散列函数;
                2。定义装载因子阈值，并且设计动态扩容策略;
                3。选择合适的散列冲突解决方法。

20|散列表(下):为什么散列表和链表经常会一起使用?
    背景：
        1。链表实现的LRU缓存淘汰算法的时间复杂度是O(n)，通过散列表可以将这个时间复杂度降低到O(1)。
        2。在跳表那一节，我提到Redis的有序集合是使用跳表来实现的，跳表可以看作一种改进版的链表
        3。Redis有序集合不仅使用了跳表，还用到了散列表。
        4。如果你熟悉Java编程语言，你会发现LinkedHashMap这样一个常用的容器，也用到了散列表和链表两种数据结构。
    LRU缓存淘汰算法
        问题：
            如何通过链表实现LRU缓存淘汰算法的。
            方法：
                1。维护一个按照访问时间从大到小有序排列的链表结构。
                    原因：
                        缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，我们就直接将链表头部的结点删除。
                2。当要缓存某个数据的时候，先在链表中查找这个数据
                    1。如果没有找到，则直接将数据放到链表的尾部
                    2。如果找到了，我们就把它移动到链表的尾部。
                    原因：
                        查找数据需要遍历链表，所以单纯用链表实现的LRU缓存淘汰算法的时间复杂很高，是O(n)。
    缓存的几个操作：
        1。往缓存中添加一个数据;
        2。从缓存中删除一个数据;
        3。在缓存中查找一个数据。
        分析：
            1。这三个操作都要涉及“查找”操作，如果单纯地采用链表的话，时间复杂度只能是O(n)。
            2。如果我们将散列表和链表两种数据结构组合使用，可以将这三个操作的时间复杂度都降低到O(1)。
        参考：
            com/suixingpay/profit/document/数据结构/图片/第20讲散列表和链表两种数据结构组合使用.png
            解释：
                1。使用双向链表存储数据，链表中的每个结点存储数据(data)，前驱指针(prev)，后继指针(next)之外
                2。新增了一个特殊的字段hnext
            问题：
                这个hnext有什么作用呢?
                答案：
                    hnext指针是为了将结点串在散列表的拉链中。
                原因：
                    1。散列表是通过链表法解决散列冲突的，所以每个结点会在两条链中。
                    2。一个链是刚刚我们提到的双向链表，另一个链是散列表中的拉链。
                    3。前驱和后继指针是为了将结点串在双向链表中
        问题：
            前面讲到的缓存的三个操作，是如何做到时间复杂度是O(1)的?
            1。查询操作：
                概念：
                    散列表中查找数据的时间复杂度接近O(1)
                优点：
                    通过散列表，我们可以很快地在缓存中找到一个数据
                过程：
                    当找到数据之后，我们还需要将它移动到双向链表的尾部。
            2。删除操作：
                概念：
                    需要找到数据所在的结点，然后将结点删除。
                优点：
                    借助散列表，我们可以在O(1)时间复杂度里找到要删除的结点
                    原因：
                        1。我们的链表是双向链表，双向链表可以通过前驱指针O(1)时间复杂度获取前驱结点
                        2。所以在双向链表中，删除结点只需要O(1)的时间复杂度
            3。添加一个数据：
                过程：
                    1。需要先看这个数据是否已经在缓存中
                        1。如果已经在其中，需要将其移动到双向链表的尾部;
                        2。如果不在其中，还要看缓存有没有满
                            1。如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部;
                            2。如果没有满，就直接将数据放到链表的尾部。
    Redis有序集合
        概念：
            1。在有序集合中，每个成员对象有两个重要的属性，key(键值)和score(分值)
            2。我们不仅会通过score来查找数据，还会通过key来查找数据。
        案例：
            1。比如用户积分排行榜有这样一个功能:我们可以通过用户的ID来查找积分信息
            2。也可以通过积分区间来查找用户ID或者姓名信息。
            3。这里包含ID、姓名和积分的用户信息，就是成员对象，用户ID就是key，积分就是score。
        细化一下Redis有序集合的操作
            1。添加一个成员对象;
            2。按照键值来删除一个成员对象;
            3。按照键值来查找一个成员对象;
            4。按照分值区间查找数据，比如查找积分在[100, 356]之间的成员对象;
            5。按照分值从小到大排序成员变量;
        分析：
            情形一：
                如果我们仅仅按照分值将成员对象组织成跳表的结构，那按照键值来删除、查询成员对象就会很慢
                思路：
                    与LRU缓存淘汰算法的解决方法类似
                方法：
                    1。再按照键值构建一个散列表，这样按照key来删除、查找一个成员对象的时间复杂度就变成了O(1)。
                    2。同时，借助跳表结构，其他操作也非常高效。
        扩展：
            Redis有序集合的操作还有另外一类，也就是查找成员对象的排名(Rank)或者根据排名区间查找成员对象
    Java LinkedHashMap
        背景：
            HashMap底层是通过散列表这种数据结构实现的
            问题
                比HashMap多了一个“Linked”，这里的“Linked”是不是说，LinkedHashMap是一个通过链表法解决散列冲突的散列表呢?
        插入顺序参考代码：
            HashMap<Integer, Integer> m = new LinkedHashMap<>();
            m.put(3, 11);
            m.put(1, 12);
            m.put(5, 23);

            for (Map.Entry e : m.entrySet()) {
              System.out.println(e.getKey()); }
            打印顺序就是3，1，5，2。
            问题：
                散列表中数据是经过散列函数打乱之后无规律存储的，这里是如何实现按照数据的插入顺序来遍历打印的呢?
            原因：
                1。LinkedHashMap也是通过散列表和链表组合在一起实现的
                2。它不仅支持按照插入顺序遍历数据，还支持按照访问顺序来遍历数据
        访问顺序访问：
            参考代码
            // 10是初始大小，0.75是装载因子，true是表示按照访问时间排序 
                HashMap<Integer, Integer> m = new LinkedHashMap<>(10, 0.75f, true);
                m.put(3, 11);
                m.put(1, 12);
                m.put(5, 23);
                m.put(2, 22);

                m.put(3, 26);
                m.get(5);
                for (Map.Entry e : m.entrySet()) {
                    System.out.println(e.getKey());
                }
            打印结果：1，2，3，5。
                分析：
                    1。每次调用put()函数，往LinkedHashMap中添加数据的时候，都会将数据添加到链表的尾部
                    2。再次将键值为3的数据放入到LinkedHashMap的时候，会先查找这个键值是否已经有了
                    3。然后，再将已经存在的(3,11)删除，并且将新的(3,26)放到 链表的尾部。
                    4。访问到key为5的数据的时候，我们将被访问到的数据移动到链表的尾部
            现象：
                按照访问时间排序的LinkedHashMap本身就是一个支持LRU缓存淘汰策略的缓存系统?
            总结：
                1。LinkedHashMap是通过双向链表和散列表这两种数据结构组合实现的。
                2。LinkedHashMap中的“Linked”实际上是指的是双向链表，并非指用链表法解决散列冲突。
    思考题：
        为什么散列表和链表经常一块使用?
        原因：
            1。散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的
            2。它无法支持按照某种顺序快速地遍历数据
            3。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历。

21|哈希算法(上):如何防止数据库中的用户信息被脱库?
    哈希算法：
        背景：
            1。不管是“散列”还是“哈希”，这都是中文翻译的差别，英文其实就是“Hash"
            2。我们常听到有人把“散列表”叫作“哈希表”“Hash表”，把“哈希算法”叫作“Hash算法”或者“散列算法”
        概念：
            将任意长度的二进制值串映射为固定长度的二进制值串
        哈希值：
            概念：
                通过原始数据映射之后得到的二进制值串就是哈希值。
        要求：
            1。从哈希值不能反向推导出原始数据(所以哈希算法也叫单向哈希算法);
            2。对输入数据非常敏感，哪怕原始数据只修改了一个Bit，最后得到的哈希值也大不相同;
            3。散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小;
            4。哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。
        案例：
            计算MD5哈希值，得到两串看起来毫无规律的字符串
            现象：
                1。可以看出来，无论要哈希的文本有多长、多短，通过MD5哈希之后
                2。得到的哈希值的长度都是相同的，而且得到的哈希值看起来像一堆随机数，完全没有规律。
        应用一：
            安全加密
                常用的加密算法：
                    1。MD5(MD5 Message-Digest Algorithm，MD5消息摘要算法)
                    2。SHA(Secure Hash Algorithm，安全散列算法)
                    3。DES(Data Encryption Standard，数据加密标准)
                    4。AES(Advanced Encryption Standard，高级加密标准)
                重要点：
                    1。很难根据哈希值反向推导出原始数据
                        目的：
                            加密的目的就是防止原始数据泄露
                    2。散列冲突的概率要很小。
                        现象：
                            不管是什么哈希算法，我们只能尽量减少碰撞冲突的概率，理论上是没办法做到完全不冲突的。为什么这么说呢?
                        原因：
                            1。哈希算法产生的哈希值的长度是固定且有限的
                            2。比如前面举的MD5的例子，哈希值是固定的128位二进制串，能表示的数据是有限的
                            3。最多能表示2^128个数据，而我们要哈希的数据是无穷的
                            4。基于鸽巢原理，如果我们对2^128+1个数据求哈希值，就必然会存在哈希值相同的情况。
                        例子
                            鸽巢原理(也叫抽屉原理)：
                                如果有10个鸽巢，有11只鸽子，那肯定有1个鸽巢中的鸽子数量多于1个，换句话说就是，肯定有2只鸽子在1个鸽巢内。
            扩展：
                1。没有绝对安全的加密。越复杂、越难破解的加密算法，需要的计算时间也越长。
                2。比如SHA-256比SHA-1要更复杂、更安全，相应的计算时间就会比较长
        应用二：
            唯一标识
            案例：
                如果要在海量的图库中，搜索一张图是否存在
                思路：
                    我们不能单纯地用图片的元信息(比如图片名称)来比对
                原因：
                    有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。
                问题：
                    那我们该如何搜索呢?
                    背景：
                        任何文件在计算中都可以表示成二进制码串
                    方法一：
                        1。拿要查找的图片的二进制码串与图库中所有图片的二进制码串一一比对。
                        2。如果相同，则说明图片在图库中存在
                        缺点：
                            每个图片小则几十KB、大则几MB，转化成二进制是一个非常长的串，比对起来非常耗时
                    方法二：
                        我们可以给每一个图片取一个唯一标识，或者说信息摘要
                        例子：
                            1。可以从图片的二进制码串开头取100个字节，从中间取100个字节
                            2。从最后再取100个字节，然后将这300个字节放到一块，
                            3。通过哈希算法(比如MD5)，得到一个哈希字符串，用它作为图片的唯一标识
                        优点：
                            通过这个唯一标识来判定图片是否在图库中，这样就可以减少很多工作量。
                        优化：
                            1。如果还想继续提高效率，我们可以把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中
                            2。当要查看某个图片是不是在图库中的时候，我们先通过哈希算法对这个图片取唯一标识
                            3。然后在散列表中查找是否存在这个唯一标识。
                                1。如果不存在，那就说明这个图片不在图库中;
                                2。如果存在，我们再通过散列表中存储的文件路径，获取到这个已经存在的图片
                                3。跟现在要插入的图片做全量的比对，看是否完全一样
                                    1。如果一样，就说明已经存在
                                    2。如果不一样，说明两张图片尽管唯一标识相同，但是并不是相同的图片。
        应用三：
            数据校验
            案例：
                1。电驴的BT下载，BT下载的原理是基于P2P协议的
                2。我们从多个机器上并行下载一个2GB的电影，这个电影文件可能会被分割成很多文件块
                3。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了。
                背景：
                    1。网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或者下载过程中出现了错误，下载的文件块可能不是完整的。
                    2。如果我们没有能力检测这种恶意修改或者文件下载出错，就会导致最终合并后的电影无法观看，甚至导致电脑中毒
                问题：
                    如何来校验文件块的安全、正确、完整呢?
                思路：
                    1。对100个文件块分别取哈希值，并且保存在种子文件中
                    2。哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同
                    3。当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对
                    4。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。

        应用四：
            散列函数
            作用：
                决定了散列冲突的概率和散列表的性能
            特点：
                1。对于散列算法冲突的要求要低很多。
                    方法：
                        即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法解决。
                2。散列函数对于散列算法计算得到的值，是否能反向解密也并不关心。
                3。散列函数中用到的散列算法，更加关注散列后的值是否能平均分布，也就是，一组数据是否能均匀地散列在各个槽中
                4。散列函数执行的快慢，也会影响散列表的性能
                5。散列函数用的散列算法一般都比较简单，比较追求效率。

    思考题：
        用户明文存储，如何保证安全？
        答案：
            1。通过哈希算法，对用户密码进行加密之后再存储
            2。最好选择相对安全的加密算法，比如SHA等(因为MD5已经号称被破解了)
            问题：
                加密之后存储就万事大吉了吗?
                破解：
                    如果用户信息被“脱库”，黑客虽然拿到是加密之后的密文，但可以通过“猜”的方式来破解密码
                原因：
                    有些用户的密码太简单。
                例子：
                    比如很多人习惯用00000、123456这样的简单数字组合做密码，很容易就被猜中。
                    过程：
                        1。那我们就需要维护一个常用密码的字典表，把字典中的每个密码用哈希算法计算哈希值，然后拿哈希值跟脱库后的密文比对
                        2。如果相同，基本上就可以认为，这个加密之后的密码对应的明文就是字典中的这个密码
                    优化：
                        针对字典攻击，我们可以引入一个盐(salt)，跟用户的密码组合在一起，增加密码的复杂度。

22|哈希算法(下):哈希算法在分布式系统中有哪些应用?
    应用五:负载均衡
        算法：
            1。轮询
            2。随机
            3。加权轮询
        问题：
            那如何才能实现一个会话粘滞(session sticky)的负载均衡算法呢?
                说明：
                  我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上。
            方法一：
                维护一张映射关系表，这张表的内容是客户端IP地址或者会话ID与服务器编号的映射关系
                流程：
                    1。客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号
                    2。然后再请求编号对应的服务器。
                缺点：
                    1。如果客户端很多，映射表可能会很大，比较浪费内存空间;
                    2。客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大;
            方法二：
                1。如果借助哈希算法，这些问题都可以非常完美地解决。
                    流程：
                        1。对客户端IP地址或者会话ID计算哈希值
                        2。将取得的哈希值与服务器列表的大小进行取模运算
                        3。最终得到的值就是应该被路由到的服务器编号
                    优点：
                        我们就可以把同一个IP过来的所有请求，都路由到同一个后端服务器

    应用六:数据分片
        案例一：
            如何统计“搜索关键词”出现的次数?
            1。假如我们有1T的日志文件，这里面记录了用户的搜索关键词，
            2。我们想要快速统计出每个关键词被搜索的次数，该怎么做呢?
            分析：
                两个难点：
                    1。搜索日志很大，没办法放到一台机器的内存中
                    2。如果只用一台机器来处理这么巨大的数据，处理时间会很长。
                方法：
                    先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度
                思路：
                    1。为了提高处理的速度，我们用n台机器并行处理。
                    2。我们从搜索记录的日志文件中，依次读出每个搜索关键词
                    3。并且通过哈希函数计算哈希值，然后再跟n取模，最终得到的值，就是应该被分配到的机器编号。
            优点：
                1。哈希值相同的搜索关键词就被分配到了同一个机器上
                2。同一个搜索关键词会被分配到同一个机器上
                3。每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果。
        案例二：
            如何快速判断图片是否在图库中?
            方法一：
                即给每个图片取唯一标识(或者信息摘要)，然后构建散列表。
                缺点：
                    在单台机器上构建散列表是行不通的。因为单台机器的内存有限
                例子：
                    假设现在我们的图库中有1亿张图片，而1亿张图片构建散列表显然远远超过了单台机器的内存上限。
            方法二：
                思路：
                    1。我们同样可以对数据进行分片，然后采用多机处理
                    2。我们准备n台机器，让每台机器只维护某一部分图片对应的散列表。
                    3。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数n求余取模，得到的值就对应要分配的机器编号
                    4。然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表
                情形一：
                    当我们要判断一个图片是否在图库中的时候
                    过程：
                        1。我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数n求余取模。
                        2。假设得到的值是k，那就去编号k的机器构建的散列表中查找。
                问题：
                    给这1亿张图片构建散列表大约需要多少台机器。
                分析：
                    1。散列表中每个数据单元包含两个信息，哈希值和图片文件的路径
                        哈希值：
                            假设我们通过MD5来计算哈希值，那长度就是128比特，也就是16字节
                        图片文件的路径：
                            文件路径长度的上限是256字节，我们可以假设平均长度是128字节。
                    2。如果我们用链表法来解决冲突，那还需要存储指针，指针只占用8字节
                计算过程：
                    1。假设一台机器的内存大小为2GB，散列表的装载因子为0.75
                    2。那一台机器可以给大约1000万(2GB*0.75/152)张图片构建散列表
                    3。如果要对1亿张图片构建索引，需要大约十几台机器
                扩展：
                    1。这种估算还是很重要的，能让我们事先对需要投入的资源、资金有个大概的了解，能更好地评估解决方案的可行性
                    2。借助这种分片的思路，可以突破单机内存、CPU等资源的限制

    应用七:分布式存储
        背景：
            1。现在互联网面对的都是海量的数据、海量的用户
            2。为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存
            3。我们有海量的数据需要缓存，所以一个缓存机器肯定是不够的。于是，我们就需要将数据分布在多台机器上。
        问题：
            如何决定将哪个数据放到哪个机器上呢?
            思路：
                1。借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模
                2。这个最终值就是应该存储的缓存机器编号。
            需求：
                如果数据增多，原来的10个机器已经无法承受了，我们就需要扩容了，比如扩到11个机器，这时候麻烦就来了
            分析：
                1。原来的数据是通过与10来取模的。比如13这个数据，存储在编号为3这台机器上
                2。但是新加了一台机器中，我们对数据按照11取模，原来13这个数据就被分配到2号这台机器上了。
            现象：
                1。所有的数据都要重新计算哈希值，然后重新搬移到正确的机器上
                2。相当于，缓存中的数据一下子都失效了，所有的数据请求都会穿透缓存，直接去请求数据库，这样就会发生雪崩效应，压垮数据库
                思路：
                    使得在新加入一个机器后，并不需要做大量的数据搬移。
                方法：
                    一致性哈希算法
                流程：
                    1。假设我们有k个机器，数据的哈希值的范围是[0, MAX]
                    2。我们将整个范围划分成m个小区间(m远大于k)，每个机器负责m/k个小区间
                    3。当有新机器加入的时候，我们就将某几个小区间的数据
                    4。从原来的机器中搬移到新的机器中。
                    5。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡
                优化：
                    它还会借助一个虚拟的环和虚拟结点，更加优美地实现出来

23|二叉树基础(上):什么样的二叉树适合用数组来存储?
    树(Tree)
        概念：
            每个元素我们叫作“节点”;用来连线相邻节点之间的关系，我们叫作“父子关系”
        例子：
            1。A节点就是B节点的父节点，B节点是A节点的子节点
            2。B、C、D这三个节点的父节点是同一个节点，所以它们之间互称为兄弟节点
            3。我们把没有父节点的节点叫作根节点
            4。我们把没有子节点的节点叫作叶子节点或者叶节点
        节点的高度=节点到叶子节点的最长路径
            高度：
                概念：
                    其实就是从下往上度量
                例子：
                    我们要度量第10层楼的高度、第13层楼的高度，起点都是地面
        节点的深度=根节点到这个节点所经历的边的个数
            深度：
                概念：
                    在生活中是从上往下度量的
                例子：
                    比如水中鱼的深度，是从水平面开始度量的
        节点的层数=节点的深度+1
            层数：
                概念：
                    跟深度的计算类似，不过，计数起点是1，也就是说根节点的位于第1层。
        树的高度=根节点的高度
    二叉树：
        概念：
            1。每个节点最多有两个“叉”，也就是两个子节点，分别是左子节点和右子节点
            2。二叉树并不要求每个节点都有两个子节点，有的节点只有左子节点，有的节点只有右子节点
        满二叉树：
            概念：
                叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点
        完全二叉树：
            概念：
                1。叶子节点都在最底下两层，最后一层的叶子节点都靠左排列
                2。并且除了最后一层，其他层的节点个数都要达到最大
        问题：
            如何表示(或者存储)一棵二叉树?
            方法一：
                基于指针或者引用的二叉链式存储法
                实现方式：
                    1。每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针
                    2。我们只要拎住根节点，就可以通过左右子节点的指针，把整棵树都串起来
                应用：
                    大部分二叉树代码都是通过这种结构来实现的。
                参考：
                    com/suixingpay/profit/document/数据结构/图片/第23讲二叉树的链式存储法.png
            方法二：
                基于数组的顺序存储法。
                完全二叉树具体实现：
                    1。我们把根节点存储在下标i = 1的位置，那左子节点存储在下标2 * i = 2的位置
                    2。右子节点存储在2 * i + 1 = 3的位置
                    3。我们再来看，基于数组的顺序存储法。我们把根节点存储在下标i = 1的位置
                    4。那左子节点存储在下标2 * i = 2的位置，右子节点存储在2 * i + 1 = 3的位置。
                    参考：
                        com/suixingpay/profit/document/数据结构/图片/第23讲完全二叉树顺序存储.png
                    总结：
                        1。如果节点X存储在数组中下标为i的位置，下标为2 * i 的位置存储的就是左子节点
                        2。下标为2 * i + 1的位置存储的就是右子节点
                        3。反过来，下标 为i/2的位置存储就是它的父节点。
                    优点：
                        1。我们只要知道根节点存储的位置，这样就可以通过下标计算，把整棵树都串起来。
                        2。用数组存储无疑是最节省内存的一种方式
                    背景：
                        1。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。
                        2。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因

                非完全二叉树：
                    缺点：
                        其实会浪费比较多的数组存储空间
                    参考：
                        com/suixingpay/profit/document/数据结构/图片/第23讲非完全二叉树顺序存储.png
    遍历
        分析：
            1。二叉树的前、中、后序遍历就是一个递归的过程。
            2。前序遍历，其实就是先打印根节点，然后再递归地打印左子树，最后递归地打印右子树
        思路：
            1。写递归代码的关键，就是看能不能写出递推公式，而写递推公式的关键就是
            2。如果要解决问题A，就假设子问题B、C已经解决
            3。然后再来看如何利用B，C来解决A。所以可以把前，中，后序遍历的递推公式都写出来
        前序遍历：
            参考：
                profit.jikeshijian.shujujiegou.tree.PreOrder23.preOrderTraverse1
        中序遍历：
            参考：
                profit.jikeshijian.shujujiegou.tree.InOrder23.inOrderTraverse
        后序遍历：
            参考：
                profit.jikeshijian.shujujiegou.tree.PostOrder23.postOrderTraverse
        总结：
            1。每个节点最多会被访问两次，所以遍历操作的时间复杂度，跟节点的个数n成正比
            2。也就是说二叉树遍历的时间复杂度是O(n)。

24|二叉树基础(下):有了如此高效的散列表，为什么还需要二叉树?
    二叉查找树(Binary Search Tree)
        背景：
            二叉查找树是为了实现快速查找而生的。
        概念：
            1。二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树
            2。不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据
        要求：
            1。在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值
            2。而右子树节点的值都大于这个节点的值
        问题：
            二叉查找树支持快速查找、插入、删除操作，现在我们就依次来看下，这三个操作是如何实现的？
            答案：
                1。二叉查找树的查找操作
                    查找过程：
                        1。先取根节点，如果它等于我们要查找的数据，那就返回
                        2。如果要查找的数据比根节点的值小，那就在左子树中递归查找
                        3。如果要查找的数据比根节点的值大，那就在右子树中递归查找
                    参考：
                        profit.jikeshijian.shujujiegou.tree.BinarySearchTree.find

                2。二叉查找树的插入操作
                    思路：
                        1。新插入的数据一般都是在叶子节点上，所以我们只需要从根节点开始
                        2。依次比较要插入的数据和节点的大小关系
                    插入过程：
                        1。如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置
                        2。如果不为空，就再递归遍历右子树，查找插入位置
                        3。同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置
                        4。如果不为空，就再递归遍历左子树，查找插入位置。
                    参考：
                        profit.jikeshijian.shujujiegou.tree.BinarySearchTree.insert

                3。二叉查找树的删除操作
                    背景：
                        1。二叉查找树的查找、插入操作都比较简单易懂，但是它的删除操作就比较复杂了
                        2。针对要删除节点的子节点个数的不同，我们需要分三种情况来处理。
                    情形一：
                        1。如果要删除的节点没有子节点，我们只需要直接将父节点中
                        2。指向要删除节点的指针置为null。比如图中的删除节点55。
                    情形二：
                        1。如果要删除的节点只有一个子节点(只有左子节点或者右子节点)
                        2。我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了
                    情形三：
                        1。如果要删除的节点有两个子节点，这就比较复杂了
                        2。我们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。
                        3。然后再删除掉这个最小节点，因为最小节点肯定没有左子节点(如果有左子结点，那就不是最小节点了)
                        4。我们可以应用上面两条规则来删除这个最小节点
                    参考：
                        profit.jikeshijian.shujujiegou.tree.BinarySearchTree.delete

                4。二叉查找树的其他操作
                    概念：
                        二叉查找树中还可以支持快速地查找最大节点和最小节点、前驱节点和后继节点
                    查找最小节点参考：
                        profit.jikeshijian.shujujiegou.tree.BinarySearchTree.findMin
                    查找最大节点参考：
                        profit.jikeshijian.shujujiegou.tree.BinarySearchTree.findMax
    支持重复数据的二叉查找树
        背景
            1。前面讲二叉查找树的时候，我们默认树中节点存储的都是数字
            2。很多时候，在实际的软件开发中，我们在二叉查找树中存储的，是一个包含很多字段的对象
            3。我们利用对象的某个字段作为键值(key)来构建二叉查找树。我们把对象中的其他字段叫作卫星数据。
            4。前面我们讲的二叉查找树的操作，针对的都是不存在键值相同的情况
        问题：
            那如果存储的两个对象键值相同，这种情况该怎么处理呢?
            方法一：
                1。二叉查找树中每一个节点不仅会存储一个数据，
                2。因此我们通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上。
            方法二：
                插入操作
                    1。每个节点仍然只存储一个数据
                    2。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树
                    3。也就是说，把这个新插入的数据当作大于这个节点的值来处理。
                查找操作：
                    1。当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止
                    2。这样就可以把键值等于要查找值的所有节点都找出来。
                删除操作：
                    先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。

    二叉查找树的时间复杂度分析
        情形一：
            二叉查找树退化成了一个链表：
                查找的时间复杂度就变成了O(n)。
        情形二：
            二叉查找树是一个满二叉树
            分析：
                1。从我前面的例子、图，以及还有代码来看，不管操作是插入、删除还是查找
                2。时间复杂度其实都跟树的高度成正比，也就是O(height)。
            问题：
                现在问题就转变成另外一个了，也就是，如何求一棵包含n个节点的完全二叉树的高度?
            思路：
                1。树的高度就等于最大层数减一，为了方便计算，我们转换成层来表示
                2。从图中可以看出，包含n个节点的完全二叉树中，第一层包含1个节点，第二层包含2个节点，第三层包含4个节点
                3。依次类推，下面一层节点个数是上一层的2倍，第K层包含的节点个数就是2^(K-1)。
                4。不过，对于完全二叉树来说，最后一层的节点个数有点儿不遵守上面的规律了
                5。它包含的节点个数在1个到2^(L-1)个之间(我们假设最大层数是L)
                6。如果我们把每一层的节点个数加起来就是总的节点个数n
                7。也就是说，如果节点的个数是n，那么n满足这样一个关系:
                    n >= 1+2+4+8+...+2^(L-2)+1
                    n <= 1+2+4+8+...+2^(L-2)+2^(L-1)
            答案：
                L的范围是[log2(n+1), log2n +1]。完全二叉树的层数小于等于log2n +1，也就是说，完全二叉树的高度小于等于log2n。
        扩展：
            1。极度不平衡的二叉查找树，它的查找性能肯定不能满足我们的需求。
            2。我们需要构建一种不管怎么删除、插入数据，在任何时候，都能保持任意节点左右子树都比较平衡的二叉查找树
            3。平衡二叉查找树的高度接近logn，所以插入、删除、查找操作的时间复杂度也比较稳定，是O(logn)。
    思考题：
        背景
            1。散列表的插入、删除、查找操作的时间复杂度可以做到常量级的O(1)，非常高效
            2。二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是O(logn)
        问题
            相对散列表，好像并没有什么优势，那我们为什么还要用二叉查找树呢?
        原因：
            散列表：
                1。数据是无序存储的，如果要输出有序的数据，需要先进行排序
                2。扩容耗时很多，而且当遇到散列冲突时，性能不稳定
                3。查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比logn小，
                    所以实际的查找速度可能不一定比O(logn)快，加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高
                4。散列表的构造比二叉查找树要复杂，需要考虑的东西很多。
                    例子：
                        比如散列函数的设计、冲突解决办法、扩容、缩容等
            二叉查找树：
                1。只需要中序遍历，就可以在O(n)的时间复杂度内，输出有序的数据序列。
                2。二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在O(logn)。
                3。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。

25|红黑树(上):为什么工程中都用红黑树这种二叉树?
    平衡二叉查找树
        概念：
            1。二叉树中任意一个节点的左右子树的高度相差不能大于1。
            2。完全二叉树、满二叉树其实都是平衡二叉树
            3。非完全二叉树也有可能是平衡二叉树。
        目的：
            解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题
        类别：
            1。Splay Tree(伸展树)
            2。Treap(树堆)
            3。红黑树：它是一种不严格的平衡二叉查找树
    红黑树：
        概念
            红黑树中的节点，一类被标记为黑色，一类被标记为红色。
        要求：
            1。根节点是黑色的;
            2。每个叶子节点都是黑色的空节点(NIL)，也就是说，叶子节点不存储数据;
            3。任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的;
            4。每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点;
        问题：
            为什么说红黑树是“近似平衡”的?
            背景：
                1。平衡二叉查找树的初衷，是为了解决二叉查找树因为动态更新导致的性能退化问题。
                    1。“平衡”的意思可以等价为性能不退化
                    2。“近似平衡”就等价为性能不会退化的太严重
                2。二叉查找树很多操作的性能都跟树的高度成正比。
                3。一棵极其平衡的二叉树(满二叉树或完全二叉树)的高度大约是log2n
                4。如果要证明红黑树是近似平衡的，我们只需要分析，红黑树的高度是否比较稳定地趋近log2n就好了。
            红黑树的高度分析：
                去掉红色节点之后二叉树的高度分析
                    思路：
                        1。如果我们将红色节点从红黑树中去掉，那单纯包含黑色节点的红黑树的高度是多少呢?
                        2。红色节点删除之后，有些节点就没有父节点了，它们会直接拿这些节点的祖父节点(父节点的父节点)作为父节点，之前的二叉树就变成了四叉树
                    分析
                        1。从任意节点到可达的叶子节点的每个路径包含相同数目的黑色节点
                        2。我们从四叉树中取出某些节点，放到叶节点位置，四叉树就变成了完全二叉树。
                        3。仅包含黑色节点的四叉树的高度，比包含相同节点个数的完全二叉树的高度还要小。
                        推出：
                            去掉红色节点的“黑树”的高度也不会超过log2n。
                            原因：
                                完全二叉树的高度近似log2n，这里的四叉“黑树”的高度要低于完全二叉树
                红色节点加回去的高度分析
                    分析：
                        1。在红黑树中，红色节点不能相邻，也就是说，有一个红色节点就要至少有一个黑色节点，将它跟其他红色节点隔开
                        2。红黑树中包含最多黑色节点的路径不会超过log2n
                        3。加入红色节点之后，最长路径不会超过2log2n，也就是说，红黑树的高度近似2log2n
                结论：
                    红黑树的高度只比高度平衡的AVL树的高度(log2n)仅仅大了一倍，在性能上，下降得并不多
    思考题：
        为什么在工程中大家都喜欢用红黑树这种平衡二叉查找树?
        Treap、Splay Tree：
            优点：
                操作的效率都很高
            缺点：
                无法避免极端情况下时间复杂度的退化。
            应用：
                尽管这种情况出现的概率不大，但是对于单次操作时间非常敏感的场景来说，它们并不适用。
        AVL树：
            概念：
                一种高度平衡的二叉树
            优点：
                查找的效率非常高
            缺点：
                A为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时
            应用：
                对于有频繁的插入、删除操作的数据集合，使用AVL树的代价就有点高了。

        红黑树：
            优点：
                1。做到了近似平衡，并不是严格的平衡，所以在维护平衡的成本上，要比AVL树要低。
                2。红黑树的插入、删除、查找各种操作性能都比较稳定。
            应用：
                对于工程应用来说，要面对各种异常情况，为了支撑这种工业级的应用，我们更倾向于这种性能稳定的平衡二叉查找树。

26|红黑树(下):掌握这些技巧，你也可以实现一个红黑树
    红黑树的平衡过程
        思路：
            1。遇到什么样的节点排布，我们就对应怎么去调整
            2。只要按照这些固定的调整规则来操作，就能将一个非平衡的红黑树调整成平衡的。
        一棵合格的红黑树需要满足这样几个要求:
            1。根节点是黑色的;
            2。每个叶子节点都是黑色的空节点(NIL)，也就是说，叶子节点不存储数据;
            3。任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的;
            4。每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点。
        分析：
            1。在插入、删除节点的过程中，第三、第四点要求可能会被破坏
            2。而我们今天要讲的“平衡调整”，实际上就是要把被破坏的第三、第四点恢复过来。
        两个重要的操作：
            1。左旋(rotate left)
                概念：
                    其实是叫围绕某个节点的左旋

            2。右旋(rotate right)
                概念：
                    围绕某个节点的右旋。

    插入操作的平衡调整
        概念：
            1。红黑树规定，插入的节点必须是红色的。而且，二叉查找树中新插入的节点都是放在叶子节点上。
            2。关于插入操作的平衡调整，有这样两种特殊情况，但是也都非常好处理。
        情形一：
            如果插入节点的父节点是黑色的，那我们什么都不用做，它仍然满足红黑树的定义。
        情形二：
            如果插入的节点是根节点，那我们直接改变它的颜色，把它变成黑色就可以了。
        关注点：
            概念：
                1。把正在处理的节点叫作关注节点。
                2。关注节点会随着不停地迭代处理，而不断发生变化。
                3。最开始的关注节点就是新插入的节点。
        如果红黑树的平衡被打破，那一般会有下面三种情况
            case1：
                如果关注节点是a，它的叔叔节点d是红色，我们就依次执行下面的操作:
                    1。将关注节点a的父节点b、叔叔节点d的颜色都设置成黑色;
                    2。将关注节点a的祖父节点c的颜色设置成红色;
                    3。关注节点变成a的祖父节点c;
                    4。跳到CASE 2或者CASE 3。
                    参考：
                        com/suixingpay/profit/document/数据结构/图片/第.pn26讲红黑树插入情况一g
            case2：
                如果关注节点是a，它的叔叔节点d是黑色，关注节点a是其父节点b的右子节点，我们就依次执行下面的操作:
                1。关注节点变成节点a的父节点b;
                2。围绕新的关注节点b左旋;
                3。跳到CASE 3。
                参考：
                    com/suixingpay/profit/document/数据结构/图片/第26讲红黑树插入情况二.png
            case3：
                如果关注节点是a，它的叔叔节点d是黑色，关注节点a是其父节点b的左子节点，我们就依次执行下面的操作:
                1。围绕关注节点a的祖父节点c右旋;
                2。将关注节点a的父节点b、兄弟节点c的颜色互换。
                3。调整结束。
                参考：
                    com/suixingpay/profit/document/数据结构/图片/第26讲红黑树插入情形三.png

    删除操作的平衡调整
        分为两步：
            1。第一步是针对删除节点初步调整。
                目的：
                    1。保证整棵红黑树在一个节点删除之后，仍然满足最后一条定义的要求
                    2。每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点
                case1：
                    如果要删除的节点是a，它只有一个子节点b，那我们就依次进行下面的操作:
                        1。删除节点a，并且把节点b替换到节点a的位置，这一部分操作跟普通的二叉查找树的删除操作一样;
                        2。节点a只能是黑色，节点b也只能是红色，其他情况均不符合红黑树的定义。这种情况下，我们把节点b改为黑色;
                        3。调整结束，不需要进行二次调整。
                        参考：
                            com/suixingpay/profit/document/数据结构/图片/第26讲针对删除节点初步调整情形一.png
                case2：
                    如果要删除的节点a有两个非空子节点，并且它的后继节点就是节点a的右子节点c。我们就依次进行下面的操作:
                        1。如果节点a的后继节点就是右子节点c，那右子节点c肯定没有左子树。我们把节点a删除，并且将节点c替换到节点a的位置。这一部分操作跟普通的二叉查找树的删除操作无异;
                        2。然后把节点c的颜色设置为跟节点a相同的颜色;
                        3。如果节点c是黑色，为了不违反红黑树的最后一条定义，我们给节点c的右子节点d多加一个黑色，这个时候节点d就成了“红-黑”或者“黑-黑”;
                        4。这个时候，关注节点变成了节点d，第二步的调整操作就会针对关注节点来做。
                        参考：
                            com/suixingpay/profit/document/数据结构/图片/第26讲删除节点初步调整情形二.png
                case3：
                    如果要删除的是节点a，它有两个非空子节点，并且节点a的后继节点不是右子节点，我们就依次进行下面的操作:
                        1。找到后继节点d，并将它删除，删除后继节点d的过程参照CASE 1;
                        2。将节点a替换成后继节点d;
                        3。把节点d的颜色设置为跟节点a相同的颜色;
                        4。如果节点d是黑色，为了不违反红黑树的最后一条定义，我们给节点d的右子节点c多加一个黑色，这个时候节点c就成了“红-黑”或者“黑-黑”;
                        5。这个时候，关注节点变成了节点c，第二步的调整操作就会针对关注节点来做。
                        参考：
                            com/suixingpay/profit/document/数据结构/图片/第26讲删除节点初步调整情形三.png

            2。针对关注节点进行二次调整，让它满足红黑树的第三条定义，即不存在相邻的两个红色节点。
                四种情况：
                    CASE 1：
                        如果关注节点是a，它的兄弟节点c是红色的，我们就依次进行下面的操作:
                            1。围绕关注节点a的父节点b左旋;
                            2。关注节点a的父节点b和祖父节点c交换颜色
                            3。关注节点不变;
                            4。继续从四种情况中选择适合的规则来调整。
                        参考：
                            com/suixingpay/profit/document/数据结构/图片/第26讲关注节点删除进行二次调整情形一.png
                    CASE 2:
                        如果关注节点是a，它的兄弟节点c是黑色的，并且节点c的左右子节点d、e都是黑色的，我们就依次进行下面的操作:
                            1。将关注节点a的兄弟节点c的颜色变成红色;
                            2。从关注节点a中去掉一个黑色，这个时候节点a就是单纯的红色或者黑色;
                            3。给关注节点a的父节点b添加一个黑色，这个时候节点b就变成了“红-黑”或者“黑-黑”;
                            4。关注节点从a变成其父节点b;
                            5。继续从四种情况中选择符合的规则来调整。
                        参考：
                            com/suixingpay/profit/document/数据结构/图片/第26讲关注节点进行二次删除调整情形二.png
                    CASE 3：
                        如果关注节点是a，它的兄弟节点c是黑色，c的左子节点d是红色，c的右子节点e是黑色，我们就依次进行下面的操作:
                            1。围绕关注节点a的兄弟节点c右旋;
                            2。节点c和节点d交换颜色;
                            3。关注节点不变;
                            4。跳转到CASE 4，继续调整。
                        参考：
                            com/suixingpay/profit/document/数据结构/图片/第26讲关注节点进行二次调整删除情形三.png
                    CASE 4:
                        如果关注节点a的兄弟节点c是黑色的，并且c的右子节点是红色的，我们就依次进行下面的操作:
                            1。围绕关注节点a的父节点b左旋;
                            2。将关注节点a的兄弟节点c的颜色，跟关注节点a的父节点b设置成相同的颜色;
                            3。将关注节点a的父节点b的颜色设置为黑色;
                            4。从关注节点a中去掉一个黑色，节点a就变成了单纯的红色或者黑色;
                            5。将关注节点a的叔叔节点e设置为黑色;
                            6。调整结束。
                        参考：
                            com/suixingpay/profit/document/数据结构/图片/第26讲关注节点进行二次调整删除情形四.png

27|递归树:如何借助树来求解递归算法的时间复杂度?
    递归树与时间复杂度分析
        思想：
            1。将大问题分解为小问题来求解，然后再将小问题分解为小小问题
            2。这样一层一层地分解，直到问题的数据规模被分解得足够小，不用继续递归分解为止。
    递归树：
        概念：
            把这个一层一层的分解过程画成图，它其实就是一棵树
    斐波那契数列的递归树
        参考：
            com/suixingpay/profit/document/数据结构/图片/第27讲斐波那契数列的递归树.png
    归并排序画成递归树：
        参考：
            com/suixingpay/profit/document/数据结构/图片/第27讲归并排序递归树.png
        时间复杂度分析：
            1。每次分解都是一分为二，所以代价很低，我们把时间上的消耗记作常量
            2。归并算法中比较耗时的是归并操作，也就是把两个子数组合并为大数组。
            3。每一层归并操作消耗的时间总和是一样的，跟要排序的数据规模有关。我们把每一层归并操作消耗的时间记作$n$。
            4。我们只需要知道这棵树的高度$h$，用高度$h$乘以每一层的时间消耗$n$，就可以得到总的时间复杂度$O(n*h)$。
        高度分析：
            1。归并排序递归树是一棵满二叉树
            2。满二叉树的高度大约是$\log_{2}n$，所以，归并排序递归实现 的时间复杂度就是$O(n\log n)$
            注意：
                这里的时间复杂度都是估算的，对树的高度的计算也没有那么精确，但是这并不影响复杂度的计算结果。
    实战一：
        分析快速排序的时间复杂度
        参考：
            com/suixingpay/profit/document/数据结构/图片/第27讲快速排序递归树.png
        时间复杂度分析：
            1。快速排序的过程中，每次分区都要遍历待分区区间的所有数据，所以，每一层分区操作所遍历的数据的个数之和就是$n$
            2。我们现在只要求出递归树的高度$h$，这个快排过程遍历的数据个数就是 $h * n$ ，也就是说，时间复杂度就是$O(h * n)$。
        高度分析：
            参考27讲
    实战二：
        分析斐波那契数列的时间复杂度
        参考27讲
    实战三：
        分析全排列的时间复杂度
        参考27讲

28|堆和堆排序:为什么说堆排序没有快速排序快?
    堆：
        概念：
            1。堆是一种特殊的树
        要求：
            1。堆是一个完全二叉树;
                概念：
                    1。除了最后一层，其他层的节点个数都是满的，
                    2。最后一层的节点都靠左排列。
            2。堆中每一个节点的值都必须大于等于(或小于等于)其子树中每个节点的值。
                类似：
                    堆中每个节点的值都大于等于(或者小于等于)其左右子节点的值。
        大顶堆：
            概念：
                每个节点的值都大于等于子树中每个节点值的堆
        小顶堆：
            概念：
                每个节点的值都小于等于子树中每个节点值的堆
    如何实现一个堆?
        思路：
            要实现一个堆，我们先要知道，堆都支持哪些操作以及如何存储一个堆
        存储方法：
            完全二叉树适合用数组来存储
            优点：
                完全二叉树非常节省存储空间的。
                原因：
                    不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。
                    针对的是下标为$i$的节点
                        左子节点：
                            下标为$i*2$的节点
                        右子节点：
                            下标为$i*2+1$的节点
                        父节点：
                            下标为$\frac{i}{2}$的节点。
    核心操作：
        1。往堆中插入一个元素
            背景：
                往堆中插入一个元素后，我们需要继续满足堆的两个特性
            思考：
                如果我们把新插入的元素放到堆的最后，你可以看我画的这个图，是不是不符合堆的特性了?
                答案：
                    不符合
                方法：
                    堆化
                    概念：
                        需要进行调整，让其重新满足堆的特性的过程
            两种堆化：
                一种是从下往上的堆化方法：
                    流程：
                        顺着节点所在的路径，向上或者向下，对比，然后交换。
                    具体实现
                        1。让新插入的节点与父节点对比大小
                        2。如果不满足子节点小于等于父节点的大小关系，我们就互换两个节点。
                        3。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。
                    堆化的过程分解图：
                        代码参考：
                            com/suixingpay/profit/document/数据结构/图片/第28讲从下往上的堆化方法.png

                一种是从上往下的堆化方法：
                参考代码：
                    profit.jikeshijian.shujujiegou.heap.Heap28.removeMax
        2。删除堆顶元素
            背景：
                堆顶元素存储的就是堆中数据的最大值或者最小值。
            方法一：
                1。假设我们构造的是大顶堆，堆顶元素就是最大的元素
                2。当我们删除堆顶元素之后，就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中
                3。然后我们再迭代地删除第二大节点，以此类推，直到叶子节点被删除。
                参考：
                    com/suixingpay/profit/document/数据结构/图片/第28讲删除堆顶元素方法一.png
                缺点：
                    1。不过这种方法有点问题，就是最后堆化出来的堆并不满足完全二叉树的特性。
                    2。数组空洞
            方法二：
                1。我们把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。
                2。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程
                3。直到父子节点之间满足大小关系为止。这就是从上往下的堆化方法。
                原因：
                    1。我们移除的是数组中的最后一个元素，而在堆化的过程中，都是交换操作，不会出现数组中的“空洞”
                    2。这种方法堆化之后的结果，肯定满足完全二叉树的特性。
                参考：
                    com/suixingpay/profit/document/数据结构/图片/第28讲删除堆顶元素方法二.png
                代码参考
                    profit.jikeshijian.shujujiegou.heap.Heap28.removeMax
                说明：
                    1。一个包含$n$个节点的完全二叉树，树的高度不会超过$\log_{2}n$。
                    2。堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是$O(\log n)$
                    3。插入数据和删除堆顶元素的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是$O(\log n)$。

    如何基于堆实现排序?
        背景：
            1。有时间复杂度是$O(n^{2})$的冒泡排序、插入排序、选择排序
            2。有时间复杂度是$O(n\log n)$的归并排序、快速排序，还有线性排序。
        堆排序：
            概念：
                1。借助于堆这种数据结构实现的排序算法
                2。时间复杂度非常稳定，是$O(n\log n)$，并且它还是原地排序算法。
        堆排序的过程大致分解成两个大的步骤，建堆和排序。
                1。建堆
                    两种思路：
                        思路一：
                            过程
                                1。在堆中插入一个元素的思路
                                2。尽管数组中包含$n$个数据，但是我们可以假设，起初堆中只包含一个数据，就是下标为$1$的数据
                                3。我们调用前面讲的插入操作，将下标从$2$到$n$的数据依次插入到堆中
                                4。这样我们就将包含$n$个数据的数组，组织成了堆
                            汇总：
                                从前往后处理数组数据，并且每个数据插入堆中时，都是从下往上堆化。
                        思路二：
                            1。是从后往前处理数组，并且每个数据都是从上往下堆化。
                            参考：
                                建堆分解步骤图
                                1。com/suixingpay/profit/document/数据结构/图片/第28讲建堆从上往下堆化图片一.png
                                2。com/suixingpay/profit/document/数据结构/图片/第28讲建堆从上往下堆化图片二.png
                            思路：
                                因为叶子节点往下堆化只能自己跟自己比较，所以我们直接从第一个非叶子节点开始，依次堆化就行了。
                            参考代码：
                                profit.jikeshijian.shujujiegou.heap.HeapSort28.buildHeap
                            特征：
                                1。对下标从$\frac{n}{2}$ 开始到$1$的数据进行堆化
                                2。下标是$\frac{n}{2}+1$到$n$的节点是叶子节点，我们不需要堆化
                            扩展：
                                对于完全二叉树来说，下标从$\frac{n}{2}+1$到$n$的节点都是叶子节点。
                    时间复杂度：
                        分析：
                            每个节点堆化的时间复杂度是$O(\log n)$，
                        问题：
                            那$\frac{n}{2}+1$个节点堆化的总时间复杂度是不是就是$O(n\log n)$呢?
                            答案：
                                答案虽然也没错，但是这个值还是不够精确。堆排序的建堆过程的时间复杂度是$O(n)$。
                        推导过程
                            1。因为叶子节点不需要堆化，所以需要堆化的节点从倒数第二层开始。
                            2。每个节点堆化的过程中，需要比较和交换的节点个数，跟这个节点的高度$k$成正比。
                            3。我把每一层的节点个数和对应的高度画了出来，你可以看看
                            4。我们只需要将每个节点的高度求和，得出的就是建堆的时间复杂度。
                        参考：
                            1。com/suixingpay/profit/document/数据结构/图片/第28讲建堆的时间复杂度.png
                            2。com/suixingpay/profit/document/数据结构/图片/第28讲建堆的时间复杂度计算过程.png
                2。排序
                    分析：
                        1。建堆结束之后，数组中的数据已经是按照大顶堆的特性来组织的
                        2。数组中的第一个元素就是堆顶，也就是最大的元素。
                    思路：
                        我们把它跟最后一个元素交换，那最大元素就放到了下标为$n$的位置。
                    过程：
                        1。有点类似上面讲的“删除堆顶元素”的操作
                        2。当堆顶元素移除之后，我们把下标为$n$的元素放到堆顶，然后再通过堆化的方法，将剩下的$n-1$个元素重构建成堆
                        3。堆化完成之后，我们再取堆顶的元素，放到下标是$n-1$的位置，一直重复这个过程
                        4。直到最后堆中只剩下标为$1$的一个元素，排序工作就完成了
                    参考：
                        com/suixingpay/profit/document/数据结构/图片/第28讲堆的排序过程.png
                    参考代码：
                        profit.jikeshijian.shujujiegou.heap.HeapSort28.sort

        空间复杂度：
            整个堆排序的过程，都只需要极个别临时存储空间，所以堆排序是原地排序算法
        时间复杂度：
            1。建堆过程的时间复杂度是$O(n)$
            2。排序过程的时间复杂度是$O(n\log n)$
            3。所以，堆排序整体的时间复杂度是$O(n\log n)$。
        稳定性：
            1。堆排序不是稳定的排序算法，因为在排序的过程
            2。存在将堆的最后一个节点跟堆顶节点互换的操作，所以就有可能改变值相同数据的原始相对顺序。

29|堆的应用:如何快速获取到Top10最热门的搜索关键词?
    堆的应用一:优先级队列
        优先级队列：
            概念：
                1。首先应该是一个队列，队列最大的特性就是先进先出
                2。在优先级队列中，数据的出队顺序不是先进先出，而是按照优先级来，优先级最高的，最先出队。
            问题：
                如何实现一个优先级队列呢?
                方法：
                    堆来实现是最直接、最高效的
                原因：
                    1。堆和优先级队列非常相似，一个堆就可以看作一个优先级队列。
                        很多时候，它们只是概念上的区分而已
                    2。往优先级队列中插入一个元素，就相当于往堆中插入一个元素
                    3。从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。
            应用：
                很多数据结构和算法都要依赖它。比如，赫夫曼编码、图的最短路径、最小生成树算法等等。
                例子一：
                    合并有序小文件
                    具体内容：
                        1。假设我们有100个小文件，每个文件的大小是100MB
                        2。每个文件中存储的都是有序的字符串。
                        3。我们希望将这些100个小文件合并成一个有序的大文件
                    方法：
                        优先级队列。
                        思路：
                            1。有点像归并排序中的合并函数。我们从这100个文件中，各取第一个字符串，放入数组中，然后比较大小
                            2。把最小的那个字符串放入合并后的大文件中，并从数组中删除。
                        过程：
                            1。假设，这个最小的字符串来自于13.txt这个小文件
                            2。我们就再从这个小文件取下一个字符串，并且放到数组中，重新比较大小
                            3。并且选择最小的放入合并后的大文件，并且将它从数组中删除
                            4。依次类推，直到所有的文件中的数据都放入到大文件为止。
                        数据结构：
                            方法一：
                                这里我们用数组这种数据结构，来存储从小文件中取出来的字符串
                                缺点：
                                    每次从数组中取最小字符串，都需要循环遍历整个数组，显然，这不是很高效。
                            方法二：(优化)
                                1。到优先级队列，也可以说是堆
                                2。我们将从小文件中取出来的字符串放入到小顶堆中，那堆顶的元素
                                3。也就是优先级队列队首的元素，就是最小的字符串
                                4。我们将这个字符串放入到大文件中，并将其从堆中删除
                                5。然后再从小文件中取出下一个字符串，放入到堆中。
                                6。循环这个过程，就可以将100个小文件中的数据依次放入到大文件中。
                            分析：
                                删除堆顶数据和往堆中插入数据的时间复杂度都是O(logn)，n表示堆中的数据个数，这里就是100
                例子二：
                    高性能定时器
                    具体内容：
                        1。假设我们有一个定时器，定时器中维护了很多定时任务，每个任务都设定了一个要触发执行的时间点
                        2。定时器每过一个很小的单位时间(比如1秒)，就扫描一遍任务，看是否有任务到达设定的执行时间
                        3。如果到达了，就拿出来执行。
                    分析：
                        这样每过1秒就扫描一遍任务列表的做法比较低效
                        原因：
                            1。任务的约定执行时间离当前时间可能还有很久，这样前面很多次扫描其实都是徒劳的
                            2。每次都要扫描整个任务列表，如果任务列表很大的话，势必会比较耗时。
                        优化方法：
                            优先级队列来解决
                            思路：
                                我们按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部(也就是小顶堆的堆顶)存储的最先执行的任务。
                            优点：
                                1。定时器就不需要每隔1秒就扫描一遍任务列表了。
                                2。定时器既不用间隔1秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。
                            过程：
                                1。它拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔T。
                                2。这个时间间隔T就是，从当前时间开始，需要等待多久，才会有第一个任务需要被执行
                                3。这样，定时器就可以设定在T秒之后，再来执行任务。
                                4。从当前时间点到(T-1)秒这段时间里，定时器都不需要做任何事情。
                                5。当T秒时间过去之后，定时器取优先级队列中队首的任务执行
                                6。然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为定时器执行下一个任务需要等待的时间。

    堆的应用二:利用堆求Top K
        背景：
            求Top K的问题抽象成两类
        1。静态数据
            概念：
                数据集合事先确定，不会再变
            问题：
                如何在一个包含n个数据的数组中，查找前K大数据呢?
            方法：
                1。维护一个大小为K的小顶堆，顺序遍历数组，从数组中取出取数据与堆顶元素比较
                2。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中
                3。如果比堆顶元素小，则不做处理，继续遍历数组
            现象：
                这样等数组中的数据都遍历完之后，堆中的数据就是前K大数据了。
            时间复杂度：
                1。遍历数组需要O(n)的时间复杂度，一次堆化操作需要O(logK)的时间复杂度
                2。所以最坏情况下，n个元素都入堆一次，所以时间复杂度就是O(nlogK)。
        2。动态数据
            概念：
                数据集合事先并不确定，有数据动态地加入到集合中。
            例子：
                一个数据集合中有两个操作，一个是添加数据，另一个询问当前的前K大数据
                分析：
                    1。如果每次询问前K大数据，我们都基于当前的数据重新计算的话，那时间复杂度就是O(nlogK)，n表示当前的数据的大小
                    2。实际上，我们可以一直都维护一个K大小的小顶堆，当有数据被添加到集合中时，我们就拿它与堆顶的元素对比
                    3。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中
                    4。如果比堆顶元素小，则不做处理。
                现象：
                    无论任何时候需要查询当前的前K大数据，我们都可以里立刻返回给他。

    堆的应用三:利用堆求中位数
        问题：
            如何求动态数据集合中的中位数。
        中位数：
            概念：
                处在中间位置的那个数
            奇数：
                把数据从小到大排列，那第$\frac{n}{2}+1$个数据就是中位数
            偶数：
                1。那处于中间位置的数据有两个，第$\frac{n}{2}$个和第$\frac{n}{2}+1$个数据，
                2。我们可以随意取一个作为中位数，比如取两个数中靠前的那个，就是第$\frac{n}{2}$个数据。
        静态数据：
            概念：
                1。中位数是固定的，我们可以先排序，第$\frac{n}{2}$个数据就是中位数
                2。每次询问中位数的时候，我们直接返回这个固定的值就好了
            优点：
                边际成本会很小
            缺点：
                排序的代价比较大
        动态数据：
            方法一：
                1。中位数在不停地变动，如果再用先排序的方法
                2。每次询问中位数的时候，都要先进行排序，那效率就不高了。
            方法二：
                借助堆这种数据结构，我们不用排序，就可以非常高效地实现求中位数操作
                具体做法：
                    1。需要维护两个堆，一个大顶堆，一个小顶堆。
                        大顶堆：
                            存储前半部分数据
                        小顶堆：
                            存储后半部分数据，且数据都大于大顶堆中的数据。
                    2。如果有n个数据，n是偶数，我们从小到大排序，那前$\frac{n}{2}$个数据存储在大顶堆中，后$\frac{n}{2}$个数据存储在小顶堆中。
                    3。大顶堆中的堆顶元素就是我们要找的中位数。
                    4。如果n是奇数，情况是类似的，大顶堆就存储$\frac{n}{2}+1$个数据，小顶堆中就存储$\frac{n}{2}$个数据。
                    参考：
                        com/suixingpay/profit/document/数据结构/图片/第29讲利用堆求中位数动态数据.png
                问题：
                    当新添加一个数据的时候，我们如何调整两个堆，让大顶堆中的堆顶元素继续是中位数呢?
                    答案：
                        1。如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆;
                        2。如果新加入的数据大于等于小顶堆的堆顶元素，我们就将这个新数据插入到小顶堆。
                    现象：
                        两个堆中的数据个数不符合前面约定的情况:
                        n是偶数：
                            两个堆中的数据个数都是$\frac{n}{2}$
                        n是奇数：
                            大顶堆有$\frac{n} {2}+1$个数据，小顶堆有$\frac{n}{2}$个数据。
                        方法：
                            可以从一个堆中不停地将堆顶元素移动到另一个堆，通过这样的调整，来让两个堆中的数据满足上面的约定。
                        参考：
                            com/suixingpay/profit/document/数据结构/图片/第29讲利用堆求中位数动态数据方法二.png
                时间复杂度：
                    1。插入数据因为需要涉及堆化，所以时间复杂度变成了O(logn)
                    2。但是求中位数我们只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是O(1)。

    堆的应用四:利用堆求中位数
        利用两个堆不仅可以快速求出中位数，还可以快速求其他百分位的数据，
        问题一：
            什么是“99%响应时间”。
                解释一：
                    1。中位数的概念就是将数据从小到大排列，处于中间位置，就叫中位数，这个数据会大于等于前面50%的数据。
                    2。99百分位数的概念可以类比中位数，如果将一组数据从小到大排列，这个99百分位数就是大于前面99%数据的那个数据。
                    例子：
                        假设有100个数据，分别是1，2，3，......，100，那99百分位数就是99，因为小于等于99的数占总个数的99%。
                解释二：
                    1。如果有100个接口访问请求，每个接口请求的响应时间都不同，比如55毫秒、100毫秒、23毫秒等，
                    2。我们把这100个接口的响应时间按照从小到大排列，排在第99的那个数据就是99%响应时间，也叫99百分位响应时间。
                总结：
                    1。如果有n个数据，将数据从小到大排列之后，99百分位数大约就是第n*99%个数据
                    2。80百分位数大约就是第n*80%个数据。
        问题二：
            如何求99%响应时间。
            答案：
                1。我们维护两个堆，一个大顶堆，一个小顶堆
                2。假设当前总数据的个数是n，大顶堆中保存n*99%个数据，小顶堆中保存n*1%个数据
                3。大顶堆堆顶的数据就是我们要找的99%响应时间。
            过程：
                1。每次插入一个数据的时候，我们要判断这个数据跟大顶堆和小顶堆堆顶数据的大小关系，然后决定插入到哪个堆中
                2。如果这个新插入的数据比大顶堆的堆顶数据小，那就插入大顶堆;
                3。如果这个新插入的数据比小顶堆的堆顶数据大，那就插入小顶堆。
            注意：
                1。为了保持大顶堆中的数据占99%，小顶堆中的数据占1%，在每次新插入数据之后，我们都要重新计算
                2。这个时候大顶堆和小顶堆中的数据个数，是否还符合99:1这个比例。
                3。如果不符合，我们就将一个堆中的数据移动到另一个堆，直到满足这个比例
            现象：
                1。通过这样的方法，每次插入数据，可能会涉及几个数据的堆化操作，所以时间复杂度是O(logn)
                2。每次求99%响应时间的时候，直接返回大顶堆中的堆顶数据即可，时间复杂度是O(1)。


30|图的表示:如何存储微博、微信等社交网络中的好友关系?
    图：
        概念：
            1。一种非线性表数据结构
            2。和树比起来，这是一种更加复杂的非线性表结构。
            顶点：
                树中的元素我们称为节点，图中的元素我们就叫作顶点(vertex)
            边：
                两个顶点建立的关系
        例子：
            1。拿微信举例子吧。我们可以把每个用户看作一个顶点。
            2。如果两个用户之间互加好友，那就在两者之间建立一条边。
            3。整个微信的好友关系就可以用一张图来表示
            顶点的度：
                每个用户有多少个好友，顶点相连接的边的条数。
        背景：
            1。微博的社交关系跟微信还有点不一样，或者说更加复杂一点
            2。微博允许单向关注，也就是说，用户A关注了用户B，但用户B可以不关注用户A。
        问题：
            如何用图来表示这种单向的社交关系呢?
        方法：
            有向图：(微博)
                概念：
                    边有方向的图
                例子：
                    1。如果用户A关注了用户B，我们就在图中画一条从A到B的带箭头的边，来表示边的方向。
                    2。如果用户A和用户B互相关注了，那我们就画一条从A指向B的边，再画一条从B指向A的边。
                顶点的入度：
                    概念：
                        表示有多少条边指向这个顶点
                    例子：
                        就表示有多少粉丝
                顶点的出度：
                    概念：
                        表示有多少条边是以这个顶点为起点指向其他顶点
                    例子：
                        就表示关注了多少人

            无向图:(微信)
                概念：
                    边没有方向的图
                度：
                    表示一个顶点有多少条边
            带权图:(QQ)
                概念：
                    每条边都有一个权重(weight)
                例子：
                    1。QQ不仅记录了用户之间的好友关系，还记录了两个用户之间的亲密度
                    2。如果两个用户经常往来，那亲密度就比较高;
                    3。如果不经常往来，亲密度就比较低

                应用：
                    我们可以通过这个权重来表示QQ好友间的亲密度。
    问题：
        如何在内存中存储图这种数据结构呢?
        方法：
            1。邻接矩阵存储方法
                设计思想：
                    存储起来比较浪费时间，使用起来比较节省时间
                原理：
                    底层依赖一个二维数组
                    无向图：
                        如果顶点i与顶点j之间有边，我们就将A[i][j]和A[j][i]标记为1;
                    有向图：
                        出度：
                            如果顶点i到顶点j之间，有一条箭头从顶点i指向顶点j的边，那我们就将A[i][j]标记为1。
                        入度：
                            如果有一条箭头从顶点j指向顶点i的边，我们就将A[j][i]标记为1。
                    带权图：
                        数组中就存储相应的权重。
                优点：
                    1。简单，直观
                        原因：
                            基于数组，所以在获取两个顶点的关系时，就非常高效
                    2。方便计算
                        原因：
                            用邻接矩阵的方式存储图，可以将很多图的运算转换成矩阵之间的运算
                        例子：
                            求解最短路径问题时会提到一个Floyd-Warshall算法，就是利用矩阵循环相乘若干次得到结果。

                缺点：
                    比较浪费存储空间
                    例子：
                        无向图：
                            1。如果A[i][j]等于1，那A[j][i]也肯定等于1，我们只需要存储一个就可以了。
                            2。无向图的二维数组中，如果我们将其用对角线划分为上下两部分
                            3。我们只需要利用上面或者下面这样一半的空间就足够了，另外一半白白浪费掉了。
                        稀疏图：
                            特点：
                                顶点很多，但每个顶点的边并不多，
                            现象
                                用邻接矩阵的存储方法就更加浪费空间了。
                            例子：
                                1。比如微信有好几亿的用户，对应到图上就是好几亿的顶点。
                                2。但是每个用户的好友并不会很多，一般也就三五百个而已
                                3。如果我们用邻接矩阵来存储，那绝大部分的存储空间都被浪费了。
                参考：
                    com/suixingpay/profit/document/数据结构/图片/第30讲邻接矩阵存储方法.png.png

            2。邻接表存储方法
                设计思想：
                    存储起来比较节省空间，但是使用起来就比较耗时间。
                原理：
                    1。每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点
                有向图：
                    每个顶点对应的链表里面，存储的是指向的顶点
                无向图：
                    每个顶点的链表中存储的，是跟这个顶点有边相连的顶点
                参考：
                    com/suixingpay/profit/document/数据结构/图片/第30讲邻接表存储方法.png
                缺点：
                    比起邻接矩阵的存储方式，在邻接表中查询两个顶点之间的关系就没那么高效了。
                    原因：
                        链表的存储方式对缓存不友好
                    例子：
                        如果我们要确定，是否存在一条从顶点2到顶点4的边，那我们就要遍历顶点2对应的那条链表，看链表中是否存在顶点4
                    思路：
                        1。在基于链表法解决冲突的散列表中，如果链过长，为了提高查找效率，
                        2。我们可以将链表换成其他更加高效的数据结构，比如平衡二叉查找树等
                    方法：
                        1。可以将邻接表同散列表一样进行“改进升级”。
                        2。可以将邻接表中的链表改成平衡二叉查找树。实际开发中，我们可以选择用红黑树。
                            优点：
                                我们就可以更加快速地查找两个顶点之间是否存在边了
                        优化一：
                            这里的二叉查找树可以换成其他动态数据结构，比如跳表、散列表等。
                        优化二：
                            我们还可以将链表改成有序动态数组，可以通过二分查找的方法来快速定位两个顶点之间否是存在边。
    思考题：
        如何存储微博、微信等社交网络中的好友关系?
        背景：
            数据结构是为算法服务的，所以具体选择哪种存储方法，与期望支持的操作有关系。
        分析：
            1。针对微博用户关系，假设我们需要支持下面这样几个操作:
                1。判断用户A是否关注了用户B;
                2。判断用户A是否是用户B的粉丝;
                3。用户A关注用户B;
                4。用户A取消关注用户B;
                5。根据用户名称的首字母排序，分页获取用户的粉丝列表;
                6。根据用户名称的首字母排序，分页获取用户的关注列表。
            2。关于如何存储一个图，前面我们讲到两种主要的存储方法，邻接矩阵和邻接表
                原因：
                    社交网络是一张稀疏图，使用邻接矩阵存储比较浪费存储空间。
                方法：
                    我们采用邻接表来存储。
            3。用一个邻接表来存储这种有向图是不够的
                原因：
                    1。查找某个用户关注了哪些用户非常容易
                    2。如果要想知道某个用户都被哪些用户关注了，也就是用户的粉丝列表，是非常困难的。
            4。我们需要一个逆邻接表。
                1。邻接表：
                    概念：
                        每个顶点的链表中，存储的就是这个顶点指向的顶点
                    作用：
                        1。存储了用户的关注关系
                        2。如果要查找某个用户关注了哪些用户，我们可以在邻接表中查找

                2。逆邻接表
                    概念：
                        每个顶点的链表中，存储的是指向这个顶点的顶点。
                    作用：
                        1。存储的是用户的被关注关系
                        2。如果要查找某个用户被哪些用户关注了，我们从逆邻接表中查找。
                参考：
                    com/suixingpay/profit/document/数据结构/图片/第30讲邻接表和逆邻接表.png
            5。基础的邻接表不适合快速判断两个用户之间的是否是关注和被关注的关系
                优化：
                    将邻接表中的链表改为支持快速查找的动态数据结构
                问题：
                    选择哪种动态数据结构呢?红黑树、跳表、有序动态数组还是散列表呢?
                答案：
                    跳表：
                    原因：
                        1。我们需要按照用户名称的首字母排序，分页来获取用户的粉丝列表或者关注列表
                        2。跳表插入、删除、查找都非常高效，时间复杂度是O(logn)，空间复杂度上稍高，是O(n)。
                        3。跳表中存储的数据本来就是有序的了，分页获取粉丝列表或关注列表，就非常高效。
                    情形一：
                        1。如果对于小规模的数据，比如社交网络中只有几万、几十万个用户，
                        2。我们可以将整个社交关系存储在内存中，上面的解决思路是没有问题的
                    情形二：
                        如果像微博那样有上亿的用户，数据规模太大，我们就无法全部存储在内存中了。这个时候该怎么办呢?
                        方法一：
                            1。我们可以通过哈希算法等数据分片方式，将邻接表存储在不同的机器上
                            2。我们在机器1上存储顶点1，2，3的邻接表，在机器2上，存储顶点4，5的邻接表。逆邻接表的处理方式也一样。
                            3。当要查询顶点与顶点关系的时候，我们就利用同样的哈希算法，先定位顶点所在的机器，然后再在相应的机器上查找。
                            参考：
                                com/suixingpay/profit/document/数据结构/图片/第30讲邻接表存储在不同的机器上.png
                        方法二：
                            1。利用外部存储(比如硬盘)，因为外部存储的存储空间要比内存会宽裕很多。数据库是我们经常用来持久化存储关系数据的，
                            具体：
                                为了高效地支持前面定义的操作，我们可以在表上建立多个索引，比如第一列、第二列，给这两列都建立索引。
                            参考：
                                com/suixingpay/profit/document/数据结构/图片/第30讲数据库的存储方式存储邻接表.png

31|深度和广度优先搜索:如何找出社交网络中的三度好友关系?
    “搜索”算法
        背景：
            1。算法是作用于具体数据结构之上的
            2。深度优先搜索算法和广度优先搜索算法都是基于“图”这种数据结构的
                原因：
                    图这种数据结构的表达能力很强，大部分涉及搜索的场景都可以抽象成“图”。
        图上的搜索算法：
            概念：
                在图中找出从一个顶点出发，到另一个顶点的路径
            优点：
                最“暴力”的深度优先、广度优先搜索，还有A*、IDA*等启发式搜索算法。

    广度优先搜索(BFS)
        概念：
            1。是一种“地毯式”层层推进的搜索策略，
            2。即先查找离起始顶点最近的，然后是次近的，依次往外搜索
        图片参考：
            com/suixingpay/profit/document/数据结构/图片/第31讲广度优先搜索示意图.png
        代码参考：
            profit.jikeshijian.shujujiegou.graph.BFS31.bfs
        时间复杂度：
            分析：
                情形一：
                        1。最坏情况下，终止顶点t离起始顶点s很远，需要遍历完整个图才能找到。
                        2。这个时候，每个顶点都要进出一遍队列，每个边也都会被访问一次，所以，广度优先搜索的时间复杂度是O(V+E)
                            V表示顶点的个数，E表示边的个数
                情形二：
                        1。对于一个连通图来说，也就是说一个图中的所有顶点都是连通的，E肯定要大于等于V-1
                        2。广度优先搜索的时间复杂度也可以简写为O(E)。
        空间复杂度：
            1。消耗主要在几个辅助变量visited数组、queue队列、prev数组上。
            2。这三个存储空间的大小都不会超过顶点的个数，所以空间复杂度是O(V)

    深度优先搜索(DFS)
        案例:(走迷宫)
            1。假设你站在迷宫的某个岔路口，然后想找到出口。
            2。你随意选择一个岔路口来走，走着走着发现走不通的时候，你就回退到上一个岔路口，重新选择一条路继续走，直到最终找到出口
        问题：
            如何在图中应用深度优先搜索，来找某个顶点到另一个顶点的路径。
        图片参考：
            com/suixingpay/profit/document/数据结构/图片/第31讲深度优先搜索.png
            说明：
                1。搜索的起始顶点是s，终止顶点是t，我们希望在图中寻找一条从顶点s到顶点t的路径。
                2。如果映射到迷宫那个例子，s就是你起始所在的位置，t就是出口。
                3。我用深度递归算法，把整个搜索的路径标记出来了。这里面实线箭头表示遍历，虚线箭头表示回退
                5。从图中我们可以看出，深度优先搜索找出来的路径，并不是顶点s到顶点t的最短路径。
        设计思想：
            回溯思想。这种思想解决问题的过程，非常适合用递归来实现
        代码参考：
            profit.jikeshijian.shujujiegou.graph.DFS31.dfs
        时间复杂度：
            每条边最多会被访问两次，一次是遍历，一次是回退。所以，图上的深度优先搜索算法的时间复杂度是O(E)，E表示边的个数
        空间复杂度：
            1。消耗内存主要是visited、prev数组和递归调用栈。
            2。visited、prev数组的大小跟顶点的个数V成正比，递归调用栈的最大深度不会超过顶点的个数，所以总的空间复杂度就是O(V)。
    思考题：
        如何找出社交网络中某个用户的三度好友关系?
        答案：
            1。社交网络可以用图来表示。
            2。这个问题就非常适合用图的广度优先搜索算法来解决，因为广度优先搜索是层层往外推进的。
        流程：
            1。首先，遍历与起始顶点最近的一层顶点，也就是用户的一度好友
            2。然后再遍历与用户距离的边数为2的顶点，也就是二度好友关系
            3。以及与用户距离的边数为3的顶点，也就是三度好友关系。
        扩展：
            稍加改造一下广度优先搜索代码，用一个数组来记录每个顶点与起始顶点的距离，非常容易就可以找出三度好友关系。

32|字符串匹配基础(上):如何借助哈希算法实现高效字符串匹配?
    BF算法
        概念：
            中文叫作暴力匹配算法，也叫朴素匹配算法。
        主串：
            例子：
                我们在字符串A中查找字符串B，那字符串A就是主串，主串的长度记作n
            n>m:
                原因：
                    在主串中查找模式串
        模式串：
            例子：
                我们在字符串A中查找字符串B，字符串B就是模式串，模式串的长度记作m。
        思想：
            在主串中，检查起始位置分别是0、1、2...n-m且长度为m的n-m+1个子串，看有没有跟模式串匹配的。
        优点：
            比较简单、好懂
        缺点：
            暴力，相应的性能也不高。
        参考：
            com/suixingpay/profit/document/数据结构/图片/第32讲字符串匹配BF算法.png
        时间复杂度：
            1。在极端情况下，比如主串是“aaaaa...aaaaaa”(省略号表示有很多重复的字符a)，模式串是“aaaaab”。
            2。我们每次都比对m个字符，要比对n-m+1次，所以，这种算法的最坏情况时间复杂度是O(n*m)。
        问题：
            BF算法的时间复杂度很高，是O(n*m)，它却是一个比较常用的字符串匹配算法。为什么这么说呢?
            原因：
                1。实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长
                    而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把m个字符都比对一下
                    大部分情况下，算法执行效率要比这个高很多。
                2。朴素字符串匹配算法思想简单，代码实现也非常简单

    RK算法
        思路：
            1。我们通过哈希算法对主串中的n-m+1个子串分别求哈希值
            2。然后逐个与模式串的哈希值比较大小。
            3。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了
            原因：
                哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了。
            分析：
                1。通过哈希算法计算子串的哈希值的时候，我们需要遍历子串中的每个字符。
                2。尽管模式串与子串比较的效率提高了，算法整体的效率并没有提高。
            问题：
                有没有方法可以提高哈希算法计算子串哈希值的效率呢?
                方法：
                    1。假设要匹配的字符串的字符集中只包含K个字符，我们可以用一个K进制数来表示一个子串
                    2。这个K进制数转化成十进制数，作为子串的哈希值。
                例子：
                    1。比如要处理的字符串只包含a~z这26个小写字母，那我们就用二十六进制来表示一个字符串
                    2。我们把a~z这26个字符映射到0~25这26个数字，a就表示0，b就表示1，以此类推，z表示25。
                    十进制：
                        概念：
                            一个数字的值是通过下面的方式计算出来的
                        例子：
                            "657"=6*10*10+5*10+7*1
                    二十六进制：
                        概念：
                            一个包含a到z这26个字符的字符串，计算哈希的时候，我们只需要把进位从10改成26就可以。
                        例子：
                        "cba"="c"*26*26+"b"*26+"a"*1
                             =2*26*26+1*26+0*1
                             =1353
                            说明：
                                我们用二十六进制来表示一个字符串，对应的哈希值就是二十六进制数转化成十进制的结果。
                        规律：
                            相邻两个子串s[i-1]和s[i](i表示子串在主串中的起始位置，子串的长度都为m)，对应的哈希值计算公式有交集
                        优点：
                            我们可以使用s[i-1]的哈希值很快的计算出s[i]的哈希值。如果用公式表示的话，就是下面这个样子:
                        参考：
                            com/suixingpay/profit/document/数据结构/图片/第32讲字符串匹配RK算法二十六进制.png

        时间复杂度：
            两部份：
                1。计算子串哈希值
                    通过设计特殊的哈希算法，只需要扫描一遍主串就能计算出所有子串的哈希值了，所以这部分的时间复杂度是O(n)。
                2。模式串哈希值与子串哈希值之间的比较
                    1。模式串哈希值与每个子串哈希值之间的时间复杂度是O(1)，
                    2。总共需要比较n-m+1个子串的哈希值，所以，这部分的时间复杂度也是O(n)。
        问题：
            1。模式串很长，相应的主串中的子串也会很长，通过上面的哈希算法计算得到的哈希值就可能很大
            2。如果超过了计算机中整型数据可以表示的范围，那该如何解决呢?
            例子：
                假设字符串中只包含a~z这26个英文字母，那我们每个字母对应一个数字，比如a对应1，b对应2，以此类推，z对应26。
                方法：
                    可以把字符串中每个字母对应的数字相加，最后得到的和作为哈希值。
                优点：
                    这种哈希算法产生的哈希值的数据范围就相对要小很多了。
                缺点：
                    哈希冲突概率也是挺高的。
                优化：
                    比如将每一个字母从小到大对应一个素数，而不是1，2，3......这样的自然数，这样冲突的概率就会降低一些。
                新的问题：
                    1。之前我们只需要比较一下模式串和子串的哈希值，如果两个值相等，那这个子串就一定可以匹配模式串
                    2。当存在哈希冲突的时候，有可能存在这样的情况，子串和模式串的哈希值虽然是相同的，但是两者本身并不匹配。
                    方法：
                        当我们发现一个子串的哈希值跟模式串的哈希值相等的时候，我们只需要再对比一下子串和模式串本身就好了。
        注意：
            1。哈希算法的冲突概率要相对控制得低一些，如果存在大量冲突，就会导致RK算法的时间复杂度退化，效率下降
            2。极端情况下，如果存在大量的冲突，每次都要再对比子串和模式串本身，那时间复杂度就会退化成O(n*m)

33|字符串匹配基础(中):如何实现文本编辑器中的查找功能?
    BM算法的核心思想
        背景：
            1。当遇到不匹配的字符时，BF算法和RK算法的做法是，模式串往后滑动一位，
            2。然后从模式串的第一个字符开始重新匹配。
        思考：
            1。当遇到不匹配的字符时，有什么固定的规律，可以将模式串往后多滑动几位呢?
            2。这样一次性往后滑动好几位，那匹配的效率岂不是就提高了?
    BM算法原理分析
        1。坏字符规则(bad character rule)
            背景：
                1。BF算法和RK算法在匹配的过程中，我们都是按模式串的下标从小到大的顺序，依次与主串中的字符进行匹配的
                2。这种匹配顺序比较符合我们的思维习惯
                3。BM算法的匹配顺序比较特别，它是按照模式串下标从大到小的顺序，倒着匹配的
            参考：
                com/suixingpay/profit/document/数据结构/图片/第33讲字符串匹配BM算法坏字符规则下标从到小匹配.png
                具体流程参考33讲
            具体实现：
                1。当发生不匹配的时候，我们把坏字符对应的模式串中的字符下标记作si
                2。如果坏字符在模式串中存在，我们把这个坏字符在模式串中的下标记作xi。
                3。如果不存在，我们把xi记作-1。那模式串往后移动的位数就等于si-xi。
            注意：
                1。如果坏字符在模式串里多处出现，那我们在计算xi的时候，选择最靠后的那个
                    原因：
                        这样不会让模式串滑动过多，导致本来可能匹配的情况被滑动略过。
                2。单纯使用坏字符规则还是不够的。
                    原因：
                        1。根据si-xi计算出来的移动位数，有可能是负数，比如主串是aaaaaaaaaaaaaaaa，模式串是baaa
                        2。不但不会向后滑动模式串，还有可能倒退。所以，BM算法还需要用到“好后缀规则”。
            时间复杂度：
                BM算法在最好情况下的时间复杂度非常低，是O(n/m)。
                例子：
                    1。比如，主串是aaabaaabaaabaaab，模式串是aaaa。
                    2。每次比对，模式串都可以直接后移四位，所以，匹配具有类似特点的模式串和主串的时候，BM算法非常高效。

        2。好后缀规则(good suffix shift)
            案例：
                1。当模式串滑动到图中的位置的时候，模式串和主串有2个字符是匹配的
                2。倒数第3个字符发生了不匹配的情况。
            问题：
                这个时候如何滑动模式串？
            答案：
                利用坏字符规则计算模式串的滑动位数，使用好后缀处理规则。
            分析：
                1。把已经匹配的bc叫作好后缀，记作{u}
                2。拿它在模式串中查找，如果找到了另一个跟{u}相匹配的子串{u*}，那我们就将模式串滑动到子串{u*}与主串中{u}对齐的位置。
                3。如果在模式串中找不到另一个等于{u}的子串，我们就直接将模式串，滑动到主串中{u}的后面
                    原因：
                        之前的任何一次往后滑动，都没有匹配主串中{u}的情况。
                问题：
                    当模式串中不存在等于{u}的子串时，我们直接将模式串滑动到主串{u}的后面。这样做是否有点太过头呢?
                    现象：
                        1。这里面bc是好后缀，尽管在模式串中没有另外一个相匹配的子串{u*}
                        2。如果我们将模式串移动到好后缀的后面，那就会错过模式串和主串可以匹配的情况。
                        3。如果好后缀在模式串中不存在可匹配的子串，那在我们一步一步往后滑动模式串的过程中
                        4。只要主串中的{u}与模式串有重合，那肯定就无法完全匹配。
                        5。但是当模式串滑动到前缀与主串中{u}的后缀有部分重合的时候，并且重合的部分相等的时候，就有可能会存在完全匹配的情况。
                    方法：
                        1。我们不仅要看好后缀在模式串中，是否有另一个匹配的子串
                        2。我们还要考察好后缀的后缀子串，是否存在跟模式串的前缀子串匹配的。
                        3。所谓某个字符串s的后缀子串，就是最后一个字符跟s对齐的子串
                    例子：
                        1。abc的后缀子串就包括c,bc。所谓前缀子串，就是起始字符跟s对齐的子串，比如abc的前缀子串有a，ab
                        2。我们从好后缀的后缀子串中，找一个最长的并且能跟模式串的前缀子串匹配的
                        3。假设是{v}，然后将模式串滑动到如图所示的位置。
            问题：
                当模式串和主串中的某个字符不匹配的时候，如何选择用好后缀规则还是坏字符规则，来计算模式串往后滑动的位数?
    BM算法代码实现
        坏字符规则：
            问题：
                1。当遇到坏字符时，要计算往后移动的位数si-xi，其中xi的计算是重点，我们如何求得xi呢
                2。或者说，如何查找坏字符在模式串中出现
            方法一：
                在模式串中顺序遍历查找
                缺点：
                    比较低效，势必影响这个算法的性能
            方法二：
                将模式串中的每个字符及其下标都存到散列表中。
                例子：
                    1。假设字符串的字符集不是很大，每个字符长度是1字节，我们用大小为256的数组
                    2。来记录每个字符在模式串中出现的位置。数组的下标对应字符的ASCII码值，数组中存储这个字符在模式串中出现的位置。
                优点：
                    可以快速找到坏字符在模式串的位置下标了。
            参考代码：
                profit.jikeshijian.shujujiegou.String.BM32.generateBC
        好后缀规则：
            好后缀的处理规则中最核心的内容:
                1。在模式串中，查找跟好后缀匹配的另一个子串
                2。在好后缀的后缀子串中，查找最长的、能跟模式串前缀子串匹配的后缀子串;
            方法一：
                用很“暴力”的匹配查找方式解决
                缺点：
                    低效
            方法二：
                思路：
                    1。因为好后缀也是模式串本身的后缀子串，所以，我们可以在模式串和主串正式匹配之前
                    2。通过预处理模式串，预先计算好模式串的每个后缀子串，对应的另一个可匹配子串的位置
                问题：
                    如何表示模式串中不同的后缀子串呢?
                分析：
                    1。后缀子串的最后一个字符的位置是固定的，下标为m-1，我们只需要记录长度就可以了
                    2。通过长度，我们可以确定一个唯一的后缀子串。
                    3。引入最关键的变量suffix数组。
                        1。suffix数组的下标k，表示后缀子串的长度
                        2。下标对应的数组值存储的是，在模式串中跟好后缀{u}相匹配的子串{u*}的起始下标值
                    问题：
                        如果模式串中有多个(大于1个)子串跟后缀子串{u}匹配，那suffix数组中该存储哪一个子串的起始位置呢?
                        方法：
                            1。为了避免模式串往后滑动得过头了
                            2。我们肯定要存储模式串中最靠后的那个子串的起始位置，也就是下标最大的那个子串的起始位置。
                        疑问：
                            这样处理就足够了吗?
                            背景：
                                1。查找跟好后缀匹配的另一个子串
                                2。还要在好后缀的后缀子串中，查找最长的能跟模式串前缀子串匹配的后缀子串。
                            分析：
                                如果我们只记录刚刚定义的suffix，实际上，只能处理规则的前半部分，也就是，在模式串中，查找跟好后缀匹配的另一个子串。
                            方法：
                                除了suffix数组之外，我们还需要另外一个boolean类型的prefix数组，来记录模式串的后缀子串是否能匹配模式串的前缀子串。
                            问题：
                                如何来计算并填充这两个数组的值?这个计算过程非常巧妙。
                                计算过程：
                                    1。我们拿下标从0到i的子串(i可以是0到m-2)与整个模式串，求公共后缀子串
                                    2。如果公共后缀子串的长度是k，那我们就记录suffix[k]=j(j表示公共后缀子串的起始下标)
                                    3。如果j等于0，也就是说，公共后缀子串也是模式串的前缀子串，我们就记录prefix[k]=true。
                                参考代码：
                                    profit.jikeshijian.shujujiegou.String.BM32.generateGS
        完整代码参考：
            profit.jikeshijian.shujujiegou.String.BM32.bm
    BM算法的性能分析及优化
        空间复杂度：
            分析：
                1。分析BM算法的内存消耗，整个算法用到了额外的3个数组
                2。其中bc数组的大小跟字符集大小有关，suffix数组和prefix数组的大小跟模式串长度m有关
                3。单纯使用好后缀规则的BM算法效率就会下降一些了。
                    原因：
                        1。因为好后缀和坏字符规则是独立的，如果我们运行的环境对内存要求苛刻
                        2。可以只使用好后缀规则，不使用坏字符规则，这样就可以避免bc数组过多的内存消耗。
        时间复杂度：
            1。我前面讲的BM算法是个初级版本。为了让你能更容易理解，有些复杂的优化我没有讲。
            2。基于我目前讲的这个版本，在极端情况下，预处理计算suffix数组、prefix数组的性能会比较差。
        例子：
            比如模式串是aaaaaaa这种包含很多重复的字符的模式串，预处理的时间复杂度就是O(m^2)

34|字符串匹配基础(下):如何借助BM算法轻松理解KMP算法?
    KMP算法基本原理
        背景
            1。我们假设主串是a，模式串是b。在模式串与主串匹配的过程中，当遇到不可匹配的字符的时候
            2。希望找到一些规律，可以将模式串往后多滑动几位，跳过那些肯定不会匹配的情况。
        坏字符：
            在模式串和主串匹配的过程中，把不能匹配的那个字符
            情形：
                当遇到坏字符的时候，我们就要把模式串往后滑动，在滑动的过程中，只要模式串和好前缀有上下重合
        好前缀：
            把已经匹配的那段字符串
            情形：
                前面几个字符的比较，就相当于拿好前缀的后缀子串，跟模式串的前缀子串在比较
        规律：
            1。在模式串和主串匹配的过程中，当遇到坏字符后，对于已经比对过的好前缀
            2。能否找到一种规律，将模式串一次性滑动很多位?
        分析：
            1。我们只需要拿好前缀本身，在它的后缀子串中，查找最长的那个可以跟好前缀的前缀子串匹配的
            2。假设最长的可匹配的那部分前缀子串是{v}，长度是k
            3。把模式串一次性往后滑动j-k位，相当于，每次遇到坏字符的时候，我们就把j更新为k，i不变，然后继续比较。
        表述方便：
            最长可匹配后缀子串：
                把好前缀的所有后缀子串中，最长的可匹配前缀子串的那个后缀子串
            最长可匹配前缀子串：
                对应的前缀子串
        问题：
            如何来求好前缀的最长可匹配前缀和后缀子串呢?
            答案：
                这个问题其实不涉及主串，只需要通过模式串本身就能求解。
            思路：
                1。能不能事先预处理计算好，在模式串和主串匹配的过程中，直接拿过来就用呢?
                2。类似BM算法中的bc、suffix、prefix数组
                3。KMP算法也可以提前构建一个数组，用来存储模式串中每个前缀(这些前缀都有可能是好前缀)的最长可匹配前缀子串的结尾字符下标
                4。我们把这个数组定义为next数组，很多书中还给这个数组起了一个名字，叫失效函数(failure function)。
            具体：
                数组的下标是每个前缀结尾字符下标，数组的值是这个前缀的最长可以匹配前缀子串的结尾字符下标
            参考代码：
                profit.jikeshijian.shujujiegou.String.BM32.kmp
    失效函数计算方法：
        问题：
            也就是next数组是如何计算出来的?
        方法一：
            1。比如要计算下面这个模式串b的next[4]，我们就把b[0, 4]的所有后缀子串，从长到短找出来
            2。依次看看，是否能跟模式串的前缀子串匹配
            缺点：
                效率非常低。
        方法二：
            思想：
                类似于动态规划；
            具体流程：
                1。我们按照下标从小到大，依次计算next数组的值
                2。当我们要计算next[i]的时候，前面的next[0]，next[1]，......，next[i-1]应该已经计算出来了
            问题：
                利用已经计算出来的next值，我们是否可以快速推导出next[i]的值呢?
            情形一：
                1。如果next[i-1]=k-1，也就是说，子串b[0, k-1]是b[0, i-1]的最长可匹配前缀子串
                2。如果子串b[0, k-1]的下一个字符b[k]，与b[0, i-1]的下一个字符b[i]匹配，那子串b[0,k]就是b[0, i]的最长可匹配前缀子串。
                3。所以，next[i]等于k。
            情形二：
                如果b[0, k-1]的下一字符b[k]跟b[0, i-1]的下一个字符b[i]不相等呢?
                答案：
                    这个时候就不能简单地通 过next[i-1]得到next[i]了。
                分析：
                    1。我们假设b[0, i]的最长可匹配后缀子串是b[r, i]
                    2。如果我们把最后一个字符去掉，那b[r, i-1]肯定是b[0, i-1]的可匹配后缀子串，但不一定是最长可匹配后缀子串
                    3。既然b[0, i-1]最长可匹配后缀子串对应的模式串的前缀子串的下一个字符并不等于b[i]
                    4。那么我们就可以考察b[0, i-1]的次长可匹配后缀子串b[x, i-1]对应的可匹配前缀子串b[0, i-1-x]的下一个字符b[i-x]是否等于b[i]。
                    5。如果等于，那b[x, i]就是b[0, i]的最长可匹配后缀子串。
                    问题：
                        如何求得b[0, i-1]的次长可匹配后缀子串呢?
                    分析：
                        1。次长可匹配后缀子串肯定被包含在最长可匹配后缀子串中
                        2。而最长可匹配后缀子串又对应最长可匹配前缀子串b[0, y]。
                        3。查找b[0, i-1]的次长可匹配后缀子串，这个问题就变成，查找b[0, y]的最长匹配后缀子串的问题了。
                        4。我们可以考察完所有的b[0, i-1]的可匹配后缀子串b[y, i-1]，直到找到一个可匹配的后缀子串，
                        5。它对应的前缀子串的下一个字符等于b[i]，那这个b[y, i]就是b[0, i]的最长可匹配后缀子串。
                参考代码
                    profit.jikeshijian.shujujiegou.String.BM32.getNexts

    KMP算法复杂度分析：
        时间复杂度：
            1。KMP算法包含两部分
                1。第一部分是构建next数组
                    思路一：
                        1。计算next数组的代码中，第一层for循环中i从1到m-1，也就是说，内部的代码被执行了m-1次。
                        2。for循环内部代码有一个while循环，如果我们能知道每次for循环、while循环平均执行的次数
                        3。假设是k，那时间复杂度就是O(k*m)。
                        缺点：
                            while循环执行的次数不怎么好统计，所以我们放弃这种分析方法。
                    思路二：
                        1。我们可以找一些参照变量，i和k。i从1开始一直增加到m，而k并不是每次for循环都会增加，所以，k累积增加的值肯定小于m
                        2。而while循环里k=next[k]，实际上是在减小k的值，k累积都没有增加超过m
                        3。所以while循环里面k=next[k]总的执行次数也不可能超过m。因此，next数组计算的时间复杂度是O(m)。

                2。第二部分才是借助next数组匹配。
                    分析：
                        1。i从0循环增长到n-1，j的增长量不可能超过i，所以肯定小于n
                        2。而while循环中的那条语句j=next[j-1]+1，不会让j增长的，那有没有可能让j不变呢?
                        3。也没有可能。因为next[j-1]的值肯定小于j-1，所以while循环中的这条语句实际上也是在让j的值减少
                        4。而j总共增长的量都不会超过n，那减少的量也不可能超过n
                        5。所以while循环中 的这条语句总的执行次数也不会超过n，所以这部分的时间复杂度是O(n)。
                3。综合两部分的时间复杂度，KMP算法的时间复杂度就是O(m+n)。

        空间复杂度：
            KMP算法只需要一个额外的next数组，数组的大小跟模式串相同。所以空间复杂度是O(m)，m表示模式串的长度
35|Trie树:如何实现搜索引擎的搜索关键词提示功能?
    什么是“Trie树”?
        概念：
            1。是一个树形结构
            2。是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。
        本质：
            利用字符串之间的公共前缀，将重复的前缀合并在一起
        参考：
            com/suixingpay/profit/document/数据结构/图片/第35讲Trie树单词创建过程.png
    如何实现一棵Trie树?
        两个操作：
            1。一个是将字符串集合构造成Trie树
                概念：
                    这个过程分解开来的话，就是一个将字符串插入到Trie树的过程
            2。另一个是在Trie树中查询一个字符串。
        问题：
            如何存储一个Trie树?
            答案：
            1。Trie树是一个多叉树。我们知道，二叉树中，一个节点的左右子节点是通过两个指针来存储的
                示例代码：
                    class BinaryTreeNode { 
                        char data;
                        BinaryTreeNode left;
                        BinaryTreeNode right;
                 }
        存储方式一：
            1。借助散列表的思想，我们通过一个下标与字符一一映射的数组，来存储子节点的指针
            2。假设我们的字符串中只有从a到z这26个小写字母，我们在数组中下标为0的位置
            3。存储指向子节点a的指针，下标为1的位置存储指向子节点b的指针
            4。以此类推，下标为25的位置，存储的是指向的子节点z的指针。
            5。如果某个字符的子节点不存在，我们就在对应的下标的位置存储null。
            参考代码
                profit.jikeshijian.shujujiegou.String.Tie35
        时间复杂度：
            1。如果要在一组字符串中，频繁地查询某些字符串，用Trie树会非常高效。
            2。构建Trie树的过程，需要扫描所有的字符串，时间复杂度是O(n)(n表示所有字符串的长度和)
            优点：
                但是一旦构建成功之后，后续的查询操作会非常高效。
            例子：
                1。每次查询时，如果要查询的字符串长度是k，那我们只需要比对大约k个节点，就能完成查询操作。
                2。跟原本那组字符串的长度和个数没有任何关系
                3。构建好Trie树后，在其中查找字符串的时间复杂度是O(k)，k表示要查找的字符串的长度。
    Trie树真的很耗内存吗?
        问题：
            “Trie树是非常耗内存的，用的是一种空间换时间的思路”这是什么原因呢?
        案例：
            1。用数组来存储一个节点的子节点的指针
            2。如果字符串中包含从a到z这26个字符，那每个节点都要存储一个长度为26的数组，并且每个数组存储一个8字节指针
                (或者是4字节，这个大小跟CPU、操作系统、编译器等有关)
            3。即便一个节点只有很少的子节点，远小于26个，比如3、4个，我们也要维护一个长度为26的数组。
        分析：
            1。Trie树的本质是避免重复存储一组字符串的相同前缀子串，但是现在每个字符(对应一个节点)的存储远远大于1个字节。
            2。数组长度为26，每个元素是8字节，那每个节点就会额外需要26*8=208个字节。而且这还是只包含26个字符的情况。
            情形一：
                如果字符串中不仅包含小写字母，还包含大写字母、数字、甚至是中文，那需要的存储空间就更多了
            情形二：
                在某些情况下，Trie树不一定会节省存储空间
                例子：
                    在重复的前缀并不多的情况下，Trie树不但不能节省内存，还有可能会浪费更多的内存。
        优点：
            非常高效
        缺点：
            有可能很浪费内存
            思路：
                1。可以稍微牺牲一点查询的效率，将每个节点中的数组换成其他数据结构，来存储一个节点的子节点指针。
                    问题：
                        用哪种数据结构呢?
                    答案：
                        比如有序数组、跳表、散列表、红黑树等。
                        例子：
                            有序数组
                                1。数组中的指针按照所指向的子节点中的字符的大小顺序排列
                                2。查询的时候，我们可以通过二分查找的方法，快速查找到某个字符应该匹配的子节点的指针
                                3。在往Trie树中插入一个字符串的时候，我们为了维护数组中数据的有序性，就会稍微慢了点。
        2。Trie树的变体，可以在一定程度上解决内存消耗的问题
        例子
            1。缩点优化，就是对只有一个子节点的节点，而且此节点不是一个串的结束节点
            2。可以将此节点与子节点合并。这样可以节省空间，但却增加了编码难度。

    Trie树与散列表、红黑树的比较
        注意
            1。在一组字符串中查找字符串，Trie树实际上表现得并不好。它对要处理的字符串有及其严苛的要求。
            2。对要处理的字符串有及其严苛的要求。
                1。字符串中包含的字符集不能太大。如果字符集太大，那存储空间可能就会浪费很多。即便可以优化，但也要付出牺牲查询、插入效率的代价。
                2。要求字符串的前缀重合比较多，不然空间消耗会变大很多
                3。如果要用Trie树解决问题，那我们就要自己从零开始实现一个Trie树，还要保证没有bug，这个在工程上是将简单问题复杂化，除非必须，一般不建议这样做。
                4。我们知道，通过指针串起来的数据块是不连续的，而Trie树中用到了指针，所以，对缓存并不友好，性能上会打个折扣。

        结论：
            针对在一组字符串中查找字符串的问题，我们在工程中，更倾向于用散列表或者红黑树
            原因：
                我们都不需要自己去实现，直接利用编程语言中提供的现成类库就行了。
        应用：
            1。Trie树只是不适合精确匹配查找，这种问题更适合用散列表或者红黑树来解决。
            2。Trie树比较适合的是查找前缀匹配的字符串，也就是类似开篇问题的那种场景。

36|AC自动机:如何用多模式串匹配实现敏感词过滤功能?
    背景：
        单模式匹配算法：
            概念：
                是在一个模式串和一个主串之间进行匹配，在一个主串中查找一个模式串。
            例子：
                KMP算法
        多模式串匹配算法：
            概念：
                在多个模式串和一个主串之间做匹配，在一个主串中查找多个模式串。
            优点：
                在主串中一次性查找多个模式串是否存在，从而大大提高匹配效率。
            例子：
                Trie树结构
    经典的多模式串匹配算法:AC自动机
    AC自动机
        概念：
            1。Trie树跟AC自动机之间的关系，就像单串匹配中朴素的串匹配算法，跟KMP算法之间的关系一样，前者针对的是多模式串而已
            3。AC自动机实际上就是在Trie树之上，加了类似KMP的next数组，只不过此处的next数组是构建在树上罢了。
        参考：
            profit.jikeshijian.shujujiegou.String.AcNode36

        操作：
            1。将多个模式串构建成Trie树;
            2。在Trie树上构建失败指针(相当于KMP中的失效函数next数组)。
        失败指针：
            概念：
                1。Trie树每个节点都有一个失败指针；
                2。将p节点的失败指针指向那个最长匹配后缀子串对应的模式串的前缀的最后一个节点，就是下图中箭头指向的节点。
                3。如果我们把树中相同的深度的节点放大同一层，那么每个节点失败指针只有可能出现在它所在层的上一层
            情形一：
                1。假设我们沿Trie树走到p节点，也就是下图中的紫色节点，那p的失败指针就是从root走到紫色节点形成的字符串abc
                2。跟所有模式串前缀匹配的最长可匹配后缀子串，就是箭头指的bc模式串。
                最长可匹配后缀子串：
                    例子：
                        1。字符串abc的后缀子串有两个bc，c，我们拿它们与其他模式串匹配
                        2。如果某个后缀子串可以匹配某个模式串的前缀，那我们就把这个后缀子串叫作可匹配后缀子串。
                        3。我们从可匹配后缀子串中，找出最长的一个，就是刚刚讲到的最长可匹配后缀子串
            思路：
                1。我们可以像KMP算法那样，当我们要求某个节点的失败指针的时候，我们通过已经求得的、深度更小的那些节点的失败指针来推导
                2。我们可以逐层依次来求解每个节点的失败指针。所以，失败指针的构建过程，是一个按层遍历树的过程。
            过程：
                首先root的失败指针为NULL，也就是指向自己。当我们已经求得某个节点p的失败指针之后，
            情形一：
                1。我们假设节点p的失败指针指向节点q，我们看节点p的子节点pc对应的字符，是否也可以在节点q的子节点中找到
                2。如果找到了节点q的一个子节点qc，对应的字符跟节点pc对应的字符相同，则将节点pc的失败指针指向节点qc。
            情形二：
                1。如果节点q中没有子节点的字符等于节点pc包含的字符，则令q=q->fail
                2。继续上面的查找，直到q是root为止，如果还没有找到相同字符的子节点，就让节点pc的失败指针指向root。
        问题：
            如何在AC自动机上匹配主串?
            答案：
                1。在匹配过程中，主串从i=0开始，AC自动机从指针p=root开始，假设模式串是b，主串是a。
                2。如果p指向的节点有一个等于b[i]的子节点x，我们就更新p指向x，这个时候我们需要通过失败指针，检测一系列失败指针为结尾的路径是否是模式串
                3。处理完之后，我们将i加一，继续这两个过程;
                4。如果p指向的节点没有等于b[i]的子节点，那失败指针就派上用场了，我们让p=p->fail，然后继续这2个过程。
            参考：
                profit.jikeshijian.shujujiegou.String.AcNode36.match
    参考：
        https://www.cnblogs.com/hyfhaha/p/10802604.html
    时间复杂度：
        1。Trie树构建的时间复杂度是O(m*len)，其中len表示敏感词的平均长度，m表示敏感词的个数
        2。假设Trie树中总的节点个数是k，每个节点构建失败指针的时候，(你可以看下代码)最耗时的环节是while循环中的q=q->fail
        3。每运行一次这个语句，q指向节点的深度都会减少1，而树的高度最高也不会超过len
        4。所以每个节点构建失败指针的时间复杂度是O(len)。整个失败指针的构建过程就是O(k*len)。

37|贪心算法:如何用贪心算法实现Huffman压缩编码?
    贪心算法
        例子：
            内容
                1。假设我们有一个可以容纳100kg物品的背包，可以装各种物品。
                2。我们有以下5种豆子，每种豆子的总量和总价值都各不相同。
            问题：
                为了让背包中所装物品的总价值最大，我们如何选择在背包中装哪些豆子?每种豆子又该装多少呢?
            参考：
                com/suixingpay/profit/document/数据结构/图片/第37讲贪心算法.png
            分析：
                1。我们只要先算一算每个物品的单价，按照单价由高到低依次来装就好了
                2。单价从高到低排列，依次是:黑豆、绿豆、红豆、青豆、黄豆，
                3。所以，我们可以往背包里装20kg黑豆、30kg绿豆、50kg红豆
            步骤：
                第一步：
                    贪心算法
                    概念：
                        针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。、
                        限制值：重量不能超过100kg
                        期望值：物品的总价值
                        这组数据：5种豆子
                        从中选择一部分：满足重量不超过100kg
                        期望值最大：总价值最大。
                第二不：
                    1。尝试看下这个问题是否可以用贪心算法解决：
                    思路：
                        每次选择当前情况下，在对限制值同等贡献量的情况下，对期望值贡献最大的数据
                            限制值同等贡献量：重量相同
                            期望值贡献最大的数据：对价值贡献最大的豆子
                第三步：
                    举几个例子看下贪心算法看下是不是最优的
                    注意：
                        1。严格地证明贪心算法的正确性，是非常复杂的，需要涉及比较多的数学推理
                        2。从实践的角度来说，大部分能用贪心算法解决的问题
                        3。贪心算法的正确性都是显而易见的，也不需要严格的数学推导证明。
                        4。用贪心算法解决问题的思路，并不总能给出最优解。
                        例子：
                            1。在一个有权图中，我们从顶点S开始，找一条到顶点T的最短路径(路径中边的权值和最小)
                            2。贪心算法的解决思路是，每次都选择一条跟当前顶点相连的权最小的边，直到找到顶点T
                            3。按照这种思路，我们求出的最短路径是S->A->E->T，路径长度是1+4+4=9。
                            答案：
                                最终求的路径并不是最短路径，因为路径S->B->D->T才是最短路径，因为这条路径的长度是2+2+2=6
                            原因：
                                1。前面的选择，会影响后面的选择
                                2。如果我们第一步从顶点S走到顶点A，那接下来面对的顶点和边，跟第一步从顶点S走到顶点B，是完全不同的。
                                3。即便我们第一步选择最优的走法(边最短)，但有可能因为这一步选择，导致后面每一步的选择都很糟糕，最终也就无缘全局最优解了。
                            参考：
                                com/suixingpay/profit/document/数据结构/图片/第37讲贪心算法有权图求解最短路径的问题.png

    贪心算法实战分析
        例子一：分糖果
            内容：
                1。我们有m个糖果和n个孩子
                2。我们现在要把糖果分给这些孩子吃，但是糖果少，孩子多(m<n)，所以糖果只能分配给一部分孩子。
                3。每个糖果的大小不等，这m个糖果的大小分别是s1，s2，s3，......，sm。
                4。每个孩子对糖果大小的需求也是不一样的，只有糖果的大小大于等于孩子的对糖果大小的需求的时候，孩子才得到满足。
                5。假设这n个孩子对糖果大小的需求分别是g1，g2，g3，......，gn。
            问题：
                如何分配糖果，能尽可能满足最多数量的孩子?
                问题抽象：
                    从n个孩子中，抽取一部分孩子分配糖果，让满足的孩子的个数(期望值)是最大的。
                    限制值：糖果个数
                    期望值：孩子的个数
                方法：
                    贪心算法来解决
                过程：
                    1。对于一个孩子来说，如果小的糖果可以满足，我们就没必要用更大的糖果，这样更大的就可以留给其他对糖果大小需求更大的孩子。
                    2。对糖果大小需求小的孩子更容易被满足，所以，我们可以从需求小的孩子开始分配糖果
                        原因：
                            满足一个需求大的孩子跟满足一个需求小的孩子，对我们期望值的贡献是一样的。
                    3。我们每次从剩下的孩子中，找出对糖果大小需求最小的，然后发给他剩下的糖果中能满足他的最小的糖果
                    4。满足的孩子个数最多的方案。

        例子二：钱币找零
            内容：
                1。假设我们有1元、2元、5元、10元、20元、50元、100元这些面额的纸币
                2。它们的张数分别是c1、c2、c5、c10、c20、c50、c100。
            问题：
                我们现在要用这些钱来支付K元，最少要用多少张纸币呢?
            方法：
                先用面值最大的来支付，如果不够，就继续用更小一点面值的，以此类推，最后剩下的用1元来补齐。
            分析：
                在贡献相同期望值(纸币数目)的情况下，我们希望多贡献点金额，这样就可以让纸币数更少，这就是一种贪心算法的解决思路

        例子三：区间覆盖
            内容：
                假设我们有n个区间，区间的起始端点和结束端点分别是[l1, r1]，[l2, r2]，[l3, r3]，......，[ln, rn]
            问题：
                我们从这n个区间中选出一部分区间，这部分区间满足两两不相交(端点相交的情况不算相交)，最多能选出多少个区间呢?
            思路：
                1。我们假设这n个区间中最左端点是lmin，最右端点是rmax。
                2。我们选择几个不相交的区间，从左到右将[lmin,rmax]覆盖上
                3。我们按照起始端点从小到大的顺序对这n个区间排序
            方法：
                1。我们每次选择的时候，左端点跟前面的已经覆盖的区间不重合的，右端点又尽量小的
                2。这样可以让剩下的未覆盖区间尽可能的大，就可以放置更多的区间
                com/suixingpay/profit/document/数据结构/图片/第37讲贪心算法有权图求解最短路径的问题.png
    思考题：
        参考第37讲

38|分治算法:谈一谈大规模计算框架MapReduce中的分治思想
    分治算法
        概念：
            1。核心思想是分而治之
            2。将原问题划分成n个规模较小，并且结构与原问题相似的子问题，
            3。递归地解决这些子问题，然后再合并其结果，就得到原问题的解。
            分治算法：一种处理问题的思想，一般都比较适合用递归来实现
            递归：一种编程技巧
        递归实现中，每一层递归都会涉及这样三个操作:
            1。分解：将原问题分解成一系列子问题;
            2。解决：递归地求解各个子问题，若子问题足够小，则直接求解;
            3。合并：将子问题的结果合并成原问题。
        分治算法能解决的问题，一般需要满足下面这几个条件:
            1。原问题与分解成的小问题具有相同的模式;
            2。原问题分解成的子问题可以独立求解，子问题之间没有相关性
            3。具有分解终止条件，也就是说，当问题足够小时，可以直接求解;
            4。可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。
    分治算法应用举例分析
        例子一：
            如何编程求出一组数据的有序对个数或者逆序对个数呢?
            方法一：
                1。拿每个数字跟它后面的数字比较，看有几个比它小的
                2。我们把比它小的数字个数记作k，通过这样的方式，把每个数字都考察一遍之后
                3。然后对每个数字对应的k值求和，最后得到的总和就是逆序对个数
                时间复杂度是O(n^2)。
            方法二：
                分治算法
                    1。我们可以将数组分成前后两半A1和A2，分别计算A1和A2的逆序对个数K1和K2
                    2。然后再计算A1与A2之间的逆序对个数K3。那数组A的逆序对个数就等于K1+K2+K3。
                    问题：
                        如何快速计算出两个子问题A1与A2之间的逆序对个数呢?
                    答案：
                        1。归并排序中有一个非常关键的操作，就是将两个有序的小数组，合并成一个有序的数组
                        2。在这个合并的过程中，我们就可以计算这两个小数组的逆序对个数了
                        3。每次合并操作，我们都计算逆序对个数，把这些计算出来的逆序对个数求和，就是这个数组的逆序对个数了。
                参考代码;
                    profit.jikeshijian.shujujiegou.String.MergeSortCounting38.count
        例子二：
            1。给10GB的订单文件按照金额排序这样一个需求，看似是一个简单的排序问题，但是因为数据量大，有10GB
            2。而我们的机器的内存可能只有2、3GB这样子
            3。无法一次性加载到内存，也就无法通过单纯地使用快排、归并等基础算法来解决了。
            思路：
                要解决这种数据量大到内存装不下的问题，我们就可以利用分治的思想
            方法：
                1。可以将海量的数据集合根据某种方法，划分为几个小的数据集合
                2。每个小的数据集合单独加载到内存来解决，然后再将小数据集合合并成大数据集合。
                3。利用这种分治的处理思路，不仅仅能克服内存的限制，还能利用多线程或者多机处理，加快处理的速度。
            具体实现：
                1。给10GB的订单排序，我们就可以先扫描一遍订单，根据订单的金额，将10GB的文件划分为几个金额区间
                2。比如订单金额为1到100元的放到一个小文件，101到200之间的放到另一个文件，以此类推。
                3。这样每个小文件都可以单独加载到内存排序，最后将这些有序的小文件合并，就是最终有序的10GB订单数据了。
        例子三：
            MapReduce的本质就是分治思想
            1。那如果我们要处理的数据是1T、10T、100T这样子的，那一台机器处理的效率肯定是非常低的。
            2。而对于谷歌搜索引擎来说，网页爬取、清洗、分析、分词、计算权重、倒排索引等等各个环节中
            3。都会面对如此海量的数据(比如网页)。
            方法：
                利用集群并行处理显然是大势所趋。
            思路：
                1。一台机器过于低效，那我们就把任务拆分到多台机器上来处理
                2。如果拆分之后的小任务之间互不干扰，独立计算，最后再将结果合并
            注意：
                1。MapReduce框架只是一个任务调度器，底层依赖GFS来存储数据，依赖Borg管理机器
                2。它从GFS中拿数据，交给Borg中的机器执行，并且时刻监控机器执行的进度
                3。一旦出现机器宕机、进度卡壳等，就重新从Borg中调度一台机器执行。
            应用：
                1。用来处理这种数据与数据之间存在关系的任务
                2。统计文件中单词出现的频率
                3。对网页分析、分词等，每个网页可以独立的分析、分词

39|回溯算法:从电影《蝴蝶效应》中学习回溯算法的核心思想
    回溯算法：
        概念：
            1。有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。
            2。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段
            3。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候(不符合期望的解)，就回退到上一个岔路口，另选一种走法继续走。
    例子一：八皇后问题
        内容：
            我们有一个8x8的棋盘，希望往里放8个棋子(皇后)，每个棋子所在的行、列、对角线都不能有另一个棋子
        思路：
            1。我们把这个问题划分成8个阶段，依次将8个棋子放到第一行、第二行、第三行......第八行。
            2。在放置的过程中，我们不停地检查当前的方法，是否满足要求。
            3。如果满足，则跳到下一行继续放置棋子;如果不满足，那就再换一种方法，继续尝试。
        参考：
            profit.jikeshijian.shujujiegou.String.Queen39.cal8queens
    例子二：0-1背包
        内容：
            1。我们有一个背包，背包总的承载重量是Wkg。现在我们有n个物品，每个物品的重量不等，并且不可分割
            2。我们现在期望选择几件物品，装载到背包中。
        问题：
            在不超过背包所能装载重量的前提下，如何让背包中物品的总重量最大?
            注意：
                贪心算法：那里讲的物品是可以分割的
                回溯算法：今天讲的这个背包问题，物品是不可分割的，要么装要么不装，所以叫0-1背包问题
            分析：
                1。对于每个物品来说，都有两种选择，装进背包或者不装进背包。
                2。对于n个物品来说，总的装法就有2^n种，去掉总重量超过Wkg的，从剩下的装法中选择总重量最接近Wkg的
                关键点：
                    我们如何才能不重复地穷举出这2^n种装法呢?
        思路：
            这里就可以用回溯的方法
        具体实现：
            1。我们可以把物品依次排列，整个问题就分解为了n个阶段，每个阶段对应一个物品怎么选择
            2。先对第一个物品进行处理，选择装进去或者不装进去，然后再递归地处理剩下的物品。
            3。当发现已经选择的物品的重量超过Wkg之后，我们就停止继续探测剩下的物品。
        参考代码：
            profit.jikeshijian.shujujiegou.String.Queen39.f
    例子三：正则表达式
        通配符：
            “*”：匹配任意多个(大于等于0个)任意字符
            “?”：匹配零个或者一个任意字符
        问题：
            判断一个给定的文本，能否跟给定的正则表达式匹配?
        判断流程：
            1。依次考察正则表达式中的每个字符，当是非通配符时，我们就直接跟文本的字符进行匹配，如果相同，则继续往下处理;如果不同，则回溯。
            2。如果遇到特殊字符的时候，我们就有多种处理方式了，也就是所谓的岔路口
        情形一：
            1。比如“*”有多种匹配方案，可以匹配任意个文本串中的字符
            2。我们就先随意的选择一种匹配方案，然后继续考察剩下的字符。
            3。如果中途发现无法继续匹配下去了，我们就回到这个岔路口，重新选择一种匹配方案，然后再继续匹配剩下的字符。
        参考代码：
            profit.jikeshijian.shujujiegou.String.Pattern39.rmatch

40|初识动态规划:如何巧妙解决“双十一”购物时的凑单问题?
    动态规划
        思路：
            1。把问题分解为多个阶段，每个阶段对应一个决策
            2。我们记录每一个阶段可达的状态集合(去掉重复的)，然后通过当前阶段的状态集合，来推导下一个阶段的状态集合，
            3。动态地往前推进然后通过当前阶段的状态集合，来推导下一个阶段的状态集合，动态地往前推进
        例子一：背包问题
            内容
                对于一组不同重量、不可分割的物品，我们需要选择一些装入背包
            问题：
                在满足背包最大重量限制的前提下，背包中物品总重量的最大值是多少呢?
            方法一：
                回溯算法：
                    profit.jikeshijian.shujujiegou.String.Dynamic40.f1
            方法二：
                回溯算法优化：
                    profit.jikeshijian.shujujiegou.String.Dynamic40.f
            方法三：
                动态规划
                    概念：
                        1。我们把整个求解过程分为n个阶段，每个阶段会决策一个物品是否放到背包中
                        2。每个物品决策(放入或者不放入背包)完之后，背包中的物品的重量会有多种情况
                        3。也就是说，会达到多种不同的状态，对应到递归树中，就是有很多不同的节点。
                    思路：
                        1。把每一层重复的状态(节点)合并，只记录不同的状态，然后基于上一层的状态集合，来推导下一层的状态集合。
                        2。我们可以通过合并每一层重复的状态，这样就保证每一层不同状态的个数都不会超过w个(w表示背包的承载重量)，也就是例子中的9。
                        3。我们就成功避免了每层状态个数的指数级增长。
                    具体步骤：
                        我们用一个二维数组states[n][w+1]，来记录每层可以达到的不同状态。
                        情形一：
                            1。第0个(下标从0开始编号)物品的重量是2，要么装入背包，要么不装入背包
                            2。决策完之后，会对应背包的两种状态，背包中物品的总重量是0或者2
                            3。我们用states[0][0]=true和states[0][2]=true来表示这两种状态。
                        情形二：
                            1。第1个物品的重量也是2，基于之前的背包状态，在这个物品决策完之后，不同的状态有3个
                            2。背包中物品总重量分别是0(0+0)，2(0+2 or 2+0)，4(2+2)
                            3。我们用states[1][0]=true，states[1][2]=true，states[1][4]=true来表示这三种状态。
                        情形三：
                            1。以此类推，直到考察完所有的物品后，整个states状态数组就都计算好了
                            2。我把整个计算的过程画了出来，你可以看看。图中0表示false，1表示true
                            3。我们只需要在最后一层，找一个值为true的最接近w(这里是9)的值，就是背包中物品总重量的最大值。
                        参考代码：
                            profit.jikeshijian.shujujiegou.String.Dynamic40.knapsack
                    时间复杂度：
                        回溯算法：指数形式2^n
                        动态规划：
                            1。耗时最多的部分就是代码中的两层for循环，所以时间复杂度是O(n*w)
                            2。n表示物品个数，w表示背包可以承载的总重量。
                            例子：
                                我们假设有10000个物品，重量分布在1到15000之间，背包可以承载的总重量是30000
                                回溯算法：时间复杂度是是2^10000
                                动态规划：10000*30000。
                                优点：
                                    和2^1000比起来，要小太多了。
                    空间复杂度：
                        1。刚才的代码需要额外申请一个n乘以w+1的二维数组，对空间的消耗比较多
                        2。有时候，我们会说，动态规划是一种空间换时间的解决思路
                        优化：
                            我们只需要一个大小为w+1的一维数组就可以解决这个问题。动态规划状态转移的过程，都可以基于这个一维数组来操作
                        参考：
                            profit.jikeshijian.shujujiegou.String.Dynamic40.knapsack2
        例子二：背包问题升级版
            问题：
                1。刚刚讲的背包问题只涉及背包重量和物品重量。我们现在引入物品价值这一变量
                2。对于一组不同重量、不同价值、不可分割的物品，我们选择将某些物品装入背包
                3。在满足背包最大重量限制的前提下，背包中可装入物品的总价值最大是多少呢?
            方法一：
                回溯方法：
                参考代码:
                    profit.jikeshijian.shujujiegou.String.Dynamic40.f2
            方法二：
                动态规划：
                    思路：
                        1。我们用一个二维数组states[n][w+1]，来记录每层可以达到的不同状态。
                        2。不过这里数组存储的值不再是boolean类型的了，而是当前状态对应的最大总价值
                        3。我们把每一层中(i, cw)重复的状态(节点)合并，只记录cv值最大的那个状态，然后基于这些状态来推导下一层的状态。
                    参考代码：
                        profit.jikeshijian.shujujiegou.String.Dynamic40.knapsack3
        思考题：
            看大于等于200并且最接近200的组合是哪一个?
            分析：
                1。它跟第一个例子中讲的0-1背包问题很像，只不过是把“重量”换成了“价格”而已。
                2。购物车中有n个商品。我们针对每个商品都决策是否购买。
                3。每次决策之后，对应不同的状态集合。
                4。我们还是用一个二维数组states[n][x]，来记录每次决策之后所有可达的状态。不过，这里的x值是多少呢?
            思路：
                1。0-1背包问题中，我们找的是小于等于w的最大值，x就是背包的最大承载重量w+1
                2。对于这个问题来说，我们要找的是大于等于200(满减条件)的值中最小的，所以就不能设置为200加1了
                3。就这个实际的问题而言，如果要购买的物品的总价格超过200太多，比如1000，那这个羊毛“薅”得就没有太大意义了。
                4。我们可以限定x值为1001。
                5。这个问题不仅要求大于等于200的总价格中的最小的，我们还要找出这个最小总价格对应都要购买哪些商品。
                6。我们可以利用states数组，倒推出这个被选择的商品序列。我先把代码写出来，待会再照着代码给你解释。
            参考代码：
                profit.jikeshijian.shujujiegou.String.Dynamic40.double11advance

41|动态规划理论:一篇文章带你彻底搞懂最优子结构、无后效性和重复子问题
    “一个模型三个特征”理论讲解
        一个模型：
            概念：
                动态规划适合解决的问题的模型，定义为“多阶段决策最优解模型”
            作用：
                用动态规划来解决最优问题
            过程：
                1。而解决问题的过程，需要经历多个决策阶段
                2。每个决策阶段都对应着一组状态。然后我们寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值。

        三个特征：
            1。最优子结构
                概念：
                    1。问题的最优解包含子问题的最优解
                作用：
                    1。可以通过子问题的最优解，推导出问题的最优解。
                    2。后面阶段的状态可以通过前面阶段的状态推导出来。
            2。无后效性
                两层含义：
                    1。在推导后面阶段的状态的时候，我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的
                    2。某阶段状态一旦确定，就不受之后阶段的决策影响。
                注意：
                    只要满足前面提到的动态规划问题模型，其实基本上都会满足无后效性。

            3。重复子问题
                不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态。
    例子：
        内容：
            1。假设我们有一个n乘以n的矩阵w[n][n]。矩阵存储的都是正整数。棋子起始位置在左上角，终止位置在右下角
            2。我们将棋子从左上角移动到右下角。每次只能向或者向下移动一位。
            3。从左上角到右下角，会有很多不同的路径可以走。我们把每条路径经过的数字加起来看作路径的长度
        问题：
            那从左上角移动到右下角的最短路径长度是多少呢?
        一个模型：
            分析：
                1。从(0, 0)走到(n-1, n-1)，总共要走2*(n-1)步，也就对应着2*(n-1)个阶段
                2。每个阶段都有向右走或者向下走两种决策，并且每个阶段都会对应一个状态集合。
            结论：
                这个问题是一个多阶段决策最优解问题，符合动态规划的模型。
            原因：
                1。我们把状态定义为min_dist(i, j)，其中i表示行，j表示列
                2。min_dist表达式的值表示从(0, 0)到达(i, j)的最短路径长度。
        三个特征：
            1。重复子问题
                方法：
                    用回溯算法来解决这个问题
                分析：
                    1。如果你自己写一下代码，画一下递归树，就会发现，递归树中有重复的节点
                    2。重复的节点表示，从左上角到节点对应的位置，有多种路线，这也能说明这个问题中存在重复子问题。
            2。无后效性
                分析：
                    1。如果我们走到(i, j)这个位置，我们只能通过(i-1, j)，(i, j-1)这两个位置移动过来
                    2。我们想要计算(i, j)位置对应的状态，只需要关心(i-1, j)，(i, j-1)两个位置 对应的状态，并不关心棋子是通过什么样的路线到达这两个位置的
                    3。我们仅仅允许往下和往右移动，不允许后退，所以，前面阶段的状态确定之后，不会被后面阶段的决策所改变
            3。重复子问题
                分析：
                    1。我们把从起始位置(0, 0)到(i, j)的最小路径，记作min_dist(i, j)。
                    2。因为我们只能往右或往下移动，所以，我们只有可能从(i, j-1)或者(i-1, j)两个位置到达(i, j)
                    3。到达(i, j)的最短路径要么经过(i, j-1)，要么经过(i-1, j)，而且到达(i, j)的最短路径肯定包含到达这两个位置的最短路径之一
                    4。min_dist(i, j)可以通过min_dist(i, j-1)和min_dist(i-1, j)两个状态推导出来
    两种动态规划解题思路总结：
        1。状态转移表法
            背景：
                一般能用动态规划解决的问题，都可以使用回溯算法的暴力搜索解决。
            思路：
                1。当我们拿到问题的时候，我们可以先用简单的回溯算法解决
                2。然后定义状态，每个状态表示一个节点，然后对应画出递归树
                    递归树的作用：
                        1。从递归树中，我们很容易可以看出来，是否存在重复子问题，以及重复子问题是如何产生的
                        2。以此来寻找规律，看是否能用动态规划解决。
                3。找到重复子问题之后，接下来，我们有两种处理思路
                    1。思路一：
                        直接用回溯加“备忘录”的方法，来避免重复子问题
                        注意：
                            从执行效率上来讲，这跟动态规划的解决思路没有差别。
                    2。思路二：
                        使用动态规划的解决方法，状态转移表法
                        具体内容：
                            1。我们先画出一个状态表。
                                注意：
                                    1。状态表一般都是二维的，所以你可以把它想象成二维数组。
                                    2。每个状态包含三个变量，行、列、数组值
                            2。我们根据决策的先后过程，从前往后，根据递推关系，分阶段填充状态表中的每个状态。
                            3。最后，我们将这个递推填表的过程，翻译成代码，就是动态规划代码了。
            例子：
                如何套用这个状态转移表法，来解决之前那个矩阵最短路径的问题?
                思路：
                    1。从起点到终点，我们有很多种不同的走法。我们可以穷举所有走法，然后对比找出一个最短走法。
                        问题：
                            如何才能无重复又不遗漏地穷举出所有走法呢?
                            方法：
                                用回溯算法这个比较有规律的穷举算法。
                            参考代码：
                                profit.jikeshijian.shujujiegou.String.Dynamic41.minDistBT
                    2。有了回溯代码之后，接下来，我们要画出递归树，以此来寻找重复子问题。
                        分析：
                            1。在递归树中，一个状态(也就是一个节点)包含三个变量(i, j, dist)，其中i，j分别表示行和列，dist表示从起点到达(i, j)的路径长度
                            2。从图中，我们看出，尽管(i, j, dist)不存在重复的，但是(i, j)重复的有很多
                            3。对于(i, j)重复的节点，我们只需要选 择dist最小的节点，继续递归求解，其他节点就可以舍弃了。
                    3。既然存在重复子问题，我们就可以尝试看下，是否可以用动态规划来解决呢?
                        方法：
                            1。画出一个二维状态表，表中的行、列表示棋子所在的位置，表中的数值表示从起点到这个位置的最短路径
                            2。我们按照决策过程，通过不断状态递推演进，将状态表填好。为了方便代码实现，我们按行来进行依次填充。
                        参考代码；
                            profit.jikeshijian.shujujiegou.String.Dynamic41.minDistDP

        2。状态转移方程法
            概念：
                状态转移方程法有点类似递归的解题思路
            分析：
                1。某个问题如何通过子问题来递归求解，也就是所谓的最优子结构
                2。根据最优子结构，写出递归公式，也就是所谓的状态转移方程。
                3。有了状态转移方程，代码实现就非常简单了
            实现方式：
                1。递归加“备忘录
                2。迭代递推。
                例子：
                    我们还是拿刚才的例子来举例。最优子结构前面已经分析过了，你可以回过头去再看下
                状态转移方程：
                    min_dist(i, j) = w[i][j] + min(min_dist(i, j-1), min_dist(i-1, j))
                注意：
                    1。状态转移方程是解决动态规划的关键
                    2。如果我们能写出状态转移方程，那动态规划问题基本上就解决一大半了，而翻译成代码非常简单。
                    3。很多动态规划问题的状态本身就不好定义，状态转移方程也就更不好想到。
            参考代码：
                下面我用递归加“备忘录”的方式，将状态转移方程翻译成来代码，你可以看看
                profit.jikeshijian.shujujiegou.String.Dynamic41.minDist
    四种算法思想比较分析
        分治算法：
            解决的问题尽管大部分也是最优解问题，但是，大部分都不能抽象成多阶段决策模型。
        回溯算法：
            概念：
                1。相当于穷举搜索。穷举所有的情况，然后对比得到最优解
                2。基本上能用的动态规划、贪心解决的问题，我们都可以用回溯算法解决
            缺点：
                时间复杂度非常高，是指数级别的
            应用：
                只能用来解决小规模数据的问题。对于大规模数据的问题，用回溯算法解决的执行效率就很低了。
        动态规划：
            优点：
                比回溯算法高效
            缺点：
                1。并不是所有问题，都可以用动态规划来解决
                2。能用动态规划解决的问题，需要满足三个特征，最优子结构、无后效性和重复子问题。
            注意：
                1。在重复子问题这一点上，动态规划和分治算法的区分非常明显。
                2。分治算法要求分割成的子问题，不能有重复子问题，而动态规划正好相反
                3。动态规划之所以高效，就是因为回溯算法实现中存在大量的重复子问题。

        贪心算法：
            概念：
                动态规划算法的一种特殊情况
            优点：
                解决问题起来更加高效，代码实现也更加简洁
            缺点：
                1。它可以解决的问题也更加有限。
                2。它能解决的问题需要满足三个条件，最优子结构、无后效性和贪心选择性(这里我们不怎么强调重复子问题)。

42|动态规划实战:如何实现搜索引擎中的拼写纠错功能?
    问题：
        如何量化两个字符串的相似度?
    方法：
        编辑距离(Edit Distance)。
    概念：
        将一个字符串转化成另一个字符串，需要的最少编辑操作次数(比如增加一个字符、删除一个字符、替换一个字符)
    特点：
        1。编辑距离越大，说明两个字符串的相似程度越小;
        2。编辑距离就越小，说明两个字符串的相似程度越大。
        3。对于两个完全相同的字符串来说，编辑距离就是0
    计算方式：
        概念：
            根据所包含的编辑操作种类的不同，编辑距离有多种不同的计算方式
        分类：
            1。莱文斯坦距离
                特点：
                    允许增加、删除、替换字符这三个编辑操作
                分析角度：
                    两个字符串差异的大小;
                例子：
                    两个字符串mitcmu和mtacnu的莱文斯坦距离是3
                问题：
                    如何编程计算莱文斯坦距离?
                思考过程：
                    1。这个问题是求把一个字符串变成另一个字符串，需要的最少编辑次数
                    2。整个求解过程，涉及多个决策阶段，我们需要依次考察一个字符串中的每个字符，跟另一个字符串中的字符是否匹配
                    3。匹配的话如何处理，不匹配的话又如何处理。所以，这个问题符合多阶段决策最优解模型。
                    方法一：回溯方法
                        分析：
                            1。回溯是一个递归处理的过程
                            2。如果a[i]与b[j]匹配，我们递归考察a[i+1]和b[j+1]
                            3。如果a[i]与b[j]不匹配，那我们有多种处理方式可选:
                                1。可以删除a[i]，然后递归考察a[i+1]和b[j];
                                2。可以删除b[j]，然后递归考察a[i]和b[j+1];
                                3。可以在a[i]前面添加一个跟b[j]相同的字符，然后递归考察a[i]和b[j+1];
                                4。可以在b[j]前面添加一个跟a[i]相同的字符，然后递归考察a[i+1]和b[j];
                                5。可以将a[i]替换成b[j]，或者将b[j]替换成a[i]，然后递归考察a[i+1]和b[j+1]。
                        参考代码：
                            profit.jikeshijian.shujujiegou.String.Dynamic42.lwstBT
                        代码分析：
                            1。根据回溯算法的代码实现，我们可以画出递归树，看是否存在重复子问题
                            2。如果存在重复子问题，那我们就可以考虑能否用动态规划来解决;
                            3。如果不存在重复子问题，那回溯就是最好的解决方法。
                        参考：
                            com/suixingpay/profit/document/数据结构/图片/第42讲动态规划莱文斯坦距离递归树.png
                        图片分析：
                            1。在递归树中，每个节点代表一个状态，状态包含三个变量(i, j, edist)
                            2。其中，edist表示处理到a[i]和b[j]时，已经执行的编辑操作的次数
                            3。在递归树中，(i, j)两个变量重复的节点很多，比如(3, 2)和(2, 3)
                            4。对于(i, j)相同的节点，我们只需要保留edist最小的，继续递归处理就可以了，剩下的节点都可以舍弃
                            5。所以，状态就从(i, j, edist)变成了(i, j, min_edist)，其 中min_edist表示处理到a[i]和b[j]，已经执行的最少编辑次数。
                    方法二：
                        动态规划，的状态转移方程。
                            具体内容参考第42讲
                        参考代码：
                            profit.jikeshijian.shujujiegou.String.Dynamic42.lwstDP

            2。最长公共子串长度
                特点：
                    只允许增加、删除字符这两个编辑操作。
                分析角度：
                    两个字符串相似程度的大小。
                例子：
                    两个字符串mitcmu和mtacnu，最长公共子串长度是4。
                问题：
                    如何编程计算最长公共子串长度?
                分析：
                    1。针对这个问题，我直接定义状态，然后写状态转移方程。
                        每个状态还是包括三个变量(i, j, max_lcs)，max_lcs表示a[0...i]和b[0...j]的最长公共子串长度。
                        问题：
                            那(i, j)这个状态都是由哪些状态转移过来的呢?
                        思路：
                            1。先来看回溯的处理思路。我们从a[0]和b[0]开始，依次考察两个字符串中的字符是否匹配。
                                1。如果a[i]与b[j]互相匹配，我们将最大公共子串长度加一，并且继续考察a[i+1]和b[j+1]。
                                2。如果a[i]与b[j]不匹配，最长公共子串长度不变，这个时候，有两个不同的决策路线:
                                3。删除a[i]，或者在b[j]前面加上一个字符a[i]，然后继续考察a[i+1]和b[j];
                                4。删除b[j]，或者在a[i]前面加上一个字符b[j]，然后继续考察a[i]和b[j+1]。
                            2。如果我们要求a[0...i]和b[0...j]的最长公共长度max_lcs(i, j)，我们只有可能通过下面三个状态转移过来:
                                1。(i-1, j-1, max_lcs)，其中max_lcs表示a[0...i-1]和b[0...j-1]的最长公共子串长度;
                                2。(i-1, j, max_lcs)，其中max_lcs表示a[0...i-1]和b[0...j]的最长公共子串长度;
                                3。(i, j-1, max_lcs)，其中max_lcs表示a[0...i]和b[0...j-1]的最长公共子串长度。
                            3。如果我们把这个转移过程，用状态转移方程写出来，就是下面这个样子:
                                1。如果:a[i]==b[j]，那么:max_lcs(i, j)就等于:
                                    max(max_lcs(i-1,j-1)+1, max_lcs(i-1, j), max_lcs(i, j-1));
                                2。如果:a[i]!=b[j]，那么:max_lcs(i, j)就等于:
                                    max(max_lcs(i-1,j-1), max_lcs(i-1, j), max_lcs(i, j-1));
                        参考代码：
                            profit.jikeshijian.shujujiegou.String.Dynamic42.lcs

    复杂问题思考小技巧：
        技巧一：
            1。当我们拿到一个问题的时候，我们可以先不思考，计算机会如何实现这个问题，而是单纯考虑“人脑”会如何去解决这个问题
            2。人脑比较倾向于思考具象化的、摸得着看得见的东西，不适合思考过于抽象的问题。
            3。我们需要把抽象问题具象化
            问题：
                那如何具象化呢?
            答案：
                可以实例化几个测试数据，通过人脑去分析具体实例的解，然后总结规律，再尝试套用学过的算法，看是否能够解决
        技巧二：
            1。多练。
            2。做多了题目之后，自然就会有感觉，看到问题，立马就能想到能否用动态规划解决，
            3。然后直接就可以寻找最优子结构，写出动态规划方程，然后将状态转移方程翻译成代码。
    思考题：
        问题
            当用户在搜索框内，输入一个拼写错误的单词时，如何实现拼写纠错功能?
        原理：
            我们就拿这个单词跟词库中的单词一一进行比较，计算编辑距离，将编辑距离最小的单词，作为纠正之后的单词，提示给用户。
        注意：
            真正用于商用的搜索引擎，拼写纠错功能显然不会就这么简单
        原因：
            1。单纯利用编辑距离来纠错，效果并不一定好
                优化思路一：
                    具体：
                        1。我们并不仅仅取出编辑距离最小的那个单词，而是取出编辑距离最小的TOP 10，
                        2。然后根据其他参数，决策选择哪个单词作为拼写纠错单词
                    例子：
                        比如使用搜索热门程度来决定哪个单词作为拼写纠错单词。
                优化思路二：
                    具体
                        我们还可以用多种编辑距离计算方法
                    例子：
                        今天讲到的两种，然后分别编辑距离最小的TOP 10，然后求交集，用交集的结果，再继续优化处理。
                优化思路三：
                    1。我们还可以通过统计用户的搜索日志，得到最常被拼错的单词列表，以及对应的拼写正确的单词
                    2。搜索引擎在拼写纠错的时候，首先在这个最长被拼错单词列表中查找
                    3。如果一旦找到，直接返回对应的正确的单词。这样纠错的效果非常好。
                优化思路五：
                    1。我们还有更加高级一点的做法，引入个性化因素
                    2。针对每个用户，维护这个用户特有的搜索喜好，也就是常用的搜索关键词
                    3。当用户输入错误的单词的时候，我们首先在这个用户常用的搜索关键词中，计算编辑距离，查找编辑距离最小的单词。

            2。词库中的数据量可能很大，搜索引擎每天要支持海量的搜索，所以对纠错的性能要求很高。
                优化思路一：
                    1。如果纠错功能的TPS不高，我们可以部署多台机器，每台机器运行一个独立的纠错功能
                    2。当有一个纠错请求的时候，我们通过负载均衡，分配到其中一台机器，来计算编辑距离，得到纠错单词。
                优化思路二：
                    1。如果纠错系统的响应时间太长，也就是，每个纠错请求处理时间过长，我们可以将纠错的词库，分割到很多台机器
                    2。分割到很多台机器。当有一个纠错请求的时候，我们就将这个拼写错误的单词，同时发送到这多台机器
                    3。让多台机器并行处理，分别得到编辑距离最小的单词，然后再比对合并，最终决定出一个最优的纠错单词

43|拓扑排序:如何确定代码源文件的编译依赖关系?
    问题：
        如何确定代码源文件的编译依赖关系?
    分析：
        1。我们知道，一个完整的项目往往会包含很多代码源文件
        2。编译器在编译整个项目的时候，需要按照依赖关系，依次编译每个源文件
        例子：
            A.cpp依赖B.cpp，那在编译的时候，编译器需要先编译B.cpp，才能编译A.cpp。
    算法分析：
    拓扑排序：
        例子：
            1。我们在穿衣服的时候都有一定的顺序，我们可以把这种顺序想成，衣服与衣服之间有一定的依赖关系
            2。比如说，你必须先穿袜子才能穿鞋，先穿内裤才能穿秋裤。
            问题：
                1。假设我们现在有八件衣服要穿，它们之间的两两依赖关系我们已经很清楚了
                2。那如何安排一个穿衣序列，能够满足所有的两两之间的依赖关系?
            答案
                参考：
                    com/suixingpay/profit/document/数据结构/图片/第43讲穿衣的拓扑排序.png
    思路：
        算法是构建在具体的数据结构之上的。
        问题一：
            针对这个问题，我们先来看下，如何将问题背景抽象成具体的数据结构?
            分析：
                1。我们可以把源文件与源文件之间的依赖关系，抽象成一个有向图。
                2。每个源文件对应图中的一个顶点，源文件之间的依赖关系就是顶点之间的边。
                3。如果a先于b执行，也就是说b依赖于a，那么就在顶点a和顶点b之间，构建一条从a指向b的边
                4。而且，这个图不仅要是有向图，还要是一个有向无环图，也就是不能存在像a->b->c->a这样的循环依赖关系
                    原因：
                        图中一旦出现环，拓扑排序就无法工作了。实际上，拓扑排序本身就是基于有向无环图的一个算法。
            参考代码：
                profit.jikeshijian.shujujiegou.graph.Graph43
        问题二：
            如何在这个有向无环图上，实现拓扑排序?
            方法一：Kahn算法
                概念：
                    用的是贪心算法思想，思路非常简单、好懂。
                分析：
                    1。定义数据结构的时候，如果s需要先于t执行，那就添加一条s指向t的边
                    2。如果某个顶点入度为0，也就表示，没有任何顶点必须先于这个顶点执行，那么这个顶点就可以执行了。
                    3。我们先从图中，找出一个入度为0的顶点，将其输出到拓扑排序的结果序列中(对应代码中就是把它打印出来)
                    4。并且把这个顶点从图中删除(也就是把这个顶点可达的顶点的入度都减1)
                参考代码
                    profit.jikeshijian.shujujiegou.graph.Graph43.topoSortByKahn
                时间复杂度：
                    分析：
                        每个顶点被访问了一次，每个边也都被访问了一次
                    推出：
                        Kahn算法的时间复杂度就是O(V+E)(V表示顶点个数，E表示边的个数)。

            方法二：DFS深度优先搜索算法
                概念：
                    更加确切的说法应该是深度优先遍历，遍历图中的所有顶点，而非只是搜索一个顶点到另一个顶点的路径。
                参考代码：
                    profit.jikeshijian.shujujiegou.graph.Graph43.topoSortByDFS
                时间复杂度：
                    分析：
                        每个顶点被访问两次，每条边都被访问一次
                    推出：
                        时间复杂度也是O(V+E)。
    总结引申
        应用：
            1。需要通过局部顺序来推导全局顺序的，一般都能用拓扑排序来解决
            2。拓扑排序还能检测图中环的存在
        例子一：
            对于Kahn算法来说，如果最后输出出来的顶点个数，少于图中顶点个数，图中还有入度不是0的顶点，那就说明，图中存在环。
            图中环的检测：
                我们在递归那一节讲过一个例子，在查找最终推荐人的时候，可能会因为脏数据，造成存在循环推荐
                例子：
                    用户A推荐了用户B，用户B推荐了用户C，用户C又推荐了用户A。
                问题：
                    如何避免这种脏数据导致的无限递归?
            思路：
                1。这就是环的检测问题
                2。因为我们每次都只是查找一个用户的最终推荐人，所以，我们并不需要动用复杂的拓扑排序算法，
                3。而只需要记录已经访问过的用户ID，当用户ID第二次被访问的时候，就说明存在环，也就说明存在脏数据
            参考代码：
                profit.jikeshijian.shujujiegou.graph.Graph43.findRootReferrerId

44|最短路径:地图软件是如何计算出最优出行路径的?
    无权图：
        图的两种搜索算法，深度优先搜索和广度优先搜索，这两种算法主要是针对无权图的搜索算法
    有权图：
        问题：
            图中的每条边都有一个权重，我们该如何计算两点之间的最短路径(经过的边的权重和最小)呢?
        例子：
            1。如果想从家开车到公司，你只需要输入起始、结束地址，地图就会给你规划一条最优出行路线。
            2。这里的最优，有很多种定义，比如最短路线、最少用时路线、最少红绿灯路线等等
    最短路线算法分析：
        1。解决软件开发中的实际问题，最重要的一点就是建模，也就是将复杂的场景抽象成具体的数据结构
            问题：
                如何抽象成数据结构呢?
                答案：
                    1。图这种数据结构的表达能力很强，显然，把地图抽象成图最合适不过了。
                    2。我们把每个岔路口看作一个顶点，岔路口与岔路口之间的路看作一条边，路的长度就是边的权重。
                    3。如果路是单行道，我们就在两个顶点之间画一条有向边
                    4。如果路是双行道，我们就在两个顶点之间画两条方向不同的边
                    5。这样，整个地图就被抽象成一个有向有权图。
                参考代码：
                    profit.jikeshijian.shujujiegou.graph.Graph44.dijkstra
        时间复杂度：
            分析：
                1。代码实现中，最复杂就是while循环嵌套for循环那部分代码了
                2。while循环最多会执行V次(V表示顶点的个数)，而内部的for循环的执行次数不确定，
                3。跟每个顶点的相邻边的个数有关，我们分别记作E0，E1，E2，......，E(V-1)
                4。如果我们把这V个顶点的边都加起来，最大也不会超过图中所有边的个数E(E表示边的个数)。
                5，for循环内部的代码涉及从优先级队列取数据、往优先级队列中添加数据、更新优先级队列中的数据
                6。优先级队列是用堆来实现的，堆中的这几个操作，时间复杂度都是O(logV)(堆中的元素个数不会超过顶点的个数V)。
            结论：
                再利用乘法原则，整个代码的时间复杂度就是O(E*logV)。
    如何计算最优出行路线?
        背景：
            1。对于一个超级大地图来说，岔路口、道路都非常多，对应到图这种数据结构上来说，就有非常多的顶点和边。
            2。如果为了计算两点之间的最短路径，在一个超级大图上动用Dijkstra算法，遍历所有的顶点和边，显然会非常耗时
        问题：
            那我们有没有什么优化的方法呢?
        思路：
            1。很多时候，为了兼顾执行效率，我们只需要计算出一个可行的次优解就可以了
            2。虽然地图很大，但是两点之间的最短路径或者说较好的出行路径，并不会很“发散”，只会出现在两点之间和两点附近的区块内
            3。所以我们可以在整个大地图上，划出一个小的区块，这个小区块恰好可以覆盖住两个点，但又不会很大。
            4。我们只需要在这个小区块内部运行Dijkstra算法，这样就可以避免遍历整个大图，也就大大提高了执行效率。
        问题点一：
            1。如果两点距离比较远，从北京海淀区某个地点，到上海黄浦区某个地点，那上面的这种处理方法
            2。显然就不工作了，毕竟覆盖北京和上海的区块并不小。
        答案：
            1。对于这样两点之间距离较远的路线规划，我们可以把北京海淀区或者北京看作一个顶点
            2。把上海黄浦区或者上海看作一个顶点，先规划大的出行路
            3。比如，如何从北京到上海，必须要经过某几个顶点，或者某几条干道，然后再细化每个阶段的小路线。
    如何计算最少时间：
        思路：
            1。前面讲最短路径的时候，每条边的权重是路的长度
            2。在计算最少时间的时候，算法还是不变，我们只需要把边的权重，从路的长度变成经过这段路所需要的时间。
            3。不过，这个时间会根据拥堵情况时刻变化
        问题：
            如何计算车通过一段路的时间呢?
        答案：
            1。每经过一条边，就要经过一个红绿灯
            2。关于最少红绿灯的出行方案，实际上，我们只需要把每条边的权值改为1即可，算法还是不变，可以继续使用前面讲的Dijkstra算法
            3。不过，边的权值为1，也就相当于无权图了，我们还可以使用之前讲过的广度优先搜索算法。
            4。因为我们前面讲过，广度优先搜索算法计算出来的两点之间的路径，就是两点的最短路径。

45|位图:如何实现网页爬虫中的URL去重功能?
背景：
    1。网页爬虫是搜索引擎中的非常重要的系统，负责爬取几十亿、上百亿的网页
    2。爬虫的工作原理是，通过解析已经爬取页面中的网页链接，然后再爬取这些链接对应的网页
    3。而同一个网页链接有可能被包含在多个页面中，这就会导致爬虫在爬取的过程中，重复爬取相同的网页
    问题：
        如何避免这些重复的爬取呢?
    方法一：
        1。我们记录已经爬取的网页链接(也就是URL)，在爬取一个新的网页之前，我们拿它的链接，在已经爬取的网页链接列表中搜索
        2。如果存在，那就说明这个网页已经被爬取过了
        3。如果不存在，那就说明这个网页还没有被爬取过，可以继续去爬取
        4。等爬取到这个网页之后，我们将这个网页的链接添加到已经爬取的网页链接列表了。
算法解析
    问题一：
        先回想下，是否可以用我们之前学过的数据结构来解决呢?
        分析：
            1。这个问题要处理的对象是网页链接，也就是URL，需要支持的操作有两个，添加一个URL和查询一个URL
                2。除了这两个功能性的要求之外，在非功能性方面，我们还要求这两个操作的执行效率要尽可能高
                3。因为我们处理的是上亿的网页链接，内存消耗会非常大，所以在存储效率上，我们要尽可能地高效
            思路一：
                支持快速地插入、查找数据的数据结构：散列表、红黑树、跳表这些动态数据结构
        问题二：
            但是对内存消耗方面，是否可以接受呢?
            分析：
                1。我们拿散列表来举例。
                2。假设我们要爬取10亿个网页(像Google、百度这样的通用搜索引擎，爬取的网页可能会更多)，为了判重
                3。我们把这10亿网页链接存储在散列表中
        问题三：
            大约需要多少内存?
            思路：
                1。假设一个URL的平均长度是64字节，那单纯存储这10亿个URL，需要大约60GB的内存空间
                2。因为散列表必须维持较小的装载因子，才能保证不会出现过多的散列冲突，导致操作的性能下降
                3。而且，用链表法解决冲突的散列表，还会存储链表指针
                4。所以，如果将这10亿个URL构建成散列表，那需要的内存空间会远大于60GB，有可能会超过100GB。
            分析：
                对于一个大型的搜索引擎来说，即便是100GB的内存要求，其实也不算太高
            方法：
                我们可以采用分治的思想，用多台机器(比如20台内存是8GB的机器)来存储这10亿网页链接。
        问题四：
            我们是否还有进一步的优化空间呢
            背景：
                1。时间复杂度并不能完全代表代码的执行时间
                2。大O时间复杂度表示法，会忽略掉常数、系数和低阶，并且统计的对象是语句的频度，不同的语句，执行时间也是不同的。
                3。时间复杂度只是表示执行时间随数据规模的变化趋势，并不能度量在特定的数据规模下，代码执行时间的多少。
            思路：
                1。如果时间复杂度中原来的系数是10，我们现在能够通过优化，将系数降为1，
                2。那在时间复杂度没有变化的情况下，执行效率就提高了10倍
                3。对于实际的软件开发来说，10倍效率的提升，显然是一个非常值得的优化。
            分析：
                1。如果我们用基于链表的方法解决冲突问题，散列表中存储的是URL
                2。那当查询的时候，通过哈希函数定位到某个链表之后，我们还需要依次比对每个链表中URL
                3。这个操作是比较耗时的，主要有两点原因。
                    原因一：
                        1。链表中的结点在内存中不是连续存储的，所以不能一下子加载到CPU缓存中
                        2。没法很好地利用到CPU高速缓存，所以数据访问性能方面会打折扣
                    原因二：
                        1。链表中的每个数据都是URL，而URL不是简单的数字，是平均长度为64字节的字符串
                        2。也就是说，我们要让待判重的URL，跟链表中的每个URL，做字符串匹配。
                        3。这样一个字符串匹配操作，比起单纯的数字比对，要慢很多
                4。对于内存消耗方面的优化，除了刚刚这种基于散列表的解决方案，貌似没有更好的法子了
            方法：
                布隆过滤器(Bloom Filter)，节省内存
                位图：
                    一种存储结构，布隆过滤器本身就是基于位图的，是对位图的一种改进
                问题：
                    我们有1千万个整数，整数的范围在1到1亿之间。如何快速查找某个整数是否在这1千万个整数中呢?
                方法一：
                    可以用散列表来解决
                方法二：
                    使用一种比较“特殊”的散列表，那就是位图
                    具体实现：
                        1。我们申请一个大小为1亿、数据类型为布尔类型(true或者false)的数组。
                        2。我们将这1千万个整数作为数组下标，将对应的数组值设置成true
                        3。比如，整数5对应下标为5的数组值设置为true，也就是array[5]=true。
                        4。当我们查询某个整数K是否在这1千万个整数中的时候，我们只需要将对应的数组值array[K]取出来，看是否等于true。
                        5。如果等于true，那说明1千万整数中包含这个整数K;相反，就表示不包含这个整数K。
                        分析：
                            1。很多语言中提供的布尔类型，大小是1个字节的，并不能节省太多内存空间。
                            2。表示true和false两个值，我们只需要用一个二进制位(bit)就可以了
                        问题点：
                            那如何通过编程语言，来表示一个二进制位呢?
                            方法：
                                1。这里就要用到位运算了
                                2。我们可以借助编程语言中提供的数据类型，比如int、long、char等类型，通过位运算，用其中的某个位表示某个数字
                            参考代码：
                                profit.jikeshijian.shujujiegou.graph.BitMap45
                            优点：
                                1。位图通过数组下标来定位数据，所以，访问效率非常高
                                2。每个数字用一个二进制位来表示，在数字范围不大的情况下，所需要的内存空间非常节省。
                                例子：
                                    1。如果用散列表存储这1千万的数据，数据是32位的整型数，也就是需要4个字节的存储空间，那总共至少需要40MB的存储空间
                                    2。如果我们通过位图的话，数字范围在1到1亿之间，只需要1亿个二进制位，也就是12MB左右的存储空间就够了。
                            缺点：
                                1。数字范围不是1到1亿，而是1到10亿
                                2。那位图的大小就是10亿个二进制位，也就是120MB的大小，消耗的内存空间，不降反增。
                                优化方法：
                                    布隆过滤器，对位图这种数据结构的一种改进。
                                        例子一：
                                            数据个数是1千万，数据的范围是1到10亿。
                                            具体做法：
                                                1。使用一个1亿个二进制大小的位图，然后通过哈希函数，对数字进行处理，让它落在这1到1亿范围内
                                                2。比如我们把哈希函数设计成f(x)=x%n。其中，x表示数字，n表示位图的大小(1亿)，也就是，对数字跟位图的大小进行取模求余。
                                            问题点：
                                                哈希函数会存在冲突的问题啊，一亿零一和1两个数字，经过你刚刚那个取模求余的哈希函数处理之后，最后的结果都是1
                                            现象：
                                                这样我就无法区分，位图存储的是1还是一亿零一了。
                                            思路：
                                                为了降低这种冲突概率，当然我们可以设计一个复杂点、随机点的哈希函数
    布隆过滤器的处理方法：
        问题一：
            既然一个哈希函数可能会存在冲突，那用多个哈希函数一块儿定位一个数据，是否能降低冲突的概率呢?
            产生位图做法：
                1。我们使用K个哈希函数，对同一个数字进行求哈希值，那会得到K个不同的哈希值
                2。我们分别记作$X_{1}$，$X_{2}$，$X_{3}$，...，$X_{K}$
                3。我们把这K个数字作为位图中的下标，将对应的BitMap[$X_{1}$]，BitMap[$X_{2}$]，BitMap[$X_{3}$]，...，BitMap[$X_{K}$]都设置成true
                4。也就是说，我们用K个二进制位，来表示一个数字的存在。
            查询时候的做法：
                1。查询某个数字是否存在的时候，我们用同样的K个哈希函数，对这个数字求哈希值
                2。分别得到$Y_{1}$，$Y_{2}$，$Y_{3}$，...，$Y_{K}$。
                3。我们看这K个哈希值，对应位图中的数值是否都为true，如果都是true，则说明，这个数字存在
                4。如果有其中任意一个不为true，那就说明这个数字不存在。
        优点：
            1。对于两个不同的数字来说，经过一个哈希函数处理之后，可能会产生相同的哈希值
            2。但是经过K个哈希函数处理之后，K个哈希值都相同的概率就非常低了。
        缺点：
            容易误判（针对的是判断存在）
            概念：
                1。如果某个数字经过布隆过滤器判断不存在，那说明这个数字真的不存在，不会发生误判
                2。如果某个数字经过布隆过滤器判断存在，这个时候才会有可能误判，有可能并不存在。
            方法：
                我们调整哈希函数的个数、位图大小跟要存储数字的个数之间的比例，那就可以将这种误判的概率降到非常低。
        问题二：
            是否比散列表更加高效呢?
            分析：
                1。布隆过滤器用多个哈希函数对同一个网页链接进行处理，CPU只需要将网页链接从内存中读取一次，进行多次哈希计算，理论上讲这组操作是CPU密集型的
                2。在散列表的处理方式中，需要读取散列冲突拉链的多个网页链接，分别跟待判重的网页链接，进行字符串匹配。
                3。这个操作涉及很多内存数据的读取，所以是内存密集型的
            结论：
                CPU计算可能是要比内存访问更快速的，所以，理论上讲，布隆过滤器的判重方式，更加快速

46|概率统计:如何利用朴素贝叶斯算法过滤垃圾短信?
    算法解析
        1。基于黑名单的过滤器
            思路：
                1。我们可以维护一个骚扰电话号码和垃圾短信发送号码的黑名单。
                2。这个黑名单的搜集，有很多途径，比如，我们可以从一些公开的网站上下载，也可以通过类似“360骚扰电话拦截”的功能，通过用户自主标记骚扰电话来收集
                3。对于被多个用户标记，并且标记个数超过一定阈值的号码，我们就可以定义为骚扰电话，并将它加入到我们的黑名单中。
            情形一：
                1。如果黑名单中的电话号码不多的话，我们可以使用散列表、二叉树等动态数据结构来存储，对内存的消耗并不会很大
                2。如果我们把每个号码看作一个字符串，并且假设平均长度是16个字节，那存储50万个电话号码，大约需要10MB的内存空间
                3。即便是对于手机这样的内存有限的设备来说，这点内存的消耗也是可以接受的。
            情形二：
                1。如果黑名单中的电话号码很多呢?比如有500万个。这个时候，如果再用散列表存储，就需要大约100MB的存储空间。
                2。为了实现一个拦截功能，耗费用户如此多的手机内存，这显然有点儿不合理。
                方法一：
                    1。布隆过滤器最大的特点就是比较省存储空间，所以，用它来解决这个问题再合适不过了。
                    2。如果我们要存储500万个手机号码，我们把位图大小设置为10倍数据大小，也就是5000万，那也只需要使用5000万个二进制位(5000万bits)
                    3。换算成字节，也就是不到7MB的存储空间。比起散列表的解决方案，内存的消耗减少了很多。
                方法二：
                    1。时间换空间的方法，可以将内存的消耗优化到极致。
                    2。我们可以把黑名单存储在服务器端上，把过滤和拦截的核心工作，交给服务器端来做。
                    3。手机端只负责将要检查的号码发送给服务器端，服务器端通过查黑名单，判断这个号码是否应该被拦截，并将结果返回给手机端。
                    优点：
                        完全不需要占用手机内存
                    缺点：
                        1。网络通信是比较慢的，所以，网络延迟就会导致处理速度降低。
                        2。这个方案还有个硬性要求，那就是只有在联网的情况下，才能正常工作
        2。基于规则的过滤器
            背景：
                1。基于黑名单的垃圾短信过滤方法，但是，如果某个垃圾短信发送者的号码并不在黑名单中，那这种方法就没办法拦截了
                2。所以，基于黑名单的过滤方式，还不够完善
            思路：
                1。对于垃圾短信来说，我们还可以通过短信的内容，来判断某条短信是否是垃圾短信。
                2。我们预先设定一些规则，如果某条短信符合这些规则，我们就可以判定它是垃圾短信。
                3。规则可以有很多，比如下面这几个:
                    1。短信中包含特殊单词(或词语)，比如一些非法、淫秽、反动词语等;
                    2。短信发送号码是群发号码，非我们正常的手机号码，比如+60389585;
                    3。短信中包含回拨的联系方式，比如手机号码、微信、QQ、网页链接等，因为群发短信的号码一般都是无法回拨的;
                    4。短信格式花哨、内容很长，比如包含各种表情、图片、网页链接等;
                    5。符合已知垃圾短信的模板。垃圾短信一般都是重复群发，对于已经判定为垃圾短信的短信，我们可以抽象成模板，将获取到的短信与模板匹配，一旦匹配，我们就可以判定为垃圾短信。
                4。如果短信只是满足其中一条规则，如果就判定为垃圾短信，那会存在比较大的误判的情况。我们可以综合多条规则进行判断。
                    具体做法：
                        1。比如，满足2条以上才会被判定为垃圾短信;
                        2。或者每条规则对应一个不同的得分，满足哪条规则，我们就累加对应的分数，某条短信的总得分超过某个阈值，才会被判定为垃圾短信

        3。基于概率统计的过滤器
            概念：
                1。这种基于概率统计的过滤方式，基础理论是基于朴素贝叶斯算法
                参考46讲
                com/suixingpay/profit/document/数据结构/图片/第46讲朴素贝叶斯算法.png

47|向量空间:如何实现一个简单的音乐推荐系统?
    背景
        1。以前我们用MP3听歌，现在直接通过音乐App在线就能听歌
        2。各种音乐App的功能越来越强大，不仅可以自己选歌听，还可以根据你歌的口味偏好
        3。给你推荐可能会喜爱的音乐，而且有时候，推荐的音乐还非常适合你的口味，甚至会惊艳到你!
    问题：
        如此智能的一个功能，你知道它是怎么实现的吗?
    核心思想：
        1。找到跟你口味偏好相似的用户，把他们爱听的歌曲推荐给你;
        2。找出跟你喜爱的歌曲特征相似的歌曲，把这些歌曲推荐给你。
    1。基于相似用户做推荐
        具体做法：
            1。我们把跟你听类似歌曲的人，看做口味相似的用户
            2。我们只需要遍历所有的用户，对比每个用户跟你共同喜爱的歌曲个数，并且设置一个阈值
            3。如果你和某个用户共同喜爱的歌曲个数超过这个阈值，我们就把这个用户看作跟你口味相似的用户，把这个用户喜爱但你还没听过的歌曲，推荐给你。
        问题：
            如何知道用户喜爱哪首歌曲呢?也就是说，如何定义用户对某首歌曲的喜爱程度呢?
        方法：
            1。可以通过用户的行为，来定义这个喜爱程度。我们给每个行为定义一个得分，得分越高表示喜爱程度越高。
    2。基于相似歌曲做推荐
        背景：
            如果用户是一个新用户，我们还没有收集到足够多的行为数据，这个时候该如何推荐呢?
        方法：
            如果某首歌曲跟你喜爱的歌曲相似，我们就把它推荐给你。
        问题点：
            如何判断两首歌曲是否相似呢?
        答案：
            1。我们对歌曲定义一些特征项，比如是伤感的还是愉快的，是摇滚还是民谣，是柔和的还是高亢的等等
            2。类似基于相似用户的推荐方法，我们给每个歌曲的每个特征项打一个分数，这样每个歌曲就都对应一个特征项向量
            3。我们可以基于这个特征项向量，来计算两个歌曲之间的欧几里得距离。欧几里得距离越小，表示两个歌曲的相似程度越大。
        前提：
            1。我们能够找到足够多，并且能够全面代表歌曲特点的特征项
            2。我们还要人工给每首歌标注每个特征项的得分。对于收录了海量歌曲的音乐App来说，这显然是一个非常大的工程
            3。人工标注有很大的主观性，也会影响到推荐的准确性。

48|Bmore树:MySQL数据库索引是如何实现的?
    背景：
        数据库索引是如何实现的呢?底层使用的是什么数据结构和算法呢?
    算法解析
        1。解决问题的前提是定义清楚问题
            问题：
                如何定义清楚问题呢?
            方法一：
                对问题进行详细的调研
            方法二：
                通过对一些模糊的需求进行假设，来限定要解决的问题的范围
            例子：
                非功能性需求：
                    安全、性能、用户体验等等。
                性能方面的需求：
                    1。时间：执行效率，希望通过索引，查询数据的效率尽可能的高
                    2。空间：存储空间，希望索引不要消耗太多的内存空间。

        2。尝试用学过的数据结构解决这个问题
            散列表：
                1。查询性能很好，时间复杂度是O(1)
                2。能支持按照区间快速查找数据。所以，散列表不能满足我们的需求。
            平衡二叉查找树：
                1。查询的性能也很高，时间复杂度是O(logn)
                2。对树进行中序遍历，我们还可以得到一个从小到大有序的数据序列，但这仍然不足以支持按照区间快速查找数据。
            跳表：
                1。在链表之上加上多层索引构成的
                2。支持快速地插入、查找、删除数据，对应的时间复杂度是O(logn)。
                3。也支持按照区间快速地查找数据
                4。只需要定位到区间起点值对应在链表中的结点，然后从这个结点开始，顺序遍历链表，直到区间终点对应的结点为止
                5。这期间遍历得到的数据就是满足区间值的数据。
                结论：
                    跳表是可以解决这个问题。实际上，数据库索引所用到的数据结构跟跳表非常相似，叫作B+树
        3。改造二叉查找树来解决这个问题
            方法：
                1。为了让二叉查找树支持按照区间来查找数据，我们可以对它进行这样的改造:
                2。树中的节点并不存储数据本身，而是只是作为索引。
                3。我们把每个叶子节点串在一条链表上，链表中的数据是从小到大有序的
            例子一：
                1。改造之后，如果我们要求某个区间的数据。我们只需要拿区间的起始值，在树中进行查找，当查找到某个叶子节点之后
                2。我们再顺着链表往后遍历，直到链表中的结点数据值大于区间的终止值为止。
                3。所有遍历到的数据，就是符合区间值的所有数据。
                问题点：
                    1。我们要为几千万、上亿的数据构建索引，如果将索引存储在内存中
                    2。尽管内存访问的速度非常快，查询的效率非常高，但是，占用的内存会非常多。
                    例子：
                        1。我们给一亿个数据构建二叉查找树索引，那索引中会包含大约1亿个节点
                        2。每个节点假设占用16个字节，那就需要大约1GB的内存空间
                        3。给一张表建立索引，我们需要1GB的内存空间
                        4。如果我们要给10张表建立索引，那对内存的需求是无法满足的
                    问题：
                        如何解决这个索引占用太多内存的问题呢?
                    思路：
                        1。借助时间换空间的思路，把索引存储在硬盘中，而非内存中
                        2。硬盘是一个非常慢速的存储设备。通常内存的访问速度是纳秒级别的，而磁盘访问的速度是毫秒级别的
                        3。读取同样大小的数据，从磁盘中读取花费的时间，是从内存中读取所花费时间的上万倍，甚至几十万倍。
                        缺点：
                            1。这种将索引存储在硬盘中的方案，尽管减少了内存消耗
                            2。但是在数据查找的过程中，需要读取磁盘中的索引，因此数据查询效率就相应降低很多。
                        现象：
                            1。二叉查找树，经过改造之后，支持区间查找的功能就实现了
                            2。为了节省内存，如果把树存储在硬盘中，那么每个节点的读取(或者访问)，都对应一次磁盘IO操作
                            3。树的高度就等于每次查询数据时磁盘IO操作的次数。
                        问题：
                            如何降低树的高度呢?
                            原因：
                                比起内存读写操作，磁盘IO操作非常耗时，所以我们优化的重点就是尽量减少磁盘IO操作，
                            方法：
                                我们把索引构建成m叉树，高度是不是比二叉树要小呢?
            参考代码
                profit.jikeshijian.shujujiegou.tree.BPlusTreeNode48
                profit.jikeshijian.shujujiegou.tree.BPlusTreeLeafNode48
    索引的优点：
        可以提高数据库的查询效率
    索引的缺点：
        数据的写入过程，会涉及索引的更新，这是索引导致写入变慢的主要原因。
    插入现象：
        1。对于一个B+树来说，m值是根据页的大小事先计算好的，每个节点最多只能有m个子节点
        2。在往数据库中写入数据的过程中，这样就有可能使索引中某些节点的子节点个数超过m
            3。这个节点的大小超过了一个页的大小，读取这样一个节点，就会导致多次磁盘IO操作。
        问题：
            我们该如何解决这个问题呢?
        思路：
            1。只需要将这个节点分裂成两个节点。但是，节点分裂之后，其上层父节点的子节点个数就有可能超过m个
            2。我们可以用同样的方法，将父节点也分裂成两个节点。这种级联反应会从下往上，一直影响到根节点
        原因：
            正是因为要时刻保证B+树索引是一个m叉树，所以，索引的存在会导致数据库写入的速度降低。
    删除现象：
        1。在删除某个数据的时候，也要对应的更新索引节点
        2。频繁的数据删除，就会导致某些结点中，子节点的数变得非常少，长此以往，如果每个节点的子节点都比较少，势必会影响索引的效率。
        处理思路：
            1。我们可以设置一个阈值。在B+树中，这个阈值等于m/2。如果某个节点的子节点个数小于m/2，我们就将它跟相邻的兄弟节点合并
            2。合并之后结点的子节点个数有可能会超过m。
            3。针对这种情况，我们可以借助插入数据时候的处理方法，再分裂节点
    B+树的特点:
        1。每个节点中子节点的个数不能超过m，也不能小于m/2;
        2。根节点的子节点个数可以不超过m/2，这是一个例外;
        3。m叉树只存储索引，并不真正存储数据，这个有点儿类似跳表;
        4。通过链表将叶子节点串联在一起，这样可以方便按区间查找;
        5。一般情况，根节点会被存储在内存中，其他节点存储在磁盘中。
    而B树实际上是低级版的B+树，或者说B+树是B树的改进版。B树跟B+树的不同点主要集中在这几个地方:
        1。B+树中的节点不存储数据，只是索引，而B树中的节点存储数据;
        2。B树中的叶子节点并不需要链表来串联。

49|搜索:如何用A*搜索算法实现游戏中的寻路功能?
    参考第49讲
50|索引:如何在海量数据中快速查找某个数据?
    为什么需要索引?
        背景：
            1。如果抛开这些业务和功能的外壳，其实它们的本质都可以抽象为“对数据的存储和计算”
            2。对应到数据结构和算法中，那“存储”需要的就是数据结构，“计算”需要的就是算法。
        存储的需求：
            1。功能上无外乎增删改查。这其实并不复杂
            2。一旦存储的数据很多，那性能就成了这些系统要关注的重点
            3。特别是在一些跟存储相关的基础系统(比如MySQL数据库、分布式文件系统等)、中间件(比如消息中间件RocketMQ等)中。
        关键点：
            1。“如何节省存储空间、如何提高数据增删改查的执行效率”，这样的问题就成了设计的重点
            2。而这些系统的实现，都离不开一个东西，那就是索引
            3。不夸张地说，索引设计得好坏，直接决定了这些系统是否优秀。
    索引的需求定义：
        1。功能性需求
            考虑点一：
                数据是格式化数据还是非格式化数据?
                分析：
                    要构建索引的原始数据，类型有很多，把它分为两类
                        一类是：结构化数据，
                            例子：
                                MySQL中的数据
                        另一类：非结构化数据
                            注意：
                                一般需要做预处理，提取出查询关键词，对关键词构建索引。
                            例子：
                                搜索引擎中网页。
            考虑点二：
                数据是静态数据还是动态数据?
                情形一：
                    静态数据：
                    特点：
                        不会有数据的增加、删除、更新操作
                    做法：
                        构建索引的时候，只需要考虑查询效率就可以了
                    优点：
                        索引的构建就相对简单些
                情形二：
                    动态数据：
                        做法：
                            不仅要考虑到索引的查询效率，在原始数据更新的同时，我们还需要动态地更新索引
                        缺点：
                            支持动态数据集合的索引，设计起来相对也要更加复杂些。
            考虑点三：
                索引存储在内存还是硬盘?
                情形一：
                    内存：
                        优点：
                            那查询的速度肯定要比存储在磁盘中的高
                        缺点：
                            数据量有限
                情形二：
                    硬盘：
                        1。如果原始数据量很大的情况下，对应的索引可能也会很大
                        2。因为内存有限，我们可能就不得不将索引存储在磁盘中了
                情形三：
                    一部分存储在内存，一部分存储在磁盘，这样就可以兼顾内存消耗和查询效率。

            考虑点四：
                单值查找还是区间查找?
                    情形一：
                        单值查找：
                            概念：
                                根据查询关键词等于某个值的数据。这种查询需求最常见
                    情形二：
                        区间查找：
                            概念：
                                查找关键词处于某个区间值的所有数据。

            考虑点五：
                单关键词查找还是多关键词组合查找?
                    单关键词查找：
                        优点：
                            索引构建起来相对简单些
                    多关键词查询：
                        注意：
                            要分多种情况
                        例子：
                            1。像MySQL这种结构化数据的查询需求，我们可以实现针对多个关键词的组合，建立索引
                            2。对于像搜索引擎这样的非结构数据的查询需求，我们可以针对单个关键词构建索引
                            3。然后通过集合操作，比如求并集、求交集等，计算出多个关键词组合的查询结果。

        2。非功能性需求
            注意：
                不管是存储在内存中还是磁盘中，索引对存储空间的消耗不能过大。
            情形一：
                1。如果存储在内存中，索引对占用存储空间的限制就会非常苛刻
                2。毕竟内存空间非常有限，一个中间件启动后就占用几个GB的内存，开发者显然是无法接受的
            情形二：
                如果存储在硬盘中，那索引对占用存储空间的限制，稍微会放宽一些

    构建索引常用的数据结构有哪些?
        几种支持动态数据集合的数据结构：
            1。散列表、红黑树、跳表、B+树
            2。位图、布隆过滤器可以作为辅助索引，有序数组可以用来对静态数据构建索引。
            散列表：
                优点：
                    增删改查操作的性能非常好，时间复杂度是O(1)。
                应用：
                    一些键值数据库，比如Redis、Memcache，就是使用散列表来构建索引的
                特点：
                    这类索引，一般都构建在内存中。
            红黑树：
                概念：
                    一种常用的平衡二叉查找树，数据插入、删除、查找的时间复杂度是O(logn)，也非常适合用来构建内存索引
                应用：
                    Ext文件系统中，对磁盘块的索引，用的就是红黑树。
            B+树：
                1。比起红黑树来说，更加适合构建存储在磁盘中的索引
                2。B+树是一个多叉树，所以，对相同个数的数据构建索引，B+树的高度要低于红黑树
                3。当借助索引查询数据的时候，读取B+树索引，需要的磁盘IO次数非常更少
                应用：
                    大部分关系型数据库的索引，比如MySQL、Oracle，都是用B+树来实现的。
            跳表：
                优点：
                    1。支持快速添加、删除、查找数据
                    2。我们通过灵活调整索引结点个数和数据个数之间的比例，可以很好地平衡索引对内存的消耗及其查询效率
                应用：
                    Redis中的有序集合，就是用跳表来构建的。
            位图：
                可以用于索引中，辅助存储在磁盘中的索引，加速数据查找的效率
            布隆过滤器：
                缺点：
                    有一定的判错率
                思路：
                    尽管对于判定存在的数据，有可能并不存在，但是对于判定不存在的数据，那肯定就不存在
                优点：
                    1。内存占用非常少。我们可以针对数据，构建一个布隆过滤器，并且存储在内存中
                    2。当要查询数据的时候，我们可以先通过布隆过滤器，判定是否存在
                    3。如果通过布隆过滤器判定数据不存在，那我们就没有必要读取磁盘中的索引了。对于数据不存在的情况，数据查询就更加快速了。
            有序数组：
                1。也可以被作为索引
                2。如果数据是静态的，也就是不会有插入、删除、更新操作
                3。那我们可以把数据的关键词(查询用的)抽取出来，组织成有序数组，然后利用二分查找算法来快速查找数据。

51|并行算法:如何利用并行处理提高算法的执行效率?
    背景：
        1。时间复杂度是衡量算法执行效率的一种标准。时间复杂度并不能跟性能划等号。
        2。在真实的软件开发中，即便在不降低时间复杂度的情况下，也可以通过一些优化手段，提升代码的执行效率。
        3。算法的目的就是为了提高代码执行的效率
    问题：
        当算法无法再继续优化的情况下，我们该如何来进一步提高执行效率呢?
    方法一：并行排序
        案例：
            假设我们要给大小为8GB的数据进行排序，并且，我们机器的内存可以一次性容纳这么多数据
        分析：
            1。对于排序来说，最常用的就是时间复杂度为O(nlogn)的三种排序算法，归并排序、快速排序、堆排序
        优化思路一：
            对归并排序并行化处理。
            具体做法：
                1。我们可以将这8GB的数据划分成16个小的数据集合，每个集合包含500MB的数据
                2。我们用16个线程，并行地对这16个500MB的数据集合进行排序
                3。这16个小集合分别排序完成之后，我们再将这16个有序集合合并
        优化思路二：
            对快速排序并行化处理
            具体做法：
                1。我们通过扫描一遍数据，找到数据所处的范围区间。我们把这个区间从小到大划分成16个小区间
                2。我们将8GB的数据划分到对应的区间中。针对这16个小区间的数据，我们启动16个线程，并行地进行排序
                3。等到16个线程都执行结束之后，得到的数据就是有序数据了。
        共同点：
            利用的都是分治的思想，对数据进行分片，然后并行处理
        区别点：
            第一种处理思路是：
                先随意地对数据分片，排序之后再合并
            第二种思路：
                先对数据按照大小划分区间，然后再排序，排完序就不需要再处理了。这个跟归并和快排的区别如出一辙。
    方法二：并行查找
        例子：
            散列表：
                特点：
                    1。一种非常适合快速查找的数据结构
                现象：
                    如果我们是给动态数据构建索引，在数据不断加入的时候，散列表的装载因子就会越来越大。
                    方法：
                        为了保证散列表性能不下降，我们就需要对散列表进行动态扩容。
                        缺点：
                            对如此大的散列表进行动态扩容，一方面比较耗时，另一方面比较消耗内存。
                            例子：
                                1。我们给一个2GB大小的散列表进行扩容，扩展到原来的1.5倍，也就是3GB大小。
                                2。实际存储在散列表中的数据只有不到2GB，所以内存的利用率只有60%，有1GB的内存是空闲的。
                            方法：
                                1。我们可以将数据随机分割成k份(比如16份)，每份中的数据只有原来的1/k
                                2然后我们针对这k个小数据集合分别构建散列表
                                优点：
                                    1。散列表的维护成本就变低了
                                    2。当某个小散列表的装载因子过大的时候，我们可以单独对这个散列表进行扩容，而其他散列表不需要进行扩容。
                                例子：
                                    1。假设现在有2GB的数据，我们放到16个散列表中，每个散列表中的数据大约是150MB
                                    2。当某个散列表需要扩容的时候，我们只需要额外增加150*0.5=75MB的内存(假设还是扩容到原来的1.5倍)
                                    3。不管从扩容的执行效率还是内存的利用率上，这种多个小散列表的处理方法，都要比大散列表高效。
            查找数据时：
                我们只需要通过16个线程，并行地在这16个散列表中查找数据。
                优点：
                    这样的查找性能，比起一个大散列表的做法，也并不会下降，反倒有可能提高。
            添加数据时：
                可以选择将这个新数据放入装载因子最小的那个散列表中，这样也有助于减少散列冲突。
    方法三：
        并行字符串匹配
        背景：
            1。在文本中查找某个关键词这样一个功能，可以通过字符串匹配算法来实现。
            2。我们之前学过的字符串匹配算法有KMP、BM、RK、BF等
            3。当在一个不是很长的文本中查找关键词的时候，这些字符串匹配算法中的任何一个，都可以表现得非常高效
        问题：
            如果我们处理的是超级大的文本，那处理的时间可能就会变得很长，那有没有办法加快匹配速度呢?
        方法：
            我们可以把大的文本，分割成k个小文本。假设k是16，我们就启动16个线程，并行地在这16个小文本中查找关键词
        优点：
            这样整个查找的性能就提高了16倍
        问题点：
            1。原本包含在大文本中的关键词，被一分为二，分割到两个小文本中
            2。这就会导致尽管大文本中包含这个关键词，但在这16个小文本中查找不到它。
            例子：
                1。我们假设关键词的长度是m。我们在每个小文本的结尾和开始各取m个字符串
                2。前一个小文本的末尾m个字符和后一个小文本的开头m个字符，组成一个长度是2m的字符串
                3。我们再拿关键词，在这个长度为2m的字符串中再重新查找一遍，就可以补上刚才的漏洞了。
    方法四：
        搜索算法：
            广度优先搜索、深度优先搜索、Dijkstra最短路径算法、A*启发式搜索算法
        对于广度优先搜索算法，我们也可以将其改造成并行算法。
        广度优先搜索算法：
            概念：
                1。一种逐层搜索的搜索策略。
                2。基于当前这一层顶点，我们可以启动多个线程，并行地搜索下一层的顶点
                3。在代码实现方面，原来广度优先搜索的代码实现，是通过一个队列来记录已经遍历到但还没有扩展的顶点
            改造点：
                需要利用两个队列来完成扩展顶点的工作。
            具体做法：
                1。假设这两个队列分别是队列A和队列B。多线程并行处理队列A中的顶点，并将扩展得到的顶点存储在队列B中。
                2。等队列A中的顶点都扩展完成之后，队列A被清空，我们再并行地扩展队列B中的顶点，并将扩展出来的顶点存储在队列A。
                3。这样两个队列循环使用，就可以实现并行广度优先搜索算法。















