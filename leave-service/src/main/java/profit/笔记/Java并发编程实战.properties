01 | 可见性、原子性和有序性问题:并发编程Bug的源头
    背景：
        1。CPU、内存、I/O三者的速度差异，是其核心矛盾
            例如：
                CPU 和内存的速度差异：
                    假设 CPU 执行一条普通指令需要一天，那么 CPU 读写内存得等待一年的时间
                        1。CPU 是天上一天
                        2。内存是地上一年
                内存和 I/O 设备的速度差异就更大了
                    1。内存是天上一天
                    2。I/O 设备是地上十年
        2。问题二：
            单方面提高 CPU 性能是无效的
                原因：
                    1。程序里大部分语句都要访问内存，有些还要访问 I/O，根据木桶理论
                            目桶理论：一只水桶能装多少 水取决于它最短的那块木板
                    2。程序整体的性能取决于最慢的操作
            方法：
                平衡这三者的速度差异
            目的：
                合理利用 CPU 的高性能
            具体体现：
                1。计算机体系结构：CPU 增加了缓存，以均衡与内存的速度差异;
                2。操作系统：增加了进程、线程，以分时复用 CPU，进而均衡 CPU 与 I/O 设备的速度差异;
                3。编译程序：优化指令执行次序，使得缓存能够得到更加合理地利用。
    并发程序问题的根源一般在哪里？
        1。源头之一:缓存导致的可见性问题
            概念：
                可见性：
                    概念：
                    一个线程对共享变量的修改，另外一个线程能够立刻看到
                单核时代：
                    CPU 缓存与内存的数据一致性容易解决
                    原因：
                        所有线程都是操作同一个 CPU 的缓存，一个线程对缓存的写，对另外一个线程来 说一定是可见的
                    例子：
                        线程 A 更新了变量 V 的值，那么线程 B 之后再访问变量 V，得到的一定是 V 的 最新值
                        原因：
                            线程 A 和线程 B 都是操作同一个 CPU 里面的缓存
                多核时代：
                    概念：
                        1。每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了
                        2。当多个线程在不同的 CPU 上执行时，这些线程操作的是不同的 CPU 缓存
                    例子：
                        线程 A 对变量 V 的操作对于线程 B 而言就不具备可见性了
                    原因：
                        线程 A 操作的是 CPU-1 上的缓存，而线程 B 操作的是 CPU-2 上的缓存

                    案例：
                        参考：profit.jikeshijian.bingfabiancheng.TestVisibility01
                        问题：
                            calc() 的执行结果是个 10000 到 20000 之间的随机数。为什么呢?
                        过程：：
                            1。假设线程 A 和线程 B 同时开始执行，那么第一次都会将 count=0 读到各自的 CPU 缓存里
                            2。执行完 count+=1 之后，各自 CPU 缓存里的值都是 1，同时写入内存后
                            3。我们会发现内存中是 1，而不是我们期望的 2
                            4。之后由于各自的 CPU 缓存里都有了 count 的值
                        分析：
                            缓存的可见性问题。
                        原因：
                            1。两个线程都是基于 CPU 缓存里的 count 值来计
                            2。所以导致最终 count 的值都是小

        2。源头之二:线程切换带来的原子性问题
            背景：
                早期系统：
                    概念：
                        1。进程要做任务切换就要切换内存映射地址
                            原因：
                                基于进程来调度 CPU，不同进程间是不共享内存空间的
                        2。线程做任务切换成本就很低
                            原因：
                                一个进程创建的所有线程，都是共享一个内存空间的
                现在系统：
                    概念：
                        1。都基于更轻量的线程来调度
                        2。现在我们提到的“任务切换”都是指“线程切换”。
                    任务切换的时机：大多数是在时间片结束的时候
            高级语言的一条语句和CPU指令的关系：
                1。高级语言里一条语句往往需要多条 CPU 指令完成
                    例子：
                        高级语言：count += 1，至少需要三条 CPU 指令
                        指令1：首先，需要把变量 count 从内存加载到 CPU 的寄存器;
                        指令2：之后，在寄存器中执行 +1 操作;
                        指令3：最后，将结果写入内存(缓存机制导致可能写入的是 CPU 缓存而不是内存)。
                2。操作系统切换时：
                    1。可以发生在任何一条CPU 指令执行完
                    2。而不是高级语言里的一条语句。
                    分析：
                        1。假设 count=0，如果线程 A 在指令 1 执行完后做线程切换
                        2。我们会发现 两个线程都执行了 count+=1 的操作
                        3。但是得到的结果不是我们期望的 2，而是 1
                    意识：
                        1。觉得 count+=1 这个操作是一个不可分割的整体，就像一个原子一样
                        2。线 程的切换可以发生在 count+=1 之前，也可以发生在 count+=1 之后，但就是不会发生 在中间
                    原子性：
                        把一个或者多个操作在 CPU 执行的过程中不被中断的特性
                        cpu保证原子性：
                            注意：
                                1。是CPU 指令级别的，而不是高级语言的操作符
                                2。很多时候我们需要在高级语言层面保证操作的原子性
        3。源头之三:编译优化带来的有序性问题
            背景：
                1。编译器为了优化性能，有时候会改 变程序中语句的先后顺序
                    例子：
                        a=6;b=7;
                    优化后：
                        b=7;a=6;
                    现象：
                        在这个例子中，编译器调整了语句的顺序，但是不影响程序的最终结果
            有序性：
                程序按照代码的先后顺序执行。
            问题：
                有时候编译器及解释器的优化可能导致意想不到的 Bug。
            案例：
                利用双重检查创建单例对象
            具体代码：
                public class Singleton {
                    static Singleton instance;
                    static Singleton getInstance(){
                        if (instance == null) {
                            synchronized(Singleton.class) {
                                if (instance == null)
                                    instance = new Singleton();
                                }
                            }
                        return instance;
                        }
                }
                正常流程：
                    1。假设有两个线程 A、B 同时调用 getInstance() 方法，他们会同时发现 instance == null
                    2。于是同时对 Singleton.class 加锁，此时 JVM 保证只有一个线程能够加锁成功
                    3。线程A加锁成功，线程B处于等待状态；
                    4。线程 A 会创建一个 Singleton 实例，之后释放锁，锁释放后
                    5。线程 B 被唤醒，线程 B 再次尝试加锁，此时是可以加锁成功的
                    6。加锁成功后，线程 B 检查 instance == null 时会发现
                    7。已经创建过 Singleton 实例了，所以线程 B 不会再创建一个 Singleton 实例。
                    我们以为的new操作：
                        1。分配一块内存 M；
                        2。在内存 M 上初始化 Singleton 对象；
                        3。然后 M 的地址赋值给 instance 变量。
                    优化后的操作：
                        1。分配一块内存 M；
                        2。将 M 的地址赋值给 instance 变量；
                        3。最后在内存 M 上初始化 Singleton 对象。
                        分析：
                            1。我们假设线程 A 先执行 getInstance() 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；
                            2。如果此时线程 B 也执行 getInstance() 方法，那么线程 B 在执行第一个判断时会发现 instance != null
                            3。所以直接返回 instance，而此时的 instance 是没有初始化过的
                            问题：
                                如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。
    汇总：
        1。缓存导致的可见性问题
        2。线程切换带来的原子性问题
        3。编译优化带来的有序性问题
        4。其实缓存、线程、编译优化的目的和我们写并发程序的目的是相同的，都是提高程序性能

02 | Java内存模型：看Java如何解决可见性和有序性问题
    什么是 Java 内存模型？
        背景：
            1。导致可见性的原因是缓存
            2。导致有序性的原因是编译优化
            思路：
                最直接的办法就是禁用缓存和编译优化
            问题：
                程序的性能可就堪忧了
            方法：
                提供给程序员按需禁用缓存和编译优化的方法即可。
                    按需禁用：按照程序员的要求来禁用
        概念：
            规范了 JVM 如何提供按需禁用缓存和编译优化的方法
                方法：
                    包括 volatile、synchronized 和 final 三个关键字，以及六项 Happens-Before 规则

    具体方法：
    1。使用 volatile 的困惑
        案例一：
            声明一个 volatile 变量 volatile int x = 0
            含义：
                1。告诉编译器，对这个变量的读写，不能使用 CPU 缓存
             2。必须从内存中读取或者写入
        案例二：(可见性)
            class VolatileExample {
                int x = 0;
                volatile boolean v = false;
                public void writer() {
                    x = 42;
                    v = true;
                }
                public void reader() {
                    if (v == true) {
                    // 这里 x 会是多少呢？
                    }
                }
            }
            过程：
                1。假设线程 A 执行 writer() 方法，按照 volatile 语义，会把变量 “v=true” 写入内存；
                2。假设线程 B 执行 reader() 方法，同样按照 volatile 语义，线程 B 会从内存中读取变量 v
                3。如果线程 B 看到 “v == true” 时，那么线程 B 看到的变量 x 是多少呢？
            结论：
                低于 1.5 版本上：
                  x 可能是 42，也有可能是 0；
                    原因：
                        变量 x 可能被 CPU 缓存而导致可见性问题
                1.5 以上的版本上：
                  x 就是等于 42
                    原因：
                        Java 内存模型在 1.5 版本对 volatile 语义进行了增强
                        问题：如何增强？
                        方法：Happens-Before 规则。
            Volatile使用条件
                1。对变量的写入操作不依赖变量的当前值，或者能确保只有单个线程更新变量的值
                2。该变量不会与其他状态变量一起纳入不变性条件中
                3。在访问变量时不需要加锁
            应用场景
                1。确保他们自身的可见性
                2。确保他们引用对象的状态的可见性
                3。标识一些重要的程序生命周期事件的发生

    2。Happens-Before 规则
        背景：
            和程序员相关的规则一共有如下六项，都是关于可见性的。
        概念：
            前面一个操作的结果对后续操作是可见的
                注意：并不是说前面一个操作发生在后续操作的前面
        作用：
            约束了编译器的优化行为，虽允许编译器优化，但是要求编译器优化后一定遵守 Happens-Before 规则。
        1。程序的顺序性规则
            概念：
                指在一个线程中，按照程序顺序，前面的操作 Happens-Before 于后续的任意操作
            例子：
                参考代码：profit.jikeshijian.bingfabiancheng.VolatileExample02
                第 6 行代码 “x = 42;” Happens-Before 于第 7 行代码 “v = true;”
                类似于单线程里面的思维：
                    程序前面对某个变量的修改一定是对后续操作可见的
        2。volatile 变量规则
            概念：
                指对一个 volatile 变量的写操作， Happens-Before 于后续对这个 volatile 变量的读操作。
#            例子：
#                变量“v=true” Happens-Before 读变量 “v=true”
        3。传递性
            概念：
                指如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens-Before C。
            案例：
                1。“x=42” :Happens-Before 写变量 “v=true” ，这是规则 1 的内容；
                2。变量“v=true” Happens-Before 读变量 “v=true”，这是规则 2 的内容 。
            根据传递性原则：
                “x=42” Happens-Before 读变量“v=true”。
                现象：
                    1。如果线程 B 读到了“v=true”，那么线程 A 设置的“x=42”对线程 B 是可见的
                    原因：
                          这就是 1.5 版本对 volatile 语义的增强
                    例子：
                        1.5 版本的并发工具包（java.util.concurrent）就是靠 volatile 语义来搞定可见性的

        4。管程中锁的规则
            概念：
                指对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。
            管程：
                1。一种通用的同步原语，在 Java 中指的就是 synchronized
                2。synchronized是 Java 里对管程的实现
                3。管程中的锁在 Java 里是隐式实现的
            案例：
                1。在进入同步块之前，会自动加锁
                2。而在代码块执行完会自动释放锁，加锁以及释放锁都是编译器帮我们实现的。
                synchronized (this) { // 此处自动加锁
                    // x 是共享变量, 初始值 =10
                    if (this.x < 12) {
                        this.x = 12; 
                    }
                } // 此处自动解锁
                流程：
                    1。假设 x 的初始值是 10，线程 A 执行完代码块后 x 的值会变成 12（执行完自动释放锁）
                    2。线程 B 进入代码块时，能够看到线程 A 对 x 的写操作，也就是线程 B 能够看到 x==12

        5。线程 start() 规则（针对子线程）
            背景：
                关于线程启动的
            概念：
                指主线程 A 启动子线程 B 后，子线程 B 能够看到主线程在启动子线程 B 前的操作。
            案例：
                1。如果线程 A 调用线程 B 的 start() 方法（即在线程 A 中启动线程 B）
                2。那么该 start() 操作 Happens-Before 于线程 B 中的任意操作。具体可参考下面示例代码。
            具体代码
                Thread B = new Thread(()->{
                    // 主线程调用 B.start() 之前
                    // 所有对共享变量的修改，此处皆可见
                    // 此例中，var==77
                });
                // 此处对共享变量 var 修改
                var = 77;
                // 主线程启动子线程
            B.start();

        6。线程 join() 规则(针对主线程)
            背景：
                关于线程等待的
            前提：对共享变量的操作
            概念：
                1。指主线程 A 等待子线程 B 完成(主线程 A 通过调用子线程 B 的 join() 方法实现)
                2。当子线程 B 完成后(主线程 A 中 join() 方法返回)
                3。主线程能够看到子线程的操作
            案例：
                1。如果在线程 A 中，调用线程 B 的 join() 并成功返回
                2。那么线程 B 中的任意操作 Happens-Before 于该 join() 操作的返回
                示例代码：
                    Thread B = new Thread(()->{
                        // 此处对共享变量 var 修改
                        var = 66;
                    });
                    // 例如此处对共享变量修改，
                    // 则这个修改结果对线程 B 可见
                    // 主线程启动子线程
                    B.start();
                    B.join()
                    // 子线程所有对共享变量的修改
                    // 在主线程调用 B.join() 之后皆可见
                    // 此例中，var==66

    3。被我们忽视的 final
        背景：
        1。volatile 为的是禁用缓存以及编译优化
        问题：
            有没有办法告诉编译器优化得更好一点呢？
        方法：
            final 关键字
        作用：
            修饰变量时，初衷是告诉编译器：这个变量生而不变，可以可劲儿优化
        注意：
            Jdk1。5以前：
                概念：
                    的确优化得很努力，以至于都优化错了。
                案例：
                    双重检查方法创建单例，构造函数的错误重排导致线程可能看到 final 变量的值会变化
            Jdk1。5以后：
                1。Java 内存模型对 final 类型变量的重排进行了约束，
                2。只要我们提供正确构造函数没有“逸出”，就不会出问题了
                案例：
                    1。在构造函数里面将 this 赋值给了全局变量 global.obj，这就是“逸出”
                    2。线程通过 global.obj 读取 x 是有可能读到 0 的
                        具体代码：
                            // 以下代码来源于【参考 1】
                            final int x;
                            // 错误的构造函数
                            public FinalFieldExample() { 
                                x = 3;
                                y = 4;
                                // 此处就是讲 this 逸出，
                                global.obj = this;
                            }
                        方法：
                            我们一定要避免“逸出”。

03 | 互斥锁（上）：解决原子性问题
    原子性
        概念：
            一个或者多个操作在 CPU 执行的过程中不被中断的特性
    问题：
        如何解决原子性问题
    源头：
        线程切换
    思考一：
        如果能够禁用线程切换那不就能解决这个问题了吗？
        方案：
            禁止 CPU 发生中断就能够禁止线程切换。
                原因：
                    操作系统做线程切换是依赖 CPU 中断的
                案例：
                    1。32 位 CPU 上执行 long 型变量的写操作
                    2。long 型变量是 64 位，
                    3。在 32 位 CPU 上执行写操作会被拆分成两次写操作(写高 32 位和写低 32 位)
                单核 CPU 场景下(可行)
                    要么都被执行，要么都没有被执行，具有原子性
                        原因：
                            1。同一时刻只有一个线程执行，禁止 CPU 中断
                            2。意味着操作系统不会重新调度线程，也就是禁止了线程切换，获得 CPU 使用权的线程就可以不间断地执行
                多核场景下(不可行)
                    bug：明明已经把变量成功写入内存，重新读出来却不是自己写入的。
                        原因：
                            1。同一时刻，有可能有两个线程同时在执行，一个线程执行在 CPU-1 上，一个线程执行在 CPU-2 上
                            2。此时禁止 CPU 中断，只能保证 CPU 上的线程连续执行，并不能保证同一时刻只有一个线程执行
                            3。如果这两个线程同时写 long 型变量高 32 位的话
    思考二：
        如果我们能够保证对共享变量的修改是互斥的，那么，无论是单核 CPU 还是多核 CPU，就都能保证原子性了
            互斥：同一时刻只有一个线程执行
        方法：
            加锁
    简易锁模型
        临界区：一段需要互斥执行的代码
        过程：
            1。线程在进入临界区之前，首先尝试加锁 lock()
            2。如果成功，则进入临界区，此时我们称这个线程持有锁
            3。否则呢就等待，直到持有锁的线程解锁
            4。持有锁的线程执行完临界区的代码后，执行解锁 unlock()。
        类似案例：
            1。像办公室里高峰期抢占坑位，每个人都是进坑锁门（加锁），出坑开门（解锁）
            2。如厕这个事就是临界区
        这样理解容易忽视的两个问题：
            1。锁的是什么？
            2。保护的又是什么？

    改进后的锁模型
        背景：
            锁和锁要保护的资源是有对应关系的
            例子：
                你用你家的锁保护你家的东西，我用我家的锁保护我家的东西
        原理：
            1。首先，我们要把临界区要保护的资源标注出来，临界区里增加了一个元素：受保护的资源 R
            2。其次，我们要保护资源 R 就得为它创建一把锁 LR
            3。最后，针对这把锁 LR，我们还需在进出临界区时添上加锁操作和解锁操作
        注意：
            创建保护资源R的锁R---》受保护资源R，这个关联关系非常重要
            忽视这个关联带来的问题：
                锁自家门来保护他家资产的事情

    Java 语言提供的锁技术：synchronized
        背景
            1。锁一种通用的技术方法
            2。Java 语言提供的 synchronized 关键字，就是锁的一种实现。
        synchronized：
            关键字可以用来修饰方法，也可以用来修饰代码块
            例子：
                class X {
                    // 修饰非静态方法
                    synchronized void foo() {
                        // 临界区
                    }
                    // 修饰静态方法
                    synchronized static void bar() {
                        // 临界区
                    }
                    // 修饰代码块
                    Object obj = new Object()；
                    void baz() {
                        synchronized(obj) {
                        // 临界区
                        }
                    }
                }
                加锁动作：
                    Java 编译器会在 synchronized 修饰的方法或代码块前后自动加上加锁 lock() 和解锁 unlock()
                锁定的对象是什么：
                    1。修饰静态方法：锁定的是当前类的 Class 对象
                        class X {
                            // 修饰静态方法
                            synchronized(X.class) static void bar() {
                            // 临界区
                            }
                        }
                    2。修饰非静态方法：锁定的是当前实例对象 this。
                        class X {
                            // 修饰非静态方法
                            synchronized(this) void foo() {
                            // 临界区
                            }
                        }

                    3。如果指定了对象，锁定的是指定的对象
    案例：
        用 synchronized 解决 count+=1 问题
        具体代码：
            class SafeCalc {
                long value = 0L;
                long get() {
                    return value;
                }
                synchronized void addOne() {
                    value += 1;
                }
            }
            分析addOne方法：
                被 synchronized 修饰后，一定能保证原子操作
                    原因：
                        无论是单核 CPU 还是多核 CPU，只有一个线程能够执行 addOne() 方法
                问题一：
                    执行addOne方法，是否有可见性问题呢？
                    背景：
                        1。管程，就是我们这里的 synchronized
                        2。管程中锁的规则：对一个锁的解锁 Happens-Before 于后续对这个锁的加锁。
                    分析：
                        1。synchronized 修饰的临界区是互斥的，也就是说同一时刻只有一个线程执行临界区的代码
                        2。前一个线程在临界区修改的共享变量（该操作在解锁之前），对后续进入临界区（该操作在加锁之后）的线程是可见的。
                            原因：
                                1。“对一个锁解锁 Happens-Before 后续对这个锁的加锁”，指的是前一个线程的解锁操作对后一个线程的加锁操作可见
                                2。Happens-Before 的传递性原则，
            分析get方法：
                问题二：
                    执行 addOne() 方法后，value 的值对 get() 方法是可见的吗？
                    答案：
                        可见性是没法保证的
                    原因：
                        1。管程中锁的规则，是只保证后续对这个锁的加锁的可见性
                        2。而 get() 方法并没有加锁操作，所以可见性没法保证
                    方法：
                        用synchronized修饰get方法
                        修改后的代码：
                            class SafeCalc {
                                long value = 0L;
                                synchronized long get() {
                                    return value;
                                }
                                synchronized void addOne() {
                                    value += 1;
                                }
                            }
                        分析：
                            get() 和 addOne() 也是互斥的
                        原因：
                            1。get() 方法和 addOne() 方法都需要访问 value 这个受保护的资源
                            2。这个资源用 this 这把锁来保护。线程要进入临界区 get() 和 addOne()，必须先获得 this 这把锁
                        类似的场景：
                            像现实世界里面球赛门票的管理，一个座位只允许一个人使用
                                1。座位：受保护资源
                                2。球场的入口：Java 类里的方法
                                3。门票：用来保护资源的“锁”
                                4。检票：由 synchronized 解决的

    锁和受保护资源的关系：
        概念：
            受保护资源和锁之间的关联关系是 N:1 的关系
                解释：
                    并发领域，一把锁可以保护多个资源，不允许多把锁保护一个资源；
                类似球赛门票的管理：
                    一个座位（资源），我们只能用一张票(锁)来保护，如果多发了重复的票，那就要打架
                现实中：
                    是可以多把锁保护同一个资源
        案例：
            1。把 value 改成静态变量，把 addOne() 方法改成静态方法
            class SafeCalc {
                static long value = 0L;
                synchronized long get() {
                    return value;
                }
                synchronized static void addOne() {
                    value += 1;
                }
            }
            问题：此时 get() 方法和 addOne() 方法是否存在并发问题呢？
            分析：
                1。改动后的代码是用两个锁保护一个资源
                    受保护的资源：静态变量 value
                    两个锁分别是：this 和 SafeCalc.class
                2。两个临界区没有互斥关系
                    原因：
                        临界区 get() 和 addOne() 是用两个锁保护的
                3。临界区 addOne() 对 value 的修改对临界区 get() 也没有可见性保证，这就导致并发问题了。

04 | 互斥锁（下）：如何用一把锁保护多个资源？
    背景：
        1。提到受保护资源和锁之间合理的关联关系应该是 N:1 的关系
        2。可以用一把锁来保护多个资源，但是不能用多把锁来保护一个资源
    问题：
        如何保护多个资源
    思考：
        首先要区分这些资源是否存在关联关系。
    保护没有关联关系的多个资源
        现实世界案例：
            球场的座位和电影院的座位就是没有关联关系的
            方法：
                球赛有球赛的门票，电影院有电影院的门票，各自管理各自的。
        编程案例：
            1。银行业务中有针对账户余额（余额是一种资源）的取款操作
            2。也有针对账户密码（密码也是一种资源）的更改操作
            方法：
                可以为账户余额和账户密码分配不同的锁来解决并发问题
            具体代码：
                参考：profit.jikeshijian.bingfabiancheng.AccountAtomicNotRelated04
            思考：
                也可以用一把互斥锁来保护多个资源
                例如：
                    我们可以用 this 这一把锁来管理账户类里所有的资源：账户余额和用户密码
                具体实现：
                    所有的方法都增加同步关键字 synchronized 就可以了
                缺点：
                    用一把锁有个问题，就是性能太差
                    比如：
                        会导致取款、查看余额、修改密码、查看密码这四个操作都是串行的。而我们用两把锁，取款和修改密码是可以并行的
        方法：(细粒度锁)
            用不同的锁对受保护资源进行精细化管理
        优点：
            能够提升性能

    保护有关联关系的多个资源
        案例：
        有关联有问题的代码
        参考：
            profit.jikeshijian.bingfabiancheng.AccountAtomicNotRelated04
        问题：
            怎么保证转账操作 transfer() 没有并发问题呢？
            猜测：
                synchronized 关键字修饰一下 transfer() 
            代码参考：
                profit.jikeshijian.bingfabiancheng.AccountRelatedSynchronized04
                分析：
                    多个资源一把锁
                        锁：this
                        多个资源：
                            1。转出账户的余额 this.balance
                            2。转入账户的余额 target.balance
                问题点：
                    this这把锁只能保护自己的资源，不能保护别人的资源

        模拟场景：
            1。假设有 A、B、C 三个账户，余额都是 200 元，我们用两个线程分别执行两个转账操作
            2。账户 A 转给账户 B 100 元，账户 B 转给账户 C 100 元
            预期最后的结果：
                账户 A 的余额是 100 元，账户 B 的余额是 200 元， 账户 C 的余额是 300 元

            分析执行过程：
                1。假设线程 1 执行账户 A 转账户 B 的操作，线程 2 执行账户 B 转账户 C 的操作
                2。这两个线程分别在两颗 CPU 上同时执行
                问题：
                    这两个线程不是互斥的，可以同时进入临界区 transfer()
                原因：
                    1。因为线程 1 锁定的是账户 A 的实例（A.this）
                    2。而线程 2 锁定的是账户 B 的实例（B.this）
                现象：
                    1。线程 1 和线程 2 都会读到账户 B 的余额为 200
                    2。导致最终账户 B 的余额可能是 300，可能是 100，就是不可能是 200。
                        300：线程 1 后于线程 2 写 B.balance，线程 2 写的 B.balance 值被线程 1 覆盖
                        100：线程 1 先于线程 2 写 B.balance，线程 1 写的 B.balance 值被线程 2 覆盖

            使用锁的正确姿势
                背景：
                    一把锁来保护多个资源。类似现实世界的包场
                    扩展：编程领域应该怎么“包场”呢？
                思路：
                    锁能覆盖所有受保护资源
                问题：
                    this 是对象级别的锁，所以 A 对象和 B 对象都有自己的锁，如何让 A 对象和 B 对象共享一把锁呢？
                方案一：
                    让所有对象都持有一个唯一性的对象，这个对象在创建 Account 时传入
                    具体代码：
                        参考：profit.jikeshijian.bingfabiancheng.AccountRelatedShareLock04
                        缺点：
                            1。要求在创建 Account 对象的时候必须传入同一个对象
                                原因：
                                    如果创建 Account 对象时，传入的 lock 不是同一个对象，那可就惨了，会出现锁自家门来保护他家资产的荒唐事
                            2。传入共享的 lock 真的很难
                                原因：
                                    创建 Account 对象的代码很可能分散在多个工程中
                方案二：
                    用 Account.class 作为共享的锁，不用担心它的唯一性
                    原因：
                        1。Account.class 是所有 Account 对象共享的
                        2。且这个对象是 Java 虚拟机在加载 Account 类的时候创建的
                    优点：
                        我们就无需在创建 Account 对象时传入了，代码更简单
                    代码：
                        参考：profit.jikeshijian.bingfabiancheng.AccountRelatedShareClassLock04

05 | 一不小心就死锁了，怎么办？
    背景：
        我们用 Account.class 作为互斥锁，来解决银行业务里面的转账问题
        现象：
            1。虽然这个方案不存在并发问题，但是所有账户的转账操作都是串行的
            2。例如账户 A 转账户 B、账户 C 转账户 D 这两个转账操作现实世界里是可以并行的
        问题：性能太差
        思路：
            参考现实世界，例如，现实转账
    向现实世界要答案
        现实柜员转账可能遇到的情况：
            1。文件架上恰好有转出账本和转入账本，那就同时拿走；
            2。如果文件架上只有转出账本和转入账本之一，那这个柜员就先把文件架上有的账本拿到手，同时等着其他柜员把另外一个账本送回来；
            3。出账本和转入账本都没有，那这个柜员就等着两个账本都被送回来。
        分析：
            1。用两把锁实现，转出账本一把，转入账本另一把
            2。在 transfer() 方法内部，我们首先尝试锁定转出账户 this（先把转出账本拿到手）
            3。然后尝试锁定转入账户 target（再把转入账本拿到手），只有当两者都成功时，才执行转账操作
        具体代码
            参考：profit.jikeshijian.bingfabiancheng.AccountDeadLock05
            优点：
                提高并行度，是性能优化
                    原因：
                        相对于Account.class,锁定范围小，用的细粒度锁
            问题：
                可能会导致死锁
                    死锁：一组互相竞争资源的线程因互相等待，导致“永久”阻塞的现象。
                        举一个现实世界的例子：
                            1。如果有客户找柜员张三做个转账业务：
                            2。账户 A 转账户 B 100 元，此时另一个客户找柜员李四也做个转账业务
                            3。账户 B 转账户 A 100 元，于是张三和李四同时都去文件架上拿账本
                            4。这时候有可能凑巧张三拿到了账本 A，李四拿到了账本 B
                            5。张三拿到账本 A 后就等着账本 B（账本 B 已经被李四拿走），而李四拿到账本 B 后就等着账本 A（账本 A 已经被张三拿走）
                            问题
                                可能会永远等待下去
                            原因：
                                张三不会把账本 A 送回去，李四也不会把账本 B 送回去
            分析代码可能执行的过程：
                1。假设线程 T1 执行账户 A 转账户 B 的操作，账户 A.transfer(账户 B)
                2。同时线程 T2 执行账户 B 转账户 A 的操作，账户 B.transfer(账户 A)
                3。当 T1 和 T2 同时执行完①处的代码时，T1 获得了账户 A 的锁（对于 T1，this 是账户 A）
                4。而 T2 获得了账户 B 的锁（对于 T2，this 是账户 B）
                5。之后 T1 和 T2 在执行②处的代码时，T1 试图获取账户 B 的锁时，发现账户 B 已经被锁定（被 T2 锁定），所以 T1 开始等待
                6。T2 则试图获取账户 A 的锁时，发现账户 A 已经被锁定（被 T1 锁定），所以 T2 也开始等待
                    现象：
                        于是 T1 和 T2 会无期限地等待下去
            方法：
                并发程序一旦死锁，一般没有特别好的方法，很多时候我们只能重启应用
            思考：
                解决死锁问题最好的办法还是规避死锁。

    如何预防死锁
    首先要分析死锁发生的条件：
        背景 ：
            以下这四个条件都发生时才会出现死锁
        条件
            1。互斥
                例子：
                    共享资源 X 和 Y 只能被一个线程占用；
            2。占有且等待
                例子：
                    线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X
            3。不可抢占
                例子：
                    其他线程不能强行抢占线程 T1 占有的资源；
            4。循环等待
                例子：
                    线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等待。
        方法：
            破坏其中一个，就可以成功避免死锁的发生
                1。占用且等待：
                    理论上做法：
                        过程：
                            我们可以一次性申请所有的资源，这样就不存在等待了
                        问题
                            代码如何实现
                                思考：
                                    现实世界的案例：转账操作
                                        它需要的资源有两个，一个是转出账户，另一个是转入账户，当这两个账户同时被申请时，我们该怎么解决这个问题呢？
                                        方法：
                                            1。可以增加一个账本管理员，然后只允许账本管理员从文件架上拿账本
                                                账本管理员：柜员不能直接在文件架上拿账本，必须通过他
                                                例子：
                                                    1。张三同时申请账本 A 和 B，账本管理员如果发现文件架上只有账本 A
                                                    2。这个时候账本管理员是不会把账本 A 拿下来给张三的
                                                    3。只有账本 A 和 B 都在的时候才会给张三
                                                优点：
                                                    保证了“一次性申请所有资源”。
                                    编程领域中的实现：
                                        思考：
                                            1。“同时申请”这个操作是一个临界区，我们也需要一个角色（Java 里面的类）来管理这个临界区
                                            2。我们就把这个角色定为 Allocator。它有两个重要功能，
                                              分别是：同时申请资源 apply() 和同时释放资源 free()。
                                        具体实现：
                                            1。账户 Account 类里面持有一个 Allocator 的单例（必须是单例，只能由一个人来分配资源）
                                            2。当账户 Account 在执行转账操作的时候，
                                            3。首先向 Allocator 同时申请转出账户和转入账户这两个资源，成功后再锁定这两个资源
                                            4。当转账操作执行完，释放锁之后
                                            5。我们需通知 Allocator 同时释放转出账户和转入账户这两个资源
                                        代码参考：
                                            profit.jikeshijian.bingfabiancheng.AllocatorDeadLock05
                                            profit.jikeshijian.bingfabiancheng.AccountDestroyHaveAndWait05
                2。不可抢占：
                    理论上做法：
                        过程：
                            1。占用部分资源的线程进一步申请其他资源时
                            2。如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。
                        问题
                            代码如何实现
                                思考：
                                    1。核心是要能够主动释放它占有的资源，这一点 synchronized 是做不到的
                                        原因：
                                            1。synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了
                                            2。线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。
                                    2。通过java.util.concurrent下lock解决

                3。循环等待：
                    理论上做法：
                        过程：
                            可以靠按序申请资源来预防
                                按序申请：
                                    概念：
                                        1。指资源是有线性顺序的，申请的时候可以先申请资源序号小的
                                        2。再申请资源序号大的，
                                作用：
                                    避免循环
                        问题
                            代码如何实现
                                思考：
                                    1。需要对资源进行排序，然后按序申请资源。
                                        具体过程：
                                            1。假设每个账户都有不同的属性 id，这个 id 可以作为排序字段
                                            2。申请的时候，我们可以按照从小到大的顺序来申请
                                        代码参考：
                                        profit.jikeshijian.bingfabiancheng.AccountDeadLockDestroyCicyleWait05
                                            说明：
                                                1。①~⑥处的代码对转出账户（this）和转入账户（target）排序，
                                                2。然后按照序号从小到大的顺序锁定账户

06 | 用“等待-通知”机制优化循环等待
    背景：
        // 一次性申请转出账户和转入账户，直到成功
        while(!actr.apply(this, target))
        1。如果 apply() 操作耗时非常短，而且并发冲突量也不大时，这个方案还挺不错的
            原因：
                1。循环上几次或者几十次就能一次性获取转出账户和转入账户了
        2。如果 apply() 操作耗时长，或者并发冲突量大的时候，循环等待这种方案就不适用了
            原因：
                可能要循环上万次才能获取到锁
            问题：
                太消耗 CPU
                    思路：
                        1。如果线程要求的条件（转出账本和转入账本同在文件架上）不满足，则线程阻塞自己，进入等待状态
                        2。当线程要求的条件（转出账本和转入账本同在文件架上）满足后，通知等待的线程重新执行
                    优点：
                        使用线程阻塞的方式就能避免循环等待消耗 CPU 的问题。
                    问题：
                        Java 语言是如何支持等待 - 通知机制
    现实中的案例：
        完美的就医流程
            1。患者先去挂号，然后到就诊门口分诊，等待叫号；
            2。当叫到自己的号时，患者就可以找大夫就诊了；
            3。就诊过程中，大夫可能会让患者去做检查，同时叫下一位患者；
            4。当患者做完检查后，拿检测报告重新分诊，等待叫号；
            5。当大夫再次叫到自己的号时，患者再去找大夫就诊。
        优点：
            1。不仅能够保证同一时刻大夫只为一个患者服务
            2。而且还能够保证大夫和患者的效率
        复杂点在哪里，容易忽视的细节：
            1。患者到就诊门口分诊,类似于线程要去获取互斥锁;
                当患者被叫到时，类似线程已经获取到锁了。
            2。大夫让患者去做检查（缺乏检测报告不能诊断病因），类似于线程要求的条件没有满足。
            3。患者去做检查，类似于线程进入等待状态;
            4.然后大夫叫下一个患者，这个步骤我们在前面的等待 - 通知机制中(忽视了)
                这个步骤对应到程序里，本质是线程释放持有的互斥锁。
            5。患者做完检查，类似于线程要求的条件已经满足
            6.患者拿检测报告重新分诊，类似于线程需要重新获取互斥锁（忽视）
        引出了等待 - 通知机制原理：
    等待 - 通知机制：
        原理：
            1。线程首先获取互斥锁，当线程要求的条件不满足时，释放互斥锁，进入等待状态；
            2。当要求的条件满足时，通知等待的线程，重新获取互斥锁。
    用 synchronized 实现等待 - 通知机制
        背景：
        等待队列：
            1。同一时刻，只允许一个线程进入 synchronized 保护的临界区
                    临界区：可以看作大夫的诊室
            2。当有一个线程进入临界区后，其他线程就只能进入图中左边的等待队列里等待
                    等待队列等待：相当于患者分诊等待
            与互斥锁的关系：
                1。等待队列和互斥锁是一对一的关系
                2。每个互斥锁都有自己独立的等待队列。
    具体方法实现：
        1。wait() 方法
            前提：
                当一个线程进入临界区后，由于某些条件不满足，需要进入等待状态
            原理：
                1。当前线程就会被阻塞，并且进入到右边的等待队列中，这个等待队列也是互斥锁的等待队列
                2。线程在进入等待队列的同时，会释放持有的互斥锁
                3。线程释放锁后，其他线程就有机会获得锁，并进入临界区了。
        2。notify()方法/notifyAll() 方法
            前提：
                那线程要求的条件满足时，该怎么通知这个等待的线程呢？
            原理：
                1。会通知等待队列（互斥锁的等待队列）中的线程，告诉它条件曾经满足过
                    问题：为什么说是曾经满足过呢？
                    原因：
                        1。因为notify() 只能保证在通知时间点，条件是满足的
                        2。而被通知线程的执行时间点和通知的时间点基本上不会重合
                        3。所以当线程执行的时候，很可能条件已经不满足了（保不齐有其他线程插队）
                2。被通知的线程要想重新执行，仍然需要获取到互斥锁
                    原因：
                        曾经获取的锁在调用 wait() 时已经释放了
        注意：
            1。wait()、notify()、notifyAll() 方法操作的等待队列是互斥锁的等待队列
                现象：
                    1。如果 synchronized 锁定的是 this，那么对应的一定是 this.wait()、this.notify()，this.notifyAll()；
                    2。如果 synchronized 锁定的是 target，那么对应的一定是 target.wait()、target.notify()、target.notifyAll() 
            2。三个方法能够被调用的前提是，已经获取了相应的互斥锁
                现象一：(正确)
                    我们会发现 wait()、notify()、notifyAll() 都是在 synchronized{}内部被调用的
                现象二：(异常)
                    如果在 synchronized{}外部调用，或者锁定的 this，而用 target.wait() 调用的话，JVM 会抛出一个运行时异常：
                    java.lang.IllegalMonitorStateException
    运用具体方法思路：
        小试牛刀：一个更好地资源分配器
            问题：
                如何解决一次性申请转出账户和转入账户
            思考点：
                1。这个等待 - 通知机制中，我们需要考虑以下四个要素。
                1。互斥锁：上一篇文章我们提到 Allocator 需要是单例的，所以我们可以用 this 作为互斥锁；
                2。线程要求的条件：转出账户和转入账户都没有被分配过。
                3。何时等待：线程要求的条件不满足就等待
                4。何时通知：当有线程释放账户时就通知。
            代码实现：
                while(条件不满足) {
                    wait();
                }
            具体代码参考：
                profit.jikeshijian.bingfabiancheng.AccountDeadLockDestroyHaveAndWait05
            增加wait-notify机制变为下面的代码
                profit.jikeshijian.bingfabiancheng.AllocatorDeadLockWaitAndNotify06

    尽量使用 notifyAll()
        notify()：
            概念：
                随机地通知等待队列中的一个线程
            猜测：
                notify() 更好一些
            原因：
                即便通知所有线程，也只有一个线程能够进入临界区
            风险：
                可能导致某些线程永远不会被通知到。
        notifyAll() ：
            概念：
                会通知等待队列中的所有线程
            建议尽量使用notifyAll()
        案例：
            1。假设我们有资源 A、B、C、D，线程 1 申请到了 AB，线程 2 申请到了 CD，
            2。此时线程 3 申请 AB，会进入等待队列（AB 分配给线程 1，线程 3 要求的条件不满足）
            3。线程 4 申请 CD 也会进入等待队列
            4。我们再假设之后线程 1 归还了资源 AB，如果使用 notify() 来通知等待队列中的线程
            5。有可能被通知的是线程 4，但线程 4 申请的是 CD，所以此时线程 4 还是会继续等待
            6。而真正该唤醒的线程 3 就再也没有机会被唤醒了。

07 | 安全性、活跃性以及性能问题
    1。安全性问题
        背景：
            这个方法不是线程安全的，这个类不是线程安全的
        问题一：
            那什么是线程安全呢？
            答案：
                正确性，含义就是程序按照我们期望的执行，不要让我们感到意外
        问题二：
            那如何才能写出线程安全的程序呢？
            思考：
                1。并发 Bug 的三个主要源头，原子性问题、可见性问题和有序性问题
                2。论上线程安全的程序，就要避免出现原子性问题、可见性问题和有序性问题。
        问题三：
            那是不是所有的代码都需要认真分析一遍是否存在这三个问题呢？
            答案：
                1。当然不是。
                2。其实只有一种情况需要：存在共享数据并且该数据会发生变化，通俗地讲就是有多个线程会同时读写同一数据。
                    思路：
                        保证线程的安全性，只要做到不共享数据或者数据状态不发生变化
                    技术方案的落地：
                        线程本地存储（Thread Local Storage，TLS）、不变模式等等
        共享会发生变化的数据的场景
            场景一：
                数据竞争：
                    1。当多个线程同时访问同一数据，并且至少有一个线程会写这个数据的时候，
                    2。如果我们不采取防护措施，那么就会导致并发 Bug
                    代码示例：
                        public class Test {
                            private long count = 0;
                            void add10K() {
                                int idx = 0;
                                while(idx++ < 10000) {
                                        count += 1;
                                    }
                            }
                        }
                    演进一：
                        1。增加两个被 synchronized 修饰的 get() 和 set() 方法
                        2。add10K() 方法里面通过 get() 和 set() 方法来访问 value 变量，修改后的代码如下所示。
                            public class Test {
                                private long count = 0;
                                synchronized long get(){
                                    return count；
                                }
                                synchronized void set(long v){
                                    count = v;
                                }
                                void add10K() {
                                    int idx = 0;
                                    while(idx++ < 10000) {
                                        set(get()+1)
                                    }
                                }
                            }
                        结论：
                            修改后的 add10K() 方法并不是线程安全的
                        分析：
                            1。假设 count=0，当两个线程同时执行 get() 方法时，get() 方法会返回相同的值 0
                            2。两个线程执行 get()+1 操作，结果都是 1
                            3。之后两个线程再将结果 1 写入了内存。你本来期望的是 2，而结果却是 1。

                        问题：
                            竞态条件：程序的执行结果依赖线程执行的顺序
                                说明：
                                    1。如果两个线程完全同时执行，那么结果是 1
                                    2。如果两个线程是前后执行，那么结果就是 2。
                                案例：转账操作
                                    1。转账操作里面有个判断条件——转出金额不能大于账户余额
                                    2。但在并发环境里面，如果不加控制，当多个线程同时对一个账号执行转出操作时，就有可能出现超额转出问题。
                                    比如：
                                        1。假设账户 A 有余额 200，线程 1 和线程 2 都要从账户 A 转出 150
                                        2。在下面的代码里，有可能线程 1 和线程 2 同时执行到第 6 行
                                        5。这样线程 1 和线程 2 都会发现转出金额 150 小于账户余额 200，于是就会发生超额转出的情况
                                        具体代码：
                                            class Account {
                                                private int balance;
                                                // 转账
                                                void transfer(
                                                Account target, int amt){
                                                    if (this.balance > amt) { //第六行
                                                        this.balance -= amt;
                                                        target.balance += amt;
                                                    }
                                                }
                                            }
                                类似理解：
                                    在并发场景中，程序的执行依赖于某个状态变量
                                    if (状态变量 满足 执行条件) {
                                        执行操作
                                    }
                    问题
                        面对数据竞争和竞态条件问题，又该如何保证线程的安全性呢？
                        分析：
                            1。其实这两类问题，都可以用互斥这个技术方案
                        思路：
                            1。实现互斥的方案有很多，CPU 提供了相关的互斥指令，操作系统、编程语言也会提供相关的 API
                        方法：
                            锁

    2。活跃性问题
        概念：
            指的是某个操作无法执行下去，例如死锁，活锁，饥饿
        例如：
            死锁：线程会互相等待，而且会一直等待下去，在技术上的表现形式是线程永久地“阻塞”了。
            活锁：有时线程虽然没有发生阻塞，但仍然会存在执行不下去的情况
                    现实世界案例：
                        1。路人甲从左手边出门，路人乙从右手边进门，两人为了不相撞，互相谦让
                        2。路人甲让路走右手边，路人乙也让路走左手边，结果是两人又相撞了
                        解决方法：
                            基本上谦让几次就解决了，因为人会交流啊
                    编程世界：
                        问题：
                            有可能会一直没完没了地“谦让”下去，没有发生阻塞但依然执行不下去的活锁
                        方法：
                            谦让时，尝试等待一个随机的时间就可以了
                            例子：
                                Raft 这样知名的分布式一致性算法中也用到了它。
                            现实案例：
                                1。路人甲走左手边发现前面有人，并不是立刻换到右手边，而是等待一个随机的时间后，再换到右手边
                                2。同样，路人乙也不是立刻切换路线，也是等待一个随机的时间再切换
                                3。由于路人甲和路人乙等待的时间是随机的，所以同时相撞后再次相撞的概率就很低了
                            优点：
                                很简单，非常有效

            饥饿：
                概念：
                    指的是线程因无法访问所需资源而无法执行下去的情况。
                原因：
                    1。如果线程优先级“不均”，在 CPU 繁忙的情况下
                    2。优先级低的线程得到执行的机会很小，就可能发生线程“饥饿”
                    3。持有锁的线程，如果执行的时间过长，也可能导致“饥饿”问题。
                方案：
                    方案一：保证资源充足
                        缺点：
                            场景比较有限
                        原因：
                            资源的稀缺性是没办法解决的

                    方案二：公平地分配资源
                        优点：
                            适用场景相对来说更多一些。
                        问题：
                            那如何公平地分配资源呢？
                            方法：
                                并发编程里，主要是使用公平锁
                                    公平锁：是一种先来后到的方案，线程的等待是有顺序的，排在等待队列前面的线程会优先获得资源。

                    方案三：就是避免持有锁的线程长时间执行
                        缺点：
                            场景比较有限
                        原因：
                            持有锁的线程执行的时间也很难缩短

    3。性能问题
        背景：
            “锁”的过度使用可能导致串行化的范围过大，这样就不能够发挥多线程的优势了
            思路：
                要尽量减少串行
                    问题：
                        1。串行对性能的影响是怎么样的呢？
                        2。假设串行百分比是 5%，我们用多核多线程相比单核单线程能提速多少呢？ 最高只能提高20倍
                            背景
                                阿姆达尔（Amdahl）定律：代表了处理器并行运算之后效率提升的能力
                                    公式：
                                        S=1/((1−p)+p/n)
                                参数说明：
                                    n 可以理解为 CPU 的核数，p 可以理解为并行百分比，那（1-p）就是串行百分比了
                                    假设cpu无穷大，加速比 S 的极限就是 20
                            回答：
                                如果我们的串行率是 5%，那么我们无论采用什么技术，最高也就只能提高 20 倍的性能。
                            结论：
                                串行对性能的影响至关重要要，一定要关注

        问题：
            怎么才能避免锁带来的性能问题呢？
            回答：
                Java SDK 并发包里之所以有那么多东西，有很大一部分原因就是要提升在某个特定领域的性能。
            方案层面来说：
                1。既然使用锁会带来性能问题，那最好的方案自然就是使用无锁的算法和数据结构了。
                    例如：
                        1。线程本地存储 (Thread Local Storage, TLS)、写入时复制 (Copy-on-write)、乐观锁等
                        2。Java 并发包里面的原子类也是一种无锁的数据结构
                        3。Disruptor 则是一个无锁的内存队列，性能都非常好……
                2。减少锁持有的时间
                    原因：
                        互斥锁本质上是将并行的程序串行化，所以要增加并行度，一定要减少持有锁的时间
                    方法：
                        使用细粒度的锁
                        例如：
                            1。Java 并发包里的 ConcurrentHashMap，它使用了所谓分段锁的技术
                            2。还可以使用读写锁，也就是读是无锁的，只有写的时候才会互斥
        三个指标：
            1。吞吐量：指的是单位时间内能处理的请求数量。吞吐量越高，说明性能越好。
            2。延迟：指的是从发出请求到收到响应的时间。延迟越小，说明性能越好。
            3。并发量：指的是能同时处理的请求数量，一般来说随着并发量的增加、延迟也会增加，基于并发量来说的
                比如：并发量是 1000 的时候，延迟是 50 毫秒。

08 | 管程：并发编程的万能钥匙
    背景：
        管程就是一把解决并发问题的万能钥匙
    什么是管程

        问题：
            为什么 Java 在 1.5 之前仅仅提供了 synchronized 关键字及 wait()、notify()、notifyAll() 这三个看似从天而降的方法？
            猜测：(错误)
                java会提供信号量这种编程原语
                原因：
                    操作系统原理课程告诉我，用信号量能解决所有并发问题
            正确答案：
                Java 采用的是管程技术
                说明
                    1。synchronized 关键字及 wait()、notify()、notifyAll() 这三个方法都是管程的组成部分
                    2。管程和信号量是等价的，所谓等价指的是用管程能够实现信号量，也能用信号量实现管程
                原因：
                    管程更容易使用
        概念：
            1。指的是管理共享变量以及对共享变量的操作过程，让他们支持并发

    MESA 模型
        背景：
            1。先后出现过三种不同的管程模型，分别是：Hasen 模型、Hoare 模型和 MESA 模型
            2。现在广泛应用的是 MESA 模型，并且 Java 管程的实现参考的也是 MESA 模型。

        管程对并发的两大核心问题的解决方法：
        互斥问题：
            参考：
                com/suixingpay/profit/document/java并发编程/图片/第8讲管程模型的代码化语义.png
            概念：
                同一时刻只允许一个线程访问共享资源
            解决思路：
                将共享变量及其对共享变量的操作统一封装起来
                过程：
                    1。管程 X 将共享变量 queue 这个队列和相关的操作入队 enq()、出队 deq() 都封装起来了
                    2。线程 A 和线程 B 如果想访问共享变量 queue，只能通过调用管程提供的 enq()、deq() 方法来实现
                    3。enq()、deq() 保证互斥性，只允许一个线程进入管程
                特点：
                    管程模型和面向对象高度契合的



        同步问题：
            参考：
                com/suixingpay/profit/document/java并发编程/图片/第8讲MESA管程模型.png
            概念：
                线程之间如何通信、协作
            解决思路
                可以参考下就医流程的思路，
                MESA 管程模型示意图参考对应的图
                    1。在管程模型里，共享变量和对共享变量的操作是被封装起来的
                    2。图中最外层的框就代表封装的意思
                    3。框的上面只有一个入口，并且在入口旁边还有一个入口等待队列
                    4。当多个线程同时试图进入管程内部时，
                    5。只允许一个线程进入，其他线程则在入口等待队列中等待
                        类似：就医流程的分诊，只允许一个患者就诊，其他患者都在门口等待
                分析：
                    1。管程里还引入了条件变量的概念，而且每个条件变量都对应有一个等待队列，
                    问题：
                        那条件变量和等待队列的作用是什么呢？
                        作用：
                            解决线程同步问题
                            案例：
                                针对管城的条件变量案例：
                                    1。假设有个线程 T1 执行出队操作，不过需要注意的是执行出队操作
                                    2。有个前提条件，就是队列不能是空的(类似管程的条件变量)
                                    3。如果线程 T1 进入管程后恰好发现队列是空的，那怎么办呢？等待啊，去哪里等呢？
                                    4。就去条件变量对应的等待队列里面等。此时线程 T1 就去“队列不空”这个条件变量的等待队列中等待
                                        类似：大夫发现你要去验个血，于是给你开了个验血的单子，你呢就去验血的队伍里排队
                                    5。线程 T1 进入条件变量的等待队列后，是允许其他线程进入管程的
                                        类似：和你去验血的时候，医生可以给其他患者诊治
                                针对管城的等待队列案例：
                                    1。再假设之后另外一个线程 T2 执行入队操作，入队操作执行成功之后
                                    2。“队列不空”这个条件对于线程 T1 来说已经满足了
                                    3。此时线程 T2 要通知 T1，告诉它需要的条件已经满足了
                                    4。当线程 T1 得到通知后，会从等待队列里面出来，但是出来之后不是马上执行
                                    5。而是重新进入到入口等待队列里面。
                                        类似：验血完，回来找大夫，需要重新分诊。
                            wait()、notify()、notifyAll() 这三个操作：
                                线程 T1 发现“队列不空”这个条件不满足，需要进到对应的等待队列里等待
                                方法：
                                    调用 wait() 来实现
                                例如：
                                    1。如果我们用对象 A 代表“队列不空”这个条件，那么线程 T1 需要调用 A.wait()
                                    2。同理当“队列不空”这个条件满足时，线程 T2 需要调用 A.notify() 来通知 A 等待队列中的一个线程，此时这个队列里面只有线程 T1
                                    3。至于 notifyAll() 这个方法，它可以通知等待队列中的所有线程。
                                具体代码参考：
                                    profit.jikeshijian.bingfabiancheng.BlockedQueueAwaitAndNotify08

    wait() 的正确姿势
        背景：
            1。有一个编程范式，就是需要在一个 while 循环里面调用 wait()。这个是 MESA 管程特有的。
                while(条件不满足) {
                    wait();
                }
            2。Hasen 模型、Hoare 模型和 MESA 模型的一个核心区别就是当条件满足后，如何通知相关线程
            3。管程要求同一时刻只允许一个线程执行
        问题：
            当线程 T2 的操作使线程 T1 等待的条件满足时，T1 和 T2 究竟谁可以执行呢？
            1。Hasen 模型
                要求 notify() 放在代码的最后
                原因：
                    1。这样 T2 通知完 T1 后，T2 就结束了
                    2。然后 T1 再执行，这样就能保证同一时刻只有一个线程执行。

            2。Hoare 模型
                过程：
                    1。T2 通知完 T1 后，T2 阻塞，T1 马上执行
                    2。等 T1 执行完，再唤醒 T2，也能保证同一时刻只有一个线程执行。
                缺点：
                    相比 Hasen 模型，T2 多了一次阻塞唤醒操作
            3。MESA 管程
                过程：
                    1。T2 通知完 T1 后，T2 还是会接着执行，T1 并不立即执行
                    2。仅仅是从条件变量的等待队列进到入口等待队列里面。
                优点：
                    是 notify() 不用放到代码的最后，T2 也没有多余的阻塞唤醒操作
                缺点：
                    当 T1 再次执行的时候，可能曾经满足的条件，现在已经不满足了，所以需要以循环方式检验条件变量

    notify() 何时可以使用
        背景：
            除非经过深思熟虑，否则尽量使用 notifyAll()
        问题：
        那什么时候可以使用 notify() 呢？需要满足以下三个条件：
            1。所有等待线程拥有相同的等待条件；
            2。所有等待线程被唤醒后，执行相同的操作；
            3。只需要唤醒一个线程。
        案例：
            1。比如上面阻塞队列的例子中，对于“队列不满”这个条件变量
            2。其阻塞队列里的线程都是在等待“队列不满”这个条件
            3。反映在代码里就是下面这 3 行代码
            4。对所有等待线程来说，都是执行这 3 行代码
               1。 重点是 while 里面的等待条件是完全相同的。
                    while (队列已满){
                        // 等待队列不满
                        notFull.await();
                    }
                2。所有等待线程被唤醒后执行的操作也是相同的，
                    // 省略入队操作...
                    // 入队后, 通知可出队
                    notEmpty.signal();
                3。只需要唤醒一个线程。所以上面阻塞队列的代码，使用 signal() 是可以的。

09 | Java线程（上）：Java线程的生命周期
    背景：
        1。实现并发程序的主要手段就是多线程
        2。Java 语言里的线程本质上就是操作系统的线程，它们是一一对应的。
        3。虽然不同的开发语言对于操作系统线程进行了不同的封装，但是对于线程的生命周期这部分，基本上是雷同的。
    通用的线程生命周期
        1。初始状态：
            概念：
                1。指的是线程已经被创建，但是还不允许分配 CPU 执行。
                    被创建：在编程语言层面被创建，操作系统层面，真正的线程还没有创建。
                2。编程语言特有的
        2。可运行状态：
            概念：
                指的是线程可以分配 CPU 执行
            特点：
                在这种状态下，真正的操作系统线程已经被成功创建了，所以可以分配 CPU 执行。
        3。运行状态：
            概念：
                当有空闲的 CPU 时，操作系统会将其分配给一个处于可运行状态的线程，被分配到 CPU 的线程的状态
        4。休眠状态：
            概念：
                运行状态的线程如果调用一个阻塞的 API（例如以阻塞方式读文件）或者等待某个事件（例如条件变量）
            特点：
                1。休眠状态的线程永远没有机会获得 CPU 使用权
                2。当等待的事件出现了，线程就会从休眠状态转换到可运行状态。
        5。终止状态：
            概念：
                线程执行完或者出现异常
            特点：
                不会切换到其他任何状态，进入终止状态也就意味着线程的生命周期结束了。

    Java 中线程的生命周期
        背景：
            1。在操作系统层面，Java 线程中的 BLOCKED、WAITING、TIMED_WAITING 是一种状态，即前面我们提到的休眠状态。
            2。只要 Java 线程处于这三种状态之一，那么这个线程就永远没有 CPU 的使用权。
        生命周期
            1。NEW（初始化状态）
            2。RUNNABLE（可运行 / 运行状态）
            3。BLOCKED（阻塞状态）
            4。WAITING（无时限等待）
            5。TIMED_WAITING（有时限等待）
            6。TERMINATED（终止状态）

        状态之间的转换：
            1。RUNNABLE 与 BLOCKED 的状态转换
                场景：
                    只有一种场景会触发这种转换，就是线程等待 synchronized 的隐式锁
                例子：
                    1。synchronized 修饰的方法、代码块同一时刻只允许一个线程执行，其他线程只能等待
                    2。这种情况下，等待的线程就会从 RUNNABLE 转换到 BLOCKED 状态
                    3。而当等待的线程获得 synchronized 隐式锁时，就又会从 BLOCKED 转换到 RUNNABLE 状态。
                问题：
                    线程调用阻塞式 API 时，是否会转换到 BLOCKED 状态呢？
                    回答：
                        1。在操作系统层面，线程是会转换到休眠状态的
                        2。Java 线程的状态不会发生变化，Java 线程的状态会依然保持 RUNNABLE 状态
                            原因：
                                1。JVM 看来，等待 CPU 使用权（操作系统层面此时处于可执行状态）与等待 I/O（操作系统层面此时处于休眠状态）没有区别
                                2。都是在等待某个资源，所以都归入了 RUNNABLE 状态。
                注意：
                    1。平时所谓的 Java 在调用阻塞式 API 时，线程会阻塞
                    2。指的是操作系统线程的状态，并不是 Java 线程的状态。

            2。RUNNABLE 与 WAITING 的状态转换
                场景：
                    场景一：
                        获得 synchronized 隐式锁的线程，调用无参数的 Object.wait() 方法
                    场景二：
                        调用无参数的 Thread.join() 方法
                        join()方法：一种线程同步方法
                        例子：
                            1。有一个线程对象 thread A，当调用 A.join() 的时候，执行这条语句的线程会等待 thread A 执行完
                            2。等待中的这个线程，其状态会从 RUNNABLE 转换到 WAITING。
                            3。当线程 thread A 执行完，原来等待它的线程又会从 WAITING 状态转换到 RUNNABLE。
                    场景三：
                        调用 LockSupport.park() 方法
                            概念：
                                当前线程会阻塞，线程的状态会从 RUNNABLE 转换到 WAITING
                            LockSupport：Java 并发包中的锁，都是基于它实现的
                                调用 LockSupport.unpark(Thread thread)：
                                    可唤醒目标线程，目标线程的状态又会从 WAITING 状态转换到 RUNNABLE。

            3。RUNNABLE 与 TIMED_WAITING 的状态转换
                场景：
                    1。调用带超时参数的 Thread.sleep(long millis) 方法；
                    2。获得 synchronized 隐式锁的线程，调用带超时参数的 Object.wait(long timeout) 方法；
                    3。调用带超时参数的 Thread.join(long millis) 方法；
                    4。调用带超时参数的 LockSupport.parkNanos(Object blocker, long deadline) 方法；
                    5。调用带超时参数的 LockSupport.parkUntil(long deadline) 方法
            4。从 NEW 到 RUNNABLE 状态
                New状态：
                    Java 刚创建出来的 Thread 对象就是 NEW 状态
                        创建 Thread 对象主要有两种方法
                            1。继承 Thread 对象，重写 run() 方法
                                // 自定义线程对象
                                class MyThread extends Thread {
                                    public void run() {
                                        // 线程需要执行的代码
                                        ......
                                    }
                                }
                                // 创建线程对象
                                MyThread myThread = new MyThread();

                            2。实现 Runnable 接口，重写 run() 方法，并将该实现类作为创建 Thread 对象的参数

                                // 实现 Runnable 接口
                                class Runner implements Runnable {
                                    @Override
                                    public void run() {
                                        // 线程需要执行的代码
                                        ......
                                    }
                                }
                                // 创建线程对象
                                Thread thread = new Thread(new Runner());
                    特点：
                        1。NEW 状态的线程，不会被操作系统调度，因此不会执行
                        2。Java 线程要执行，就必须转换到 RUNNABLE 状态
                    问题：
                        如何NEW 状态转换到 RUNNABLE
                        方法：
                            只要调用线程对象的 start() 方法就可以了
                                MyThread myThread = new MyThread();
                                // 从 NEW 状态转换到 RUNNABLE 状态
                                myThread.start()；

            5。从 RUNNABLE 到 TERMINATED 状态
                背景：
                    1。不建议使用stop方法
                        原因：
                            1。真的杀死线程，不给线程喘息的机会，如果线程持有 synchronized 隐式锁，也不会释放
                            2。那其他线程就再也没机会获得 synchronized 隐式锁
                    2。类似的方法还有 suspend() 和 resume() 方法也不建议使用
                interrupt()方法：
                    概念：
                        通知线程，线程有机会执行一些后续操作，同时也可以无视这个通知
                    问题：
                        被 interrupt 的线程，是怎么收到通知的呢？
                    方法：
                      一种是异常，另一种是主动检测。
                        异常
                            异常例子一：
                                1。当线程 A 处于 WAITING、TIMED_WAITING 状态时，如果其他线程调用线程 A 的 interrupt() 方法
                                2。会使线程 A 返回到 RUNNABLE 状态，同时线程 A 的代码会触发 InterruptedException 异常
                                    原因：
                                       1。WAITING、TIMED_WAITING 状态的触发条件，都是调用了类似 wait()、join()、sleep() 这样的方法
                                        2。我们看这些方法的签名，发现都会 throws InterruptedException 这个异常
                                            这个异常的触发条件就是：其他线程调用了该线程的 interrupt() 方法。
                            异常例子二：
                                1。当线程 A 处于 RUNNABLE 状态时，并且阻塞在 java.nio.channels.InterruptibleChannel 上时
                                2。如果其他线程调用线程 A 的 interrupt() 方法，线程 A 会触发 java.nio.channels.ClosedByInterruptException 这个异常
                                3。而阻塞在 java.nio.channels.Selector 上时，如果其他线程调用线程 A 的 interrupt() 方法
                                4。线程 A 的 java.nio.channels.Selector 会立即返回。
                        主动检测例子：
                            如果线程处于 RUNNABLE 状态，并且没有阻塞在某个 I/O 操作上
                            比如：
                                1。中断计算圆周率的线程 A，这时就得依赖线程 A 主动检测中断状态了
                                2。如果其他线程调用线程 A 的 interrupt() 方法，那么线程 A 可以通过 isInterrupted() 方法，检测是不是自己被中断了

10 | Java线程（中）：创建多少线程才是合适的？
    背景
        1。用多线程还是比较简单的，但是使用多少个线程却是个困难的问题
        2。各种线程池的线程数量调整成多少是合适的？
        3。Tomcat 的线程数、Jdbc 连接池的连接数是多少？
        4。那我们应该如何设置合适的线程数呢？
    前提思考：
        1。为什么要使用多线程？
            目的：
                提升程序性能
                    感性认识：(不靠谱)
                        就是快、快、快
                    问题：如何度量性能。
                        度量性能指标：
                            1。延迟
                                概念：
                                    指的是发出请求到收到响应这个过程的时间
                                特点：
                                    延迟越短，意味着程序执行得越快，性能也就越好。
                            2。吞吐量
                                概念：
                                    指的是在单位时间内能处理请求的数量；
                                特点：
                                    吞吐量越大，意味着程序能处理的请求越多，性能也就越好。
                            两者的联系：
                                1。同等条件下，延迟越短，吞吐量越大
                                2。它们隶属不同的维度（一个是时间维度，一个是空间维度），并不能互相转换
                理性认识：
                    提升性能，从度量的角度，主要是降低延迟，提高吞吐量
                    问题：
                        怎么降低延迟，提高吞吐量呢？
                    方法：
                        1。优化算法：属于算法范畴
                        2。将硬件的性能发挥到极致：跟并发编程息息相关
                            计算机主要有哪些硬件：
                                1。一个是 I/O
                                2。一个是 CPU
                本质：
                    1。在并发编程领域，提升性能，就是提升硬件的利用率
                    2。再具体点来说，就是提升 I/O 的利用率和 CPU 的利用率。
                    问题：
                        操作系统不是已经解决了硬件的利用率问题了吗？
                    答案：
                        的确是这样
                        原因：
                            1。操作系统已经解决了磁盘和网卡的利用率问题，
                            2。利用中断机制还能避免 CPU 轮询 I/O 状态
                            2。也提升了 CPU 的利用率
                        缺点：
                            1。操作系统解决硬件利用率问题的对象往往是单一的硬件设备，
                            2。而我们的并发程序，往往需要 CPU 和 I/O 设备相互配合工作
                         问题：
                            我们需要解决 CPU 和 I/O 设备综合利用率的问
                            方法：
                                多线程。

    2。多线程的应用场景有哪些？
        如何利用多线程来提升 CPU 和 I/O 设备的利用率？
        案例一：
            假设程序按照 CPU 计算和 I/O 操作交叉执行的方式运行，而且 CPU 计算和 I/O 操作的耗时是 1:1
            方式一：(一个线程)
                过程
                    1。如果只有一个线程，执行 CPU 计算的时候，I/O 设备空闲
                    2。执行 I/O 操作的时候，CPU 空闲
                缺点：
                   CPU 的利用率和 I/O 设备的利用率都是 50%。
            方式二：(两个线程)
                过程：
                    1。当线程 A 执行 CPU 计算的时候，线程 B 执行 I/O 操作
                    2。当线程 A 执行 I/O 操作的时候，线程 B 执行 CPU 计算
                优点：
                    CPU 的利用率和 I/O 设备的利用率就都达到了 100%。
                影响：
                    单位时间处理的请求数量翻了一番，也就是说吞吐量提高了 1 倍
                结论：
                    如果 CPU 和 I/O 设备的利用率都很低，那么可以尝试通过增加线程来提高吞吐量。
        案例二：
            方式一：(单核时代)
                背景
                    单核时代，多线程主要就是用来平衡 CPU 和 I/O 设备的
                多线程：
                条件
                        如果程序只有 CPU 计算，而没有 I/O 操作的话
                    缺点：
                        多线程不但不会提升性能，还会使性能变得更差
                    原因：
                        增加了线程切换的成本。

            方式二：(多核时代)
                多线程
                    优点：
                        提升性能
                    案例：
                        计算 1+2+… … +100 亿的值，如果在 4 核的 CPU 上利用 4 个线程执行
                        过程：
                            1。线程 A 计算 [1，25 亿)
                            2。线程 B 计算 [25 亿，50 亿)
                            3。线程 C 计算 [50，75 亿)
                            4。线程 D 计算 [75 亿，100 亿]，之后汇总
                        优点：
                            1。理论上应该比一个线程计算 [1，100 亿] 快将近 4 倍，响应时间能够降到 25%
                            2。一个线程，对于 4 核的 CPU，CPU 的利用率只有 25%
                            3。而 4 个线程，则能够将 CPU 的利用率提高到 100%


    创建多少线程合适？
        背景：
            1。我们的程序一般都是 CPU 计算和 I/O 操作交叉执行的
            2。I/O 设备的速度相对于 CPU 来说都很慢
        CPU 密集型计算：
            概念：
                计算大部分场景下都是纯 CPU 计算
            案例：
                对于一个 4 核的 CPU，每个核一个线程
                理论上
                    创建 4 个线程就可以了，对于 CPU 密集型的计算场景，理论上“线程的数量 =CPU 核数”就是最合适的
                实际的工程中：
                    线程的数量一般会设置为“CPU 核数 +1”
                    原因：
                        1。线程因为偶尔的内存页失效或其他原因导致阻塞时
                        2。这个额外的线程可以顶上，从而保证 CPU 的利用率。


        I/O 密集型计算：
            概念：
                I/O 操作执行的时间相对于 CPU 计算来说都非常长
            案例一：
                1。如果 CPU 计算和 I/O 操作的耗时是 1:1，那么 2 个线程是最合适的
            案例二：
                如果 CPU 计算和 I/O 操作的耗时是 1:2，那多少个线程合适呢？
                答案：
                    3个线程
                    分析
                        1。CPU 在 A、B、C 三个线程之间切换，对于线程 A
                        2。当 CPU 从 B、C 切换回来时，线程 A 正好执行完 I/O 操作
                        3。这样 CPU 和 I/O 设备的利用率都达到了 100%。

            适合单核最佳的线程数是：
                与程序中 CPU 计算和 I/O 操作的耗时比相关的，
                公式：
                    最佳线程数 =1 +（I/O 耗时 / CPU 耗时）
            多核最佳的线程数是
                分析：
                    1。令 R=I/O 耗时 / CPU 耗时
                    2。当线程 A 执行 IO 操作时，另外 R 个线程正好执行完各自的 CPU 计算
                公式：
                    最佳线程数 =CPU 核数 * [ 1 +（I/O 耗时 / CPU 耗时）]

11 | Java线程（下）：为什么局部变量是线程安全的？
    背景：
    多个线程同时访问共享变量的时候，会导致并发问题。
    问题：
        那在 Java 语言里，是不是所有变量都是共享变量呢？
        原因：
            看到给方法里面的局部变量设置同步
            问题：
                Java 方法里面的局部变量是否存在并发问题呢？
                案例：
                    会根据传入的参数 n ，返回 1 到 n 的斐波那契数列，斐波那契数列类似这样
                过程：
                    1。1、1、2、3、5、8、13、21、34……第 1 项和第 2 项是 1
                    2。从第 3 项开始，每一项都等于前两项之和
                    3。数组 r 用来保存数列的结果，每次计算完一项，都会更新数组 r 对应位置中的值
                        具体代码：

                            // 返回斐波那契数列
                            int[] fibonacci(int n) {
                                // 创建结果数组
                                int[] r = new int[n];
                                // 初始化第一、第二个数
                                r[0] = r[1] = 1;  // ①
                                // 计算 2..n
                                for(int i = 2; i < n; i++) {
                                    r[i] = r[i-2] + r[i-1];
                                }
                                return r;
                            }
                问题：
                    当多个线程调用 fibonacci() 这个方法的时候，数组 r 是否存在数据竞争（Data Race）呢？
                模拟：
                    1。假设多个线程执行到 ① 处，多个线程都要对数组 r 的第 1 项和第 2 项赋值
                    2。这里看上去感觉是存在数据竞争的，不过感觉再次欺骗了你。
                思考：
                    局部变量存在哪里，跟什么有关系（调用栈）
    方法是如何执行的：
        翻译成 CPU 的指令相对简单：
            例如：
                1。int a = 7；//声明一个 int 变量 a
                2。int[] b = fibonacci(a);//调用方法 fibonacci(a)
                3。int[] c = b;//将 b 赋值给 c。
        方法的调用比较复杂：
            过程：
                1。当调用 fibonacci(a) 的时候，CPU 要先找到方法 fibonacci() 的地址
                2。然后跳转到这个地址去执行代码
                3。最后 CPU 执行完方法 fibonacci() 之后，要能够返回
                4。首先找到调用方法的下一条语句的地址：也就是int[] c=b;的地址
                5。再跳转到这个地址去执行。
                问题：
                    CPU 去哪里找到调用方法的参数和返回地址？
                答案：
                    通过 CPU 的堆栈寄存器
                        背景：
                            1。CPU 支持一种栈结构，栈你一定很熟悉了，就像手枪的弹夹，先入后出
                            2。因为这个栈是和方法调用相关的，因此经常被称为调用栈。
                            案例：
                                有三个方法 A、B、C，他们的调用关系是 A->B->C（A 调用 B，B 调用 C）
                            调用原理
                                1。每个方法在调用栈里都有自己的独立空间，称为栈帧
                                2。每个栈帧里都有对应方法需要的参数和返回地址
                                4。当调用方法时，会创建新的栈帧，并压入调用栈
                                5。当方法返回时，对应的栈帧就会被自动弹出
                            结论：
                                栈帧和方法是同生共死的。
            原理：
                方法的调用也是利用栈结构解决
    局部变量存哪里？
        概念：
            局部变量就是放到了调用栈里
        原因：
            1。局部变量的作用域是方法内部，也就是说当方法执行完
            2。局部变量就没用了，局部变量应该和方法同生共死
            3。调用栈的栈帧就是和方法同生共死的，所以局部变量放到调用栈里那儿是相当的合理

    调用栈与线程
        背景：
            两个线程可以同时用不同的参数调用相同的方法
        问题：
            那调用栈和线程之间是什么关系呢？
            答案：
                每个线程都有自己独立的调用栈。
                原因：
                    因为如果不是这样，那两个线程就互相干扰了
        概念：
            1。因为每个线程都有自己的调用栈
            2。局部变量保存在线程各自的调用栈里面，不会共享，也就没有并发问题
                原因：
                    没有共享，就没有伤害。
    线程封闭
        概念：
            1。方法里的局部变量，因为不会和其他线程共享，所以没有并发问题；
            2。官方点：仅在单线程内访问数据
        案例：
            数据库连接池里获取的连接 Connection，在 JDBC 规范里并没有要求这个 Connection 必须是线程安全的
            原因：
                1。数据库连接池通过线程封闭技术，保证一个 Connection 一旦被一个线程获取之后
                2。在这个线程关闭 Connection 之前的这段时间里，不会再分配给其他线程，从而保证了 Connection 不会有并发问题。

12 | 如何用面向对象思想写好并发程序？
    背景：
        1。在设计之初都是直接按照单线程的思路来写程序的
        2。而忽略了本应该重视的并发问题；
    问题一：
        面向对象思想与并发编程有关系吗？
        答案：
            没有
            原因：
                它们分属两个不同的领域
            但是在 Java 语言里，面向对象思想能够让并发编程变得更简单。
    问题二：
        那如何才能用面向对象思想写好并发程序呢
        方法：
            从封装共享变量、识别共享变量间的约束条件和制定并发访问策略这三个方面下手
            1。封装共享变量
                背景
                    并发程序，我们关注的一个核心问题，不过是解决多线程同时访问共享变量的问题
                现实世界的案例：
                    1。我们类比过球场门票的管理，现实世界里门票管理的一个核心问题是
                    2。所有观众只能通过规定的入口进入，否则检票就形同虚设
                    编程领域：
                        对于共享变量的访问路径就类似于球场的入口，必须严格控制
                    方法：
                        通过面向对象思路
                封装：
                    概念：
                        1。面向对象思想里面有一个很重要的特性
                        2。将属性和实现细节封装在对象内部
                        3。外界对象只能通过目标对象提供的公共方法来间接访问这些内部属性
                        类比：
                            和门票管理模型匹配度相当的高
                                球场里的座位==对象属性
                                球场入口==公共方法
                        思路：
                            1。把共享变量作为对象的属性
                            2。那对于共享变量的访问路径就是对象的公共方法，
                            3。所有入口都要安排检票程序就相当于我们前面提到的并发访问策略。
                思路：
                    将共享变量作为对象属性封装在内部，对所有公共方法制定并发访问策略。
                案例：
                    1。很多统计程序都要用到计数器来说，下面的计数器程序共享变量只有一个，就是 value
                    2。我们把它作为 Counter 类的属性，并且将两个公共方法 get() 和 addOne() 声明为同步方法，
                    具体代码
                        public class Counter {
                            private long value;
                            synchronized long get(){
                                return value;
                            }
                            synchronized long addOne(){
                                return ++value;
                            }
                        }
                    分析：
                        1。单个共享变量
                        现实给中，多个共享变量
                            例如：
                                信用卡账户有卡号、姓名、身份证、信用额度、已出账单、未出账单等很多共享变量
                            问题
                                如果每个共享变量，都考虑并发安全性，都考虑它的并发安全问题，那我们就累死了
                            现象
                                很多共享变量的值是不会变的，例如信用卡账户的卡号、姓名、身份证。
                            方法：
                                对于这些不会发生变化的共享变量，建议你用 final 关键字来修饰
                            优点：
                                既能避免并发问题，也能很明了地表明你的设计意图

            2。识别共享变量间的约束条件
                背景：
                    识别共享变量间的约束条件非常重要。
                    原因：
                        这些约束条件，决定了并发访问策略。
                案例：
                    1。库存管理里面有个合理库存的概念，库存量不能太高，也不能太低，它有一个上限和一个下限
                    用代码模拟下：
                        参考代码：
                            profit.jikeshijian.bingfabiancheng.SafeShareVarCondtionWM12
                            演进
                            profit.jikeshijian.bingfabiancheng.SafeShareVarAddCondtionWM12
                            上述类中的问题：
                                没有识别出库存下限要小于库存上限这个约束条件之前
                注意：
                    1。一定要识别出所有共享变量之间的约束条件，如果约束条件识别不足，很可能导致制定的并发访问策略南辕北辙
                    2。共享变量之间的约束条件，反映在代码里，基本上都会有 if 语句，所以，一定要特别注意竞态条件。

            3。制定并发访问策略
                背景：
                    制定并发访问策略，是一个非常复杂的事情
                1。避免共享：
                    概念：
                        避免共享的技术主要是利于线程本地存储以及为每个任务分配独立的线程。
                2。不变模式：
                    背景：
                        这个在 Java 领域应用的很少，但在其他领域却有着广泛的应用
                    比如：
                        Actor 模式、CSP 模式以及函数式编程的基础都是不变模式。
                3。管程及其他同步工具
                    概念：
                        1。Java 领域万能的解决方案是管程，但是对于很多特定场景
                        2。使用 Java 并发包提供的读写锁、并发容器等同步工具会更好。
    宏观原则
        1。优先使用成熟的工具类：
            概念：
                Java SDK 并发包里提供了丰富的工具类，基本上能满足你日常的需要
            建议：
                熟悉它们，用好它们，而不是自己再“发明轮子”
            原因：
                并发工具类不是随随便便就能发明成功的。

        2。迫不得已时才使用低级的同步原语
            概念：
                1。低级的同步原语主要指的是 synchronized、Lock、Semaphore 等
                2。这些虽然感觉简单，但实际上并没那么简单，一定要小心使用。
        3。避免过早优化
            概念：
                1。安全第一，并发程序首先要保证安全，出现性能瓶颈后再优化
                2。在设计期和开发期，很多人经常会情不自禁地预估性能的瓶颈，并对此实施优化
                3。但残酷的现实却是：性能瓶颈不是你想预估就能预估的。

13 | 理论基础模块热点问题答疑
    那这个“串行的故事”是怎样的呢？
        01：
            1。CPU 与内存、I/O 的速度差异，系统软件（操作系统、编译器）在解决这个核心矛盾的同时
            2。引入了可见性、原子性和有序性问题，这三个问题就是很多并发程序的 Bug 之源。
        02：
            Java 内存模型，以应对可见性和有序性问题。
        03和04的内容
            多方考量用好互斥锁才是关键，解决原子性
        05
            1。互斥锁是解决并发问题的核心工具，但它也可能会带来死锁问题
            2。介绍了死锁的产生原因以及解决方案；同时还引出一个线程间协作的问题
        06。
            介绍线程间的协作机制：等待 - 通知。
        07。
            换一个角度，站在宏观的角度重新审视并发编程相关的概念和理论
        08。
            1。是 Java 并发编程技术的基础，是解决并发问题的万能钥匙
            2。是 Java 并发编程技术的基础，是解决并发问题的万能钥匙
            3。所以，学好管程，就相当于掌握了一把并发编程的万能钥匙。
        09、10和11：
            1。介绍了线程相关的知识，毕竟 Java 并发编程是要靠多线程来实现的
            2。有针对性地学习这部分知识也是很有必要的，包括线程的生命周期
            3。如何计算合适的线程数以及线程内部是如何执行的。
        12
            介绍了如何用面向对象思想写好并发程序，因为在 Java 语言里，面向对象思想能够让并发编程变得更简单。
    思考题：
        1。 用锁的最佳实践
            《03 | 互斥锁（上）：解决原子性问题》
            synchronized (new Object())
            分析：
                1。每次调用方法 get()、addOne() 都创建了不同的锁，相当于无锁
                2。一个合理的受保护资源与锁之间的关联关系应该是 N:1”
                3。JVM 开启逃逸分析之后，这行代码在实际执行的时候会被优化掉，也就是说在真实执行的时候，这行代码压根就不存在。
            示例代码：
                class SafeCalc {
                    long value = 0L;
                    long get() {
                        synchronized (new Object()) {
                            return value;
                        }
                    }
                    void addOne() {
                        synchronized (new Object()) {
                         value += 1;
                        }
                    }
                }
            《04 | 互斥锁（下）：如何用一把锁保护多个资源？》
                1。一个是锁有可能会变化，另一个是 Integer 和 String 类型的对象不适合做锁。
                2。如果锁发生变化，就意味着失去了互斥功能。
                3。Integer 和 String 类型的对象在 JVM 里面是可能被重用的，除此之外，JVM 里可能被重用的对象还有 Boolean
                4。意味着你的锁可能被其他代码使用，如果其他代码 synchronized(你的锁)
                5。而且不释放，那你的程序就永远拿不到锁，这是隐藏的风险
                class Account {
                    // 账户余额  
                    private Integer balance;
                    // 账户密码
                    private String password;
                    // 取款
                    void withdraw(Integer amt) {
                        synchronized(balance) {
                            if (this.balance > amt){
                                this.balance -= amt;
                            }
                    }
                    }
                    // 更改密码
                    void updatePassword(String pw){
                        synchronized(password) {
                            this.password = pw;
                        }
                    }
                }
                结论：
                    1。锁，应是私有的、不可变的、不可重用的
                    2。我们经常看到别人家的锁，都长成下面示例代码这样，这种写法貌不惊人，却能避免各种意想不到的坑，
                        // 普通对象锁
                        private final Object 
                        lock = new Object();
                        // 静态对象锁
                        private static final Object
                        lock = new Object(); 

        2。锁的性能要看场景
        《05 | 一不小心就死锁了，怎么办？》
            比较while(!actr.apply(this, target));
              这个方法和synchronized(Account.class)的性能哪个更好
                思考：
                    1。要看具体的应用场景，不同应用场景它们的性能表现是不同的。
                    2。如果转账操作非常费时，那么前者的性能优势就显示出来了，因为前者允许 A->B、C->D 这种转账业务的并行。
                    3。不同的并发场景用不同的方案，这是并发编程里面的一项基本原则
                    4。没有通吃的技术和方案，因为每种技术和方案都是优缺点和适用场景的。

        3。竞态条件需要格外关注
            《07 | 安全性、活跃性以及性能问题》
                1。竞态条件问题非常容易被忽略，contains() 和 add() 方法虽然都是线程安全的，但是组合在一起却不是线程安全的
                2。所以你的程序里如果存在类似的组合操作，一定要小心
                    void addIfNotExist(Vector v, 
                        Object o){
                        if(!v.contains(o)) {
                            v.add(o);
                        }
                    }
            解决办法：《12 | 如何用面向对象思想写好并发程序？》
                1。需要将共享变量 v 封装在对象的内部，而后控制并发访问的路径
                2。这样就能有效防止对 Vector v 变量的滥用，从而导致并发问题
                class SafeVector{
                    private Vector v; 
                    // 所有公共方法增加同步控制
                    synchronized
                    void addIfNotExist(Object o){
                        if(!v.contains(o)) {
                            v.add(o);
                        }
                    }
                }
        4。方法调用是先计算参数
            认为set(get()+1);
            分析：
                1。这条语句是进入 set() 方法之后才执行 get() 方法，其实并不是这样的
                2。方法的调用，是先计算参数，然后将参数压入调用栈之后才会执行方法体
                3。方法调用的过程在11这篇文章中我们已经做了详细的介绍
                while(idx++ < 10000) {
                    set(get()+1);
                }
            案例：
                logger.debug("The var1：" +var1 + ", var2:" + var2);
                方法调用前会先计算参数。
                    更好的写法是下面这个：
                        logger.debug("The var1：{}, var2:{}",var1, var2);
                    原因：
                        仅仅是讲参数压栈，而没有参数的计算。使用{}占位符是写日志的一个良好习惯。

        5。InterruptedException 异常处理需小心
        《 09 | Java 线程（上）：Java 线程的生命周期》
            背景：
                1。主要是希望你能够注意 InterruptedException 的处理方式
                2。当你调用 Java 对象的 wait() 方法或者线程的 sleep() 方法时，需要捕获并处理 InterruptedException 异常
                3。本意是通过 isInterrupted() 检查线程是否被中断了，如果中断了就退出 while 循环。
                4。当其他线程通过调用th.interrupt().
                5。来中断 th 线程时，会设置 th 线程的中断标志位，从而使th.isInterrupted()返回 true
                6。这样就能退出 while 循环了。
                    Thread th = Thread.currentThread();
                    while(true) {
                        if(th.isInterrupted()) {
                    break;
                    }
                    // 省略业务代码无数
                    try {
                            Thread.sleep(100);
                        }catch (InterruptedException e){
                         e.printStackTrace();
                    }
                    }
            分析：
                1。这看上去一点问题没有，实际上却是几乎起不了作用。
                    原因：
                        1。这段代码在执行的时候，大部分时间都是阻塞在 sleep(100) 上，当其他线程通过调用th.interrupt().来中断 th 线程时
                        2。大概率地会触发 InterruptedException 异常，在触发 InterruptedException 异常的同时
                        3。JVM 会同时把线程的中断标志位清除，所以这个时候th.isInterrupted()返回的是 false。
                正确的方式：是捕获异常之后重新设置中断标志位
                    try {
                        Thread.sleep(100);
                    }catch(InterruptedException e){
                    // 重新设置中断标志位
                        th.interrupt();
                    }

        6。理论值 or 经验值
            《10 | Java 线程（中）：创建多少线程才是合适的？》
                经验值为“最佳线程 =2 * CPU 的核数 + 1”，是否合理？
                原因：
                    1。从理论上来讲，这个经验值一定是靠不住的
                    2。但是经验值对于很多“I/O 耗时 / CPU 耗时”不太容易确定的系统来说，却是一个很好到初始值。
                确定过程：
                    1。最佳线程数最终还是靠压测来确定的
                    2。实际工作中大家面临的系统，“I/O 耗时 / CPU 耗时”往往都大于 1，所以基本上都是在这个初始值的基础上增加
                    3。增加的过程中，应关注线程数是如何影响吞吐量和延迟的
                    4。一般来讲，随着线程数的增加，吞吐量会增加，延迟也会缓慢增加
                    5。但是当线程数增加到一定程度，吞吐量就会开始下降，延迟会迅速增加
                实际：
                    1。不同的 I/O 模型对最佳线程数的影响非常大
                        案例：
                            大名鼎鼎的 Nginx 用的是非阻塞 I/O，
                            采用的是多进程单线程结构，Nginx 本来是一个 I/O 密集型系统
                        答案：最佳进程数设置的却是 CPU 的核数，完全参考的是 CPU 密集型的算法。

14 | Lock&Condition（上）：隐藏在并发包中的管程
    背景：
        1。并发编程的两大核心问题：
            1。互斥：同一时刻只允许一个线程访问共享资源
            2。同步：即线程之间如何通信、协作
            管程都可以解决
        2。Java SDK 并发包通过 Lock 和 Condition 两个接口来实现管程
            1。其中 Lock 用于解决互斥问题
            2。Condition 用于解决同步问题。
    问题"
        1。为什么还要在 SDK 里提供另外一种实现呢？
        2。区别在哪里呢？是否是重复造轮子呢？
    再造管程的理由
        背景：
            1。Java 的 1.5 版本中，synchronized 性能不如 SDK 里面的 Lock
            2。但 1.6 版本之后，synchronized 做了很多优化，将性能追了上来，所以 1.6 之后的版本又有人推荐使用 synchronized 了
        问题：
            破坏不可抢占条件方案，synchronized 没有办法解决
        原因：
            1。是 synchronized 申请资源的时候，如果申请不到，线程直接进入阻塞状态了
            2。而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源。但我们希望的是：
                1.对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时
                2.如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。
        解决互斥锁的三个方案：
            1。能够响应中断：
                synchronized的问题：
                    1。持有锁 A 后，如果尝试获取锁 B 失败，那么线程就进入阻塞状态
                    2。一旦发生死锁，就没有任何机会来唤醒阻塞的线程
                思路：
                    1。但如果阻塞状态的线程能够响应中断信号，也就是说当我们给阻塞的线程发送中断信号的时候，能够唤醒它
                    2。那它就有机会释放曾经持有的锁 A
                结果：
                    破坏了不可抢占条件了
                方法：
                    void lockInterruptibly() 
                    throws InterruptedException;
            2。支持超时：
                思路：
                    1。如果线程在一段时间之内没有获取到锁，不是进入阻塞状态，而是返回一个错误，
                    2。那这个线程也有机会释放曾经持有的锁。
                结果：
                    也能破坏不可抢占条件
                方法：
                    boolean tryLock(long time, TimeUnit unit) 
                    throws InterruptedException;
            3。非阻塞地获取锁：
                思路：
                    1。如果尝试获取锁失败，并不进入阻塞状态，而是直接返回
                    2。那这个线程也有机会释放曾经持有的锁
                结果：
                    破坏不可抢占条件。
                方法：
                    // 支持非阻塞获取锁的 API
                    boolean tryLock();
            优点：
                1。可以全面弥补 synchronized 的问题。
                2。这就是重复造轮子的主要原因，体现在 API 上，就是 Lock 接口的三个方法

    如何保证可见性
        背景：
            1。 Java 里多线程的可见性是通过 Happens-Before 规则保证的
            2。而 synchronized 之所以能够保证可见性，也是因为有一条 synchronized 相关的规则
                synchronized 的解锁 Happens-Before 于后续对这个锁的加锁
            问题：
                Java SDK 里面 Lock 靠什么保证可见性呢？
                    例子：
                        线程 T1 对 value 进行了 +=1 操作，那后续的线程 T2 能够看到 value 的正确结果吗？
                        代码示例：
                            class X {
                                private final Lock rtl =new ReentrantLock();
                                int value;
                                public void addOne() {
                                    // 获取锁
                                    rtl.lock();
                                    try {
                                        value+=1;
                                    } finally {
                                        // 保证锁能释放
                                        rtl.unlock();
                                    }
                                }
                            }
            答案：
                可以
            原因：
                1。利用了 volatile 相关的 Happens-Before 规则
                案例;
                    1。Java SDK 里面的 ReentrantLock，内部持有一个 volatile 的成员变量 state，获取锁的时候，会读写 state 的值；
                    2。解锁的时候，也会读写 state 的值（简化后的代码如下面所示）
                    3。也就是说，在执行 value+=1 之前，程序先读写了一次 volatile 变量 state
                    4。在执行 value+=1 之后，又读写了一次 volatile 变量 state
            分析:
                根据相关的 Happens-Before 规则：
                    1。顺序性规则：对于线程 T1，value+=1 Happens-Before 释放锁的操作 unlock()；
                    2。volatile 变量规则：线程T1的unlock()操作Happen-Before于线程T2的lock()操作；
                    3。传递性规则：线程 T1 的 value+=1操作Happen-Before于线程T2的lock()操作
                    具体代码：
                        class SampleLock {
                            volatile int state;
                            // 加锁
                            lock() {
                                // 省略代码无数
                                state = 1;
                            }
                            // 解锁
                            unlock() {
                                // 省略代码无数
                                state = 0;
                            }
                        }

    什么是可重入锁
        可重入锁：
            概念：
                指的是线程可以重复获取同一把锁
                参考代码
                    profit.jikeshijian.bingfabiancheng.XReentrantLock14
        可重入函数：
            概念：
                1。指的是多个线程可以同时调用该函数，每个线程都能得到正确结果；
                2。同时在一个线程内支持线程切换，无论被切换多少次，结果都是正确的
            注意：
                可重入函数是线程安全的。
                原因：
                    1。多线程可以同时执行，还支持线程切换
    公平锁与非公平锁
        背景：
            1。ReentrantLock这个类有两个构造函数
                1。一个是无参构造函数
                2。一个是传入 fair 参数的构造函数
                    fair 参数代表的是锁的公平策略
                        true：需要构造一个公平锁
                        false：构造一个非公平锁
                // 无参构造函数：默认非公平锁
                public ReentrantLock() {
                    sync = new NonfairSync();
                }
                // 根据公平策略参数创建锁
                public ReentrantLock(boolean fair){
                    sync = fair ? new FairSync() 
                    : new NonfairSync();
                }
            分析：
                1。锁都对应着一个等待队列
                2。如果一个线程没有获得锁，就会进入等待队列，当有线程释放锁的时候，就需要从等待队列中唤醒一个等待的线程
                3。如果是公平锁，唤醒的策略就是谁等待的时间长，就唤醒谁，很公平；
                4。如果是非公平锁，则不提供这个公平保证，有可能等待时间短的线程反而先被唤醒。

    用锁的最佳实践
        背景：
            并发大师 Doug Lea《Java 并发编程：设计原则与模式》一书中，推荐的三个用锁的最佳实践
        三个用锁的最佳实践，
            1。永远只在更新对象的成员变量时加锁
            2。永远只在访问可变的成员变量时加锁
            3。永远不在调用其他对象的方法时加锁

15 | Lock和Condition(下):Dubbo如何用管程 实现异步转同步?
    背景：
        1。Java SDK 并发包里的 Lock 有别于 synchronized 隐式锁的三个特性
            能够响应中断、支持超时和非阻塞地获取锁
        2。Java 语言内置的管程里只有一个条件变量，而 Lock&Condition 实现的管程是支持多个条件变量的
        3。并发场景下，支持多个条件变量能够让我们的并发程序可读性更好，实现起来也更容易
            例子：
                实现一个阻塞队列，就需要两个条件变量。
    那如何利用两个条件变量快速实现阻塞队列呢?
        案例：
            1。一个阻塞队列，需要两个条件变量，一个是队列不空(空队列不允许出队)
            2。另一个是队列不满(队列已满不允许入队)
        代码参考：
            profit.jikeshijian.bingfabiancheng.BlockedQueueReentranLock15
        概念：
        1。Lock 和 Condition 实现的管程
            线程等待和通知需要调用 await()、 signal()、signalAll()
        2。 synchronized 实现的管程
            线程等待和通知需要调用  wait()、notify()、notifyAll()
        问题：
            Dubbo 中， Lock 和 Condition 是怎么用的？
        回答：
            同步转异步，并且通过等待-通知机制
    同步与异步：
        概念：
            俗点来讲就是调用方是否需要等待结果
                1。如果需要等待结果，就是同步；
                2。如果不需要等待结果，就是异步。
            例子：
                1。有一个计算圆周率小数点后 100 万位的方法pai1M()，这个方法可能需 要执行俩礼拜，
                2。如果调用pai1M()之后，线程一直等着计算结果，等俩礼拜之后结果返回，就可 以执行 printf("hello world")了，这个属于同步
                3。如果调用pai1M()之后，线程不用等待 计算结果，立刻就可以执行 printf("hello world")，这个就属于异步
                    // 计算圆周率小说点后 100 万位 
                    String pai1M() {
                        // 省略代码无数
                    }
                    pai1M()
                    printf("hello world")
        同步：
            是 Java 代码默认的处理方式
        如何让程序支持异步：
            1。调用方创建一个子线程，在子线程中执行方法调用，这种调用我们称为异步调用;
            2。方法实现的时候，创建一个新的线程执行主要逻辑，主线程直接 return，这种方法我们一般称为异步方法。

    dubbo源码分析
        DemoService service = 初始化部分省略
        String message = 
        service.sayHello("dubbo");
        System.out.println(message);
        分析：
            1。本来发送请求是异步的，但是调用线程却阻塞了
            2。说明 Dubbo 帮我们做了异步转同步的事情
            3。通过调用栈，你能看到线程是阻塞在 DefaultFuture.get() 方法上
        参考：
            https://blog.csdn.net/cowbin2012/article/details/103443899
        调用过程：
            1。调用 lock() 获取锁，在 finally 里面调用 unlock() 释放锁；
            2。获取锁后，通过经典的在循环中调用 await() 方法来实现等待。
            3。当 RPC 结果返回时，会调用 doReceived() 方法，这个方法里面，调用 lock() 获取锁，
            4。在 finally 里面调用 unlock() 释放锁，获取锁后通过调用 signal() 来通知调用线程，结果已经返回，不用继续等待了。
        类似的案例：
            1。创建云主机，就是一个异步的 API，调用虽然成功了，但是云主机并没有创建成功
            2。你需要调用另外一个 API 去轮询云主机的状态
            3。如果你需要在项目内部封装创建云主机的 API，
            4。你也会面临异步转同步的问题，因为同步的 API 更易用。

16 Semaphore：如何快速实现一个限流器
    背景：
        1。Semaphore，现在普遍翻译为“信号量”，以前也曾被翻译成“信号灯”
            原因：
                类似现实生活里 的红绿灯，车辆能不能通行，要看是不是绿灯
        2。在编程世界里，线程能不能执行，也要看 信号量是不是允许。
    信号量模型
        概念：
            一个计数器，一个等待队列，三个方法
                解释：
                    计数器和等待队列对外是透明的，信号量模型提供的三个方法来访问它们
                    三个方法：
                        1。init():设置计数器的初始值。
                        2。down()计数器的值减 1;如果此时计数器的值小于 0，则当前线程将被阻塞，否则当前线程 可以继续执行。
                        3。up():计数器的值加 1;如果此时计数器的值小于或者等于 0，则唤醒等待队列中的一个线 程，并将其从等待队列中移除。
                        注意：
                            三个方法都是原子性的，并且这个原子性是由信号量模型的 实现方保证的。
                        案例：
                            1。Java SDK 里面，信号量模型是由 java.util.concurrent.Semaphore 实现的，
                            2。Semaphore 这个类能够保证这三个方法都是原子操作
                                参考代码：
                                    profit.jikeshijian.bingfabiancheng.Semaphore16
                                    注意：
                                        1。信号量模型里面，down()、up() 这两个操作历史上最早称为 P 操作和 V 操作， 所以信号量模型也被称为 PV 原语
                                        2。在 Java SDK 并发包里，down() 和 up() 对应的则是 acquire() 和 release()。
                                        问题：
                                            信号量如何保持互斥的
                                        参考代码分析
                                            profit.jikeshijian.bingfabiancheng.SemaphoreexclusionOne16
                                                acquire() 就是信号量里的 down() 操作，release() 就是信号量里的 up() 操作。
    快速实现一个限流器
        背景：
            我们用信号量实现了一个最简单的互斥锁功能
            问题：
                既然有 Java SDK 里面提供了 Lock，为啥还要提供一个 Semaphore ?
            原因：
                1。其实实现一个互斥锁，仅仅是 Semaphore 的部分功能
                2。Semaphore 还有一个功能是 Lock 不容易实现的，Semaphore 可以允许多个线程访问一个临界区。
            例如：
                连接池、对象池、线程池等等
                1。数据库连接池，在同一时刻，一定是允许多个线程同时使用连接池的
                2。当然，每个连接在被释放前，是不允许其他线程使用的。
        需求：
            我在工作中也遇到了一个对象池的需求
                对象池：
                    概念：
                        1。指的是一次性创建出 N 个 对象，之后所有的线程重复利用这 N 个对象
                        2。当然对象在被释放前，也是不允许其他线程使用 的。
                    思路：
                        可以用list保存实例对象
                    难点：
                        限流器的设计
                            限流：指 的是不允许多于 N 个线程同时进入临界区
                    方法：
                        想到了信号量的解决方案。
        过程：
            profit.jikeshijian.bingfabiancheng.SemaphoreexclusionOne16
            1。上面的代码中信号量设置成了1，这个 1 表示只允许一个线程进入临界区
            2。把计数器的值设置成对象池里对象的个数 N
        结果：
            就能完美解决对象池的限流问题了
        具体代码参考
            profit.jikeshijian.bingfabiancheng.ObjPoolSemaphoreN16

17 | ReadWriteLock:如何快速实现一个完备的缓存?
    背景：
        1。管程和信号量这两个同步原语在 Java 语言中的实现
        2。理论上用这两个同步原语中 任何一个都可以解决所有的并发问题
    问题：
        那 Java SDK 并发包里为什么还有很多其他的工具类呢
        原因：
            分场景优化性能，提升易用性。
    场景
        读多写少场景
    思考：
        为了优化性能，我们经常会使用缓存
        前提：
            缓存的数据一定是读多写少的
            例子：
                元数据和基础数据基本上不会发生变化(写少)，但是使用它们的地方却很多(读多)。
        例如：
            缓存元数据、缓存基础数据等
        方法：
            ReadWriteLock
            优点：
                非常容易使 用，并且性能很好。
    那什么是读写锁呢?
        背景：
            读写锁，并不是 Java 语言特有的，而是一个广为使用的通用技术
        三条基本原则:
            1。允许多个线程同时读共享变量;
            2。只允许一个线程写共享变量;
            3。如果一个写线程正在执行写操作，此时禁止读线程读共享变量。
        读写锁与互斥锁的区别：
            1。读写锁：
                1。允许多个线程同时读共享变量
                2。读写锁的写操作是互斥的，当一个线程在写共享变量的时候，是不允许其他线程执行写操作和读操作。
            2。互斥锁：不允许多个线程同时读共享变量
    快速实现一个缓存
        例子：
            参考代码：
                profit.jikeshijian.bingfabiancheng.CacheReadWriteLock17
        缓存的使用：
            思路：
                使用缓存首先要解决缓存数据的初始化问题
            如何初始化：
                1。采用一次性加载的方式
                    场景：
                        如果源头数据的数据量不大，就可以采用一次性加载的方式
                    方式：
                        简单
                    具体实现：
                        1。只需在应用启动的时候把源头数据查询出来
                        2。依次调用类似上面示例代码中的 put() 方法就可以 了
                2。可以使用按需加载的方式
                    场景：
                        如果源头数据量非常大，那么就需要按需加载了，按需加载也叫懒加载
                            懒加载：
                                指的是只有当应用查询 缓存，并且数据不在缓存里的时候，才触发加载源头相关数据进缓存的操作。
                    案例：
                        具体参考代码：
                        profit.jikeshijian.bingfabiancheng.CacheNumLargeReadWriteLock17

    读写锁的升级与降级
        ReadWriteLock的升级不允许
            原因：
                1。读锁还没有释放，此时获取写锁，会导致写锁永久等待
                2。最终导致相关线程都被阻塞，永远也没有机会被唤醒
            参考代码：
                profit.jikeshijian.bingfabiancheng.CacheNumLargeReadWriteLockUP17
        ReadWriteLock锁的降级却是允许的
            参考代码
                profit.jikeshijian.bingfabiancheng.CacheDataNumLargeReadWriteLockDown17
    总结：
        1。读写锁类似于 ReentrantLock，也支持公平模式和非公平模式
        2。读锁和写锁都实现了 java.util.concurrent.locks.Lock 接口，所以除了支持 lock() 方法外，tryLock()、 lockInterruptibly() 等方法也都是支持的。
            注意：
            1。只有写锁支持条件变量，
            2。读锁是不支持条件变量的，读锁调用 newCondition() 会抛出 UnsupportedOperationException 异常。
    问题：
        上面的例子解决缓存的初始化问题，但是没有解决数据的同步问题；
            方案一：
                超时机制：
                    概念：
                        1。指的是加载进缓存 的数据不是长久有效的
                        2。而是有时效的，当缓存的数据超过时效，也就是超时之后，这条数据在 缓存中就失效了
                        3。而访问缓存中失效的数据，会触发缓存重新从源头把数据加载进缓存。
            方案二：
                也可以在源头数据发生变化时，快速反馈给缓存
                例子：
                    1。MySQL 作为数据源头，可以通过近实时地解析 binlog 来识别数据是否发生了变化
                    2。如果发生了 变化就将最新的数据推送给缓存。
            方案三：
                数据库和缓存的双写方案

18 | StampedLock:有没有比读写锁更快的锁?
    背景：
        读写锁允许多个线程同时读共享变量，适用于读多写少的场景
        问题：
            在读多写少的场景中，还有没有更快的技术方案呢?
        方法：
            Java在 1.8 这个版本里，提供了一种叫StampedLock的锁，它的性能就比读写锁还要好。
    StampedLock 支持的三种锁模式
        ReadWriteLock 支持两种模式：一种是读锁，一种是写锁
        StampedLock 支持三种模式：写锁、悲观读锁和乐观读
    注意：
        共同点
            1。写锁、悲观读锁的语义和ReadWriteLock的写锁、读锁的 语义非常类似
            2。允许多个线程同时获取悲观读锁，但是只允许一个线程获取写锁，写锁和悲观读 锁是互斥的
        区别：
            3。不同的是:StampedLock 里的写锁和悲观读锁加锁成功之后，都会返回一个 stamp;
            4。然后解锁的时候，需要传入这个 stamp
    示例代码：
        final StampedLock sl =new StampedLock();
        // 获取 / 释放悲观读锁示意代码
        long stamp = sl.readLock();
            try {
                // 省略业务相关代码
            } finally {
                sl.unlockRead(stamp);
            }

        // 获取 / 释放写锁示意代码
        long stamp = sl.writeLock();
            try {
                // 省略业务相关代码
            } finally {
                sl.unlockWrite(stamp);
            }
        结论：
            StampedLock的性能要比 ReadWriteLock要好
        原因：
            1。StampedLock 支持乐观读的方式
                乐观读：不是“乐观读锁”，乐观读这个操作 是无锁的
            2。ReadWriteLock 支持多个线程同时读，但是当多个线程同时读的时候，所有的写操作会被阻塞
            3。而StampedLock 提供的乐观读，是允许一个线程获取写锁的，也就是说不是所有的写操作都被阻塞
        参考示例代码：
            profit.jikeshijian.bingfabiancheng.PointStampedLock18
        上述代码展示了乐观读升级为悲观读锁过程；

    进一步理解乐观读
        背景：
            StampedLock 的乐观读和数据库的乐观锁有异曲同工之妙
        案例：
            在 ERP 的生产模块里，会有多个人通过 ERP 系统提供的 UI 同时修改同一条生产订单
        问题：
            如何保证生产订单数据是并发安全的呢?
        方法：
            乐观锁
        具体代码实现流程：
            1。在生产订单的表 product_doc 里增加了一个数值型版本号字段 version
            2。每次更新 product_doc 这个表的时候，都将 version 字段加 1
            3。生产订单的 UI 在展示的时候，需 要查询数据库
            4。此时将这个 version 字段和其他业务字段一起返回给生产订单 UI
            查询sql：
                select id，... ，version from product_doc where id=777
            更新sql：
                update product_doc set version=version+1，...    where id=777 and version=9

    StampedLock 使用注意事项
        注意：
            1。StampedLock 不支持重入
            2。StampedLock 的悲观读锁、写锁都不支持条件变量，
            3。使用 StampedLock 一定不要调用中断操作如果需要支持中断功能，一定使用可中断的 悲观读锁 readLockInterruptibly() 和写锁 writeLockInterruptibly()
            原因：
                1。如果线程阻塞在 StampedLock 的 readLock() 或者 writeLock() 上时
                2。此时调用该阻塞线程的 interrupt() 方法，会导致 CPU 飙升
                    原因：
                        内部实现里while循环里面对中断的处理有点问题
                案例：
                    参考代码：
                        1。线程 T1 获取写锁之后将自己阻塞，线程 T2 尝试获取悲观读锁，也会阻塞
                        2。如果此时调用线程 T2 的 interrupt() 方法来中断线程 T2 的话，你会发现线程 T2 所在 CPU 会飙升到 100%。
                        final StampedLock lock= new StampedLock();
                        Thread T1 = new Thread(()->{
                            // 获取写锁
                            lock.writeLock();
                            // 永远阻塞在此处，不释放写锁
                            LockSupport.park();
                        });
                        T1.start();
                        // 保证 T1 获取写锁
                        Thread.sleep(100);
                        Thread T2 = new Thread(()->
                            // 阻塞在悲观读锁
                            lock.readLock()
                        );
                        T2.start();
                        // 保证 T2 阻塞在读锁
                        Thread.sleep(100);
                        // 中断线程 T2
                        // 会导致线程 T2 所在 CPU 飙升
                        T2.interrupt();
                        T2.join();
    需要仔细参考的StampedLock 读写模板
        profit.jikeshijian.bingfabiancheng.PointStampedLock18
    思考题
        参考代码：
            StampedLock支持锁的升级和降级
        profit.jikeshijian.bingfabiancheng.StampedLockDownUP18

19 | CountDownLatch和CyclicBarrier:如何让多线程 步调一致?
    案例：对账系统
        1。用户通过在线商城下单，会生成电子订单，保存在订单库;
        2。之后物流会生成派送单给用户发货，派送单保存在派送单库
        3。为了防止漏派送或者重复派送，对账系统每天还会校验是否存在异常订单。
        处理逻辑：
            1。首先查询订单
            2。然后查询派送单
            3。之后对比订单和派送单，将差异写入差异库
    核心代码示例：
        1。就是在一个单线程里面循环查询订单、派送单
        2。然后执行对账，最后将写入差异库。
            参考代码：
                profit.jikeshijian.bingfabiancheng.SerialPDOrdersOptimisedOne19
    演进一：
            利用并行优化对账系统
            瓶颈：
                查询未对账订单 getPOrders() 和查询派送单 getDOrders() 相对较慢
                原因：
                    订单量和派送单量巨大
                分析：
                    对账系统是单线程执行的
                方法：
                    能否利用多线程并行处理。
                    问题：
                        查询未对账订单 getPOrders() 和查询派送 单 getDOrders() 是否可以并行处理呢?
                        答案：
                            可以的
                        原因：
                            这两个操作并没有先后顺序的依赖。
                    方法：
                        两个最耗时的操作并行之后
                    优点：
                        同等时间里，并行执行的吞吐量近乎单线程的 2 倍，
                    参考代码示例：
                        profit.jikeshijian.bingfabiancheng.ParallelPDOrdersOptimisedOne19
                      上述代码的缺点：
                            while 循环里面每次都会创建新的线程，而创建线程可是个耗时的操作
                            思路：
                                创建出来的线程能够循环利用
                            方法：
                                线程池
    用 CountDownLatch 实现线程等待
        CountDownLatch：
            概念：
                1。主要用来解决一个线程等待 多个线程的场景
                2。可以类比旅游团团长要等待所有的游客到齐才能去下一个景点
            缺点：
                1。计数器是不能循环利用的，也就是说一旦计数器减到 0
                2。再有线程调用 await()，该线程会直接通过
        演进二：
            参考代码
                利用线程池的来优化
                    profit.jikeshijian.bingfabiancheng.ThreadPoolPDOrdersOptimisedTwo19
                问题：
                    线程池的方案里，线程根本就不会退出，所以 join() 方法已经失效了。
                思考：
                    1。最直接的办法是弄一个计数器，初始值设置成2，当执行完pos = getPOrders();
                    2。这个操作之后将计数器减1，执行完dos = getDOrders()
                    3。之后也将计数器减1，在主线程 里，等待计数器等于 0;
                    4。当计数器等于 0 时，说明这两个查询操作执行完了。
                    5。等待计数器等于 0 其实就是一个条件变量，用管程实现起来也很简单。
                建议：
                    不要自己实现
                    原因：
                        Java 并发包里已经提供了实现类似功能 的工具类:CountDownLatch
        演进三：
            参考代码：
                profit.jikeshijian.bingfabiancheng.CountDownLatchPDOrdersOptimisedThree19
                优点：
                    getPOrders() 和 getDOrders() 这两个查询操作并行了
                缺点：
                    两个查询操作和对账操 作 check()、save() 之间还是串行的
                分析：
                    1。这两个查询操作和对账操作也是可以并行的
                    2。在执行对账操作的时候，可以同时去执行下一轮的查询操作，
                思考：
                    1。两次查询操作能够和对账操作并行，对账操作还 依赖查询操作的结果(生产者 - 消费者)
                        生产者：两次查询操作
                        消费者：对账操作
                    2。生产者 - 消费者模型，那就需要有个队列，来保存生产者生产的数据，而消费者 则从这个队列消费数据。
    ：
    具体落实方案：
        设计了两个队列，并且两个队列的元素之间还有对应关系
        流程：
            1。订单查询操作将订单查询结果插入订单队列，派送单查询操作将派送单插入派送单队列
            2。这两个队列的元素之间是有一一对应的关系的
        优点：
            数据一定不会乱掉
            原因：
                1。对账操作可以每次从订单队列出一个元素，从派送单队列出一个元素
                2。然后对这两个元素执行对账操作
        问题：
            如何用双队列来实现完全的并行、
            最直接的想法：
                1。一个线程 T1 执行订单的查 询工作，一个线程 T2 执行派送单的查询工作
                2。当线程 T1 和 T2 都各自生产完 1 条数据的时候， 通知线程 T3 执行对账操作
            前提条件：
                1。线程 T1 和线程 T2 的工作要步调一致，不能一个跑得太快，一个跑得太慢
                2。只有这样才能做到各自生产 完 1 条数据的时候，通知线程 T3。
            流程：
                1。线程 T1 和线程 T2 只有都生产完 1 条数据的时候，才能一 起向下执行
                2。线程 T1 和线程 T2 要互相等待，步调要一致
                3。同时当线程 T1 和 T2 都生 产完一条数据的时候，还要能够通知线程 T3 执行对账操作。
            难点：
                1。一个是线程 T1 和 T2 要做到步调一 致
                2。另一个是要能够通知到线程 T3
            思路：
                1。利用一个计数器来解决这两个难点，计数器初始化为 2
                2。线程 T1 和 T2 生产完一条数 据都将计数器减 1，如果计数器大于 0 则线程 T1 或者 T2 等待。
                3。如果计数器等于 0，则通知线程 T3，并唤醒等待的线程 T1 或者 T2，与此同时，将计数器重置为 2
                4。这样线程 T1 和线程 T2 生产 下一条数据的时候就可以继续使用这个计数器了。
            不建议：
                自己实现
            原因：
                Java 并发包里也已经提供了相关的工具 类:CyclicBarrier
    用 CyclicBarrier 实现线程同步
        演进四：
            参考代码：
                profit.jikeshijian.bingfabiancheng.CyclicBarrierPDOrdersOptimisedFour19
        CyclicBarrier
            概念：
                是一组线程之间互相等待，更像是几个驴友之间不离不弃
            优点：
                1。计数器是可以循环利用的
                2。具备自动重置的 功能，一旦计数器减到 0 会自动重置到你设置的初始值
                3。还可以设置回 调函数，可以说是功能丰富。

20 | 并发容器:都有哪些“坑”需要我们填?
    背景：
        1。Java 并发包有很大一部分内容都是关于并发容器的
        2。Java 1.5 之前提供的同步容器虽然也能保证线程安全，但是性能很差
        3。Java 1.5 版本之后提供 的并发容器在性能方面则做了很多优化，并且容器的类型也更加丰富了
    同步容器及其注意事项
        背景：
            1。Java 中的容器主要可以分为四个大类，分别是 List、Map、Set 和 Queue
            2。并不是所有的 Java 容器都是线程安全的
                比如：常用的 ArrayList、HashMap 就不是线程安全的
            问题：
                如何将非线程安全的容器变成线程安全的容器?
                思路：
                    1。把非线程安全的容器封装在对象内部
                    2。然后控制好访问路径就可以了。
                方案一：
                    参考代码
                        以 ArrayList 为例，变成安全的
                            profit.jikeshijian.bingfabiancheng.SafeArrayList20
                方案二：
                    1。Java SDK Collections 这个类中还提供了一套完备的包装类
                    2。分别把 ArrayList、 HashSet 和 HashMap 包装成了线程安全的 List、Set 和 Map。
                        List list = Collections.synchronizedList(new ArrayList());
                        Set set = Collections.synchronizedSet(new HashSet());
                        Map map = Collections.synchronizedMap(new HashMap());
        同步容器
            概念：
                都是基于 synchronized 这个同步关键字实现的
            例子：
                Vector、Stack 和 Hashtable
                特点：
                    1。这三个容器不 是基于包装类实现的，但同样是基于 synchronized 实现的
                    2。对这三个容器的遍历，同样要加锁保 证互斥。
        注意一：
            组合操作往往隐藏着竞态条件问题，即便每个操作都能保证原子性，也并不能保证组合操作的原子性
        注意二：
            用迭代器遍历容器
                示例一：
                    代码
                        List list = Collections.synchronizedList(new ArrayList());
                        Iterator i = list.iterator();
                        while (i.hasNext())
                        foo(i.next());
                    分析：
                        通过迭代器遍 历容器 list，对每个元素调用 foo() 方法
                    问题：
                        这就存在并发问题，这些组合的操作不具备原子性。
                    思路：
                        锁住 list 之后再执行遍历操作
                示例二：
                    代码：
                        List list = Collections.synchronizedList(new ArrayList());
                        synchronized (list) {  
                            Iterator i = list.iterator(); 
                            while (i.hasNext())
                            foo(i.next());
                        }
    并发容器及其注意事项
        背景：
            1。Java 在 1.5 版本之前所谓的线程安全的容器，主要指的就是同步容器
                同步容器缺点：
                    性能差
                        原因：
                            所有方法都用 synchronized 来保证互斥，串行度太高了
            2。因此 Java 在 1.5 及之后版本提供了性能更高的容器，我们一般称为并发容器。
        并发容器：
            1。List：
                CopyOnWriteArrayList:
                    概念：
                        CopyOnWrite：写的时候会将共享变量新复制一份出来
                    优点：
                        读操作完全无锁。
                    实现原理：
                        1。内部维护了一个数组，成员变量 array 就指向这个内部数组
                        2。所有的读操 作都是基于 array 进行的
                        3。迭代器 Iterator 遍历的就是 array 数组。
                    问题：
                        如果在遍历 array 的同时，还有一个写操作，例如增加元素，CopyOnWriteArrayList 是如何处理 的呢?
                    过程：
                        1。CopyOnWriteArrayList 会将 array 复制一份
                        2。然后在新复制处理的数组上执行增加元素的 操作
                        3。执行完之后再将 array 指向这个新的数组
                        4。读写是可以并行的，遍 历操作一直都是基于原 array 执行
                        5。而写操作则是基于新 array 进行
                    注意一：
                        应用场景
                            概念：
                                CopyOnWriteArrayList 仅适用于写操作非常少的场景，而且能够容忍读写的短暂不一致
                            例子：
                                上面的例子中，写入的新元素并不能立刻被遍历到
                    注意二：
                        概念：
                            CopyOnWriteArrayList 迭代器是只读的，不支持增删改。
                            原因：
                                迭代器遍历的仅仅是一个快照，而对快照进行增删改是没有意义的。
            2。Map：
                ConcurrentHashMap
                    特点：
                        key 是无序的
                    注意：
                        key 和 value 都不能为空，否则会抛出NullPointerException
                ConcurrentSkipListMap
                    概念：
                        1。SkipList 本身就是一种数据结构，中文一般都翻译为“跳表”
                        2。跳表插入、删除、查询操作平均的时间复杂度是 O(log n)，理论上和并发线程数没有关系
                        3。在并发度非常高的情况下，若你对 ConcurrentHashMap 的性能还不满意，可以尝试一下 ConcurrentSkipListMap。
                    特点：
                        key 是 有序的
                    场景：
                        需要保证 key 的顺序，就只能使用 ConcurrentSkipListMap。
                    注意：
                        key 和 value 都不能为空，否则会抛出NullPointerException
            3。Set：
                CopyOnWriteArraySet
                ConcurrentSkipListSet
                    使用场景可以参考前面 讲述的 CopyOnWriteArrayList 和 ConcurrentSkipListMap它们的原理都是一样的

            4。Queue：
                背景：
                    1。Java 并发包里面 Queue 这类并发容器是最复杂的
                    2。可以从两个维度来分类
                        1。一个维度是 阻塞与非阻塞
                            阻塞：
                                1。指的是当队列已满时，入队操作阻塞
                                2。当队列已空时，出队操作阻塞。

                        2。一个维度是单端与双端
                            单端：
                                指的是只能队尾入队，队首出队
                            双端
                                指的是队首队尾皆可入 队出队
                        注意：
                            1。Java 并发包里阻塞队列都用 Blocking 关键字标识
                            2。单端队列使用 Queue 标识
                            3。双端队 列使用 Deque 标识

                这两个维度组合后，可以将 Queue 细分为四大类
                    BlockingDeque:
                        双端阻塞队列
                            LinkedBlockingDeque:
                    BlockQueue:
                        单端阻塞队列
                            概念：
                                内部一般会持有一个队列
                            例子：
                                ArrayBlockingQueue:队列是数组
                                LinkedBlockingQueue:链表
                                SynchronousQueue:不持有队列，此时 生产者线程的入队操作必须等待消费者线程的出队操作
                                LinkedTransferQueue:
                                    特点：
                                        融合 LinkedBlockingQueue 和 SynchronousQueue 的功能
                                    优点：
                                        性能比 LinkedBlockingQueue 更好;
                                PriorityBlockingQueue:支持按照优先级出队
                                DelayQueue:支持延时出队。
                    ConcurrentLinkedQueue:单端非阻塞队列
                    ConcurrentLinkedDeque:双端非阻塞队列

                注意：
                    1。使用队列时，需要格外注意队列是否支持有界
                        有界：指的是内部的队列是否有容量限制
                    2。实际工作中，一般都不建议使用无界的队列，因为数据量大了之后很容易导致 OOM
                    3。只有 ArrayBlockingQueue 和 LinkedBlockingQueue 是支持有界 的
                    4。在使用其他无界队列时，一定要充分考虑是否存在导致 OOM 的隐患。
    思考题：
        1。线上系统 CPU 突然飙升，你怀疑有同学在并发场景里使用了 HashMap
        2。因为在 1.8 之前的版本 里并发执行 HashMap.put() 可能会导致 CPU 飙升到 100%，你觉得该如何验证你的猜测呢?
        思路：
            jdk1.8以前的HashMap并发扩容的时候会导致陷入死循环，所以会导致cpu飙升，那么验证猜想我觉得
        验证方法：
            1。线上查故障，用dump分析线程。
            2。用1.8以前的jdk在本地模拟。

21 | 原子类:无锁工具类的典范
    背景
        1。可见性问题可以用 volatile 来解决
        2。原子性问题我们前面一直都是采用的互斥锁方案。
    扩展：
        其实对于简单的原子性问题，还有一种无锁方案
    方案：
        Java SDK 并发包将这种无锁方案封装提炼之 后，实现了一系列的原子类
        问题：
            如何利用原子类 解决累加器问题
        实现过程：
            1。将原来的 long 型变量 count 替换为了原子类 AtomicLong
            2。原来的 count +=替换成了 count.getAndIncrement()
            参考代码：
                profit.jikeshijian.bingfabiancheng.AtomicLongTest
        优点：
            无锁方案相对互斥锁方案，最大的好处就是性能
                原因：
                    互斥锁方案：
                        1。为了保证互斥性，需要执行加锁、 解锁操作，而加锁、解锁操作本身就消耗性能
                        2。同时拿不到锁的线程还会进入阻塞状态，进而触 发线程切换，线程切换对性能的消耗也很大
                    无锁方案：
                        1。完全没有加锁、解锁的性能 消耗，同时还能保证互斥性
                        2。既解决了问题，又没有带来新的问题，可谓绝佳方案
                        问题：
                            如何做到的

    无锁方案的实现原理
        前提：
            其实原子类性能高的秘密很简单，硬件支持而已
        原因：
            CPU 为了解决并发问题，提供了 CAS 指令(CAS，全称是 Compare And Swap，即“比较并交换”)
                CAS 指令包含 3 个参数：
                    1。共享变量 的内存地址 A
                    2。用于比较的值 B
                    3。共享变量的新值 C
                原理：
                    1。并且只有当内存中地址 A 处的值等于 B 时，才能将内存中地址 A 处的值更新为新值 C
                    2。作为一条 CPU 指令，CAS 指令本身是能够保证原子性的。
                    参考代码：
                        profit.jikeshijian.bingfabiancheng.SimulatedCAS21
                    问题：
                        如何理解"只有当目前 count 的值和期望值 expect 相等时，才会将 count 更新为 newValue"
                        例子：
                            1。count += 1的一个核心问题是：基于内存中 count 的当前值 A 计算出来的 count+=1 为 A+1
                            2。在将 A+1 写入内存的时候，很可能此时内存中 count 已经被其他线程更新过了
                            3。只有当内存中 count 的值等于期望值 A 时，才能 将内存中 count 的值更新为计算结果 A+1，这不就是 CAS 的语义吗
    案例：
        CAS+自旋
            参考代码：
                profit.jikeshijian.bingfabiancheng.SimulatedCASTwo21
                注意：
                    1。上述参考代码中CAS 这种无锁方案，完全没有加锁、解锁操作
                    2。即便 两个线程完全同时执行 addOne() 方法，也不会有线程被阻塞
                优点：
                    相对于互斥锁方案来说，性 能好了很多。
                    问题
                        ABA问题
                            例子:
                                1。假设 count 原本是 A，线程 T1 在执行完代码1处之后，执行代码2处之前
                                2。有可能 count 被线程 T2 更新成 了 B，之后又被 T3 更新回了 A
                                3。这样线程 T1 虽然看到的一直是 A，但是其实已经被其他线程更 新过了，这就是 ABA 问题。
                            现象：
                                1。可能大多数情况下我们并不关心 ABA 问题
                                    例如：
                                        数值的原子递增
                                2。但也不能所有情况下都不关心
                                    例如：
                                        1。原子化的更新对象很可能就需要关心 ABA 问题
                                        2。因为两个 A 虽然相等，但是第二个 A 的属性可能已经发生变化了
                            方法：
                                在使用 CAS 方案的时候，一定要先 check 一下。
    看 Java 如何实现原子化的 count += 1
        参考源码：
            java.util.concurrent.atomic.AtomicLong
            概念：
                其中getAndAddLong() 方法的实现，基本上就是 CAS 使用的经典范例
                注意：
                    1。Java 提供的原子类里 面 CAS 一般被实现为 compareAndSet()
                    2。compareAndSet() 的语义和 CAS 指令的语义的差别仅 仅是返回值不同而已
                    3。compareAndSet() 里面如果更新成功，则会返回 true，否则返回 false
                        do {
                            // 获取当前值
                            oldV = xxxx;
                            // 根据当前值计算新值 newV = ...oldV...
                        }while(!compareAndSet(oldV,newV);
    原子类概览
        1。原子化的基本数据类型
            例子：
                1。AtomicBoolean
                2。AtomicInteger
                3。AtomicLong
            方法：
                getAndIncrement() // 原子化 i++
                getAndDecrement() // 原子化的 i--
                incrementAndGet() // 原子化的 ++i
                decrementAndGet() // 原子化的 --i
                // 当前值 +=delta，返回 += 前的值
                getAndAdd(delta)
                // 当前值 +=delta，返回 += 后的值
                addAndGet(delta)
                //CAS 操作，返回是否成功
                 compareAndSet(expect, update)
                 // 以下四个方法
                 // 新值可以通过传入 func 函数来计算
                 getAndUpdate(func)
                 updateAndGet(func)
                 getAndAccumulate(x,func)
                 accumulateAndGet(x,func)
        2。原子化的对象引用类型
            例子：
                1。AtomicReference：
                    方法：
                        提供的方法和原子化的基本数据类型差不多
                    缺点
                        ABA问题没有解决
                2。AtomicStampedReference：
                    优点：
                        可以解决 ABA 问题
                        原理：
                            实现的 CAS 方法就增加了版本号参数，
                3。AtomicMarkableReference：
                    优点：
                        可以解决 ABA 问题
                        原理：
                            将版本号简化成了一个 Boolean 值，
                    作用：
                        实现对象引用的原子化更新

            解决ABA思路：
                1。增加一个版本号维度就可以了
                2。每次执行CAS操作，附加更新一个版本号，只要保证版本号是递增的，
                3。那么即便 A 变成 B 之后再变回 A，版本号也不 会变回来(版本号递增的)。

        3。原子化数组
            例子：
                1。AtomicIntegerArray
                2。AtomicLongArray
                3。AtomicReferenceArray
                作用：
                    可以原子化地更新数组里面的每一个元素
                提供的方法和原子化的基本数据类型的区别：
                    每个方法多了一个数组的索引参数
        4。原子化对象属性更新器
            例子：
                1。AtomicIntegerArray
                2。AtomicLongArray
                3。AtomicReferenceArray
                作用：
                    可以原子化地更新对象的属性
                原理：
                    这三个方法都是利用反射机制实现的
                注意：
                    1。对象属性必须是 volatile 类型的，只有这样才能保证可见性
                    2。如果对象属性不是 volatile 类型的，newUpdater() 方法会抛出 IllegalArgumentException 这个运行时异常。

        5。原子化的累加器
            例子：
                1。DoubleAccumulator
                2。DoubleAdder
                3。LongAccumulator
                4。LongAdder
                作用：
                    用来执行累加操作
                优点：
                    相比原子化的基本数据类型，速度更快
                缺点：
                    但是不支持 compareAndSet() 方法
                场景：
                    如果你仅仅需要累加操作，使用原子化的累加器性能会更好。

22-Executor与线程池：如何创建正确的线程池？
    背景：
        线程是一个重量级的对象，应该避免频繁创建和销毁。
        原因：
            创建线程远不是创建一个对象那么简单。成本很高
                1。创建对象：仅仅是在JVM的堆里分配一块内存而已
                2。创建线程：需要调用操作系统内核的API，然后操作系统要为线程分配一系列的资源
        问题
            如何避免
        方案：
            线程池
                问题：
                    初次接触并发包里线程池相关的工具类时，多少会都有点蒙，不知道该从哪里入手
                    原因：
                        线程池和一般意义上的池化资源是不同的
                            一般意义上的池化资源：
                                1。当你需要资源的时候就调用acquire()方法来申请资源
                                2。用完之后就调用release()释放资源
                            线程池：
                                压根就没有申请线程和释放线程的方法
    线程池是一种生产者-消费者模式
        问题：
            为什么线程池没有采用一般意义上池化资源的设计方法呢？
        思考：
            假设我们获取到一个空闲线程T1，然后该如何使用T1呢？
                原因：
                    Thread对象的所有方法，都不存在类似execute(Runnable target)这样的公共方法。
                    过程：
                        1。通过调用T1的execute()方法，传入一个Runnable对象来执行具体业务逻辑
                        2。就像通过构造函数Thread(Runnable target)创建线程一样
                    参考代码：
                        profit.jikeshijian.bingfabiancheng.ThreadPoolOne22
                        结论：
                            线程池的设计，没有办法直接采用一般意义上池化资源的设计方法
        问题：
            线程池该如何设计呢？
            方案
                普遍采用的都是生产者-消费者模式
                    生产者：线程池的使用方
                    消费者：线程池本身
            参考代码
                profit.jikeshijian.bingfabiancheng.MyThreadPoolTwo22

    如何使用Java中的线程池
        背景：
            Java提供的线程池相关的工具类中，最核心的是ThreadPoolExecutor
        代码示例：
            ThreadPoolExecutor(
                int corePoolSize,
                int maximumPoolSize,
                long keepAliveTime,
                TimeUnit unit,
                BlockingQueue<Runnable> workQueue,
                ThreadFactory threadFactory,
                RejectedExecutionHandler handler) 
            理解思路：
                可以把线程池类比为一个项目组，而线程就是项目组的成员
                7个参数：
                    1。corePoolSize
                        概念：
                            表示线程池保有的最小线程数
                        案例：
                            有些项目很闲，但是也不能把人都撤了，至少要留corePoolSize个人坚守阵地。
                    2。maximumPoolSize
                        概念：
                            表示线程池创建的最大线程数
                        案例：
                            1。当项目很忙时，就需要加人，但是也不能无限制地加。最多就加到maximumPoolSize个人。
                            2。当项目闲下来时，就要撤人了，最多能撤到corePoolSize个人。
                    3。keepAliveTime & unit
                        概念：
                            用来定义这个“一段时间”的参数
                        问题：
                            上面提到项目根据忙闲来增减人员，那在编程世界里，如何定义忙和闲呢？
                            案例：
                            1。一个线程如果在一段时间内，都没有执行任务，说明很闲
                            2。如果一个线程空闲了keepAliveTime & unit(一段时间)这么久
                            3。而且线程池的线程数大于 corePoolSize 
                            4。那么这个空闲的线程就要被回收了
                    4。workQueue
                        概念：
                            工作队列
                        原理：
                            利用阻塞队列实现生产者-消费者模式
                    5。threadFactory
                        作用：
                            通过这个参数你可以自定义如何创建线程
                        例如：
                            可以给线程指定一个有意义的名字
                    6。handler
                        概念：
                            通过这个参数你可以自定义任务的拒绝策略
                        案例：
                            1。如果线程池中所有的线程都在忙碌，并且工作队列也满了（前提是工作队列是有界队列），
                            2。那么此时提交任务，线程池就会拒绝接收
                            3。至于拒绝的策略，你可以通过handler这个参数来指定
                            4。ThreadPoolExecutor已经提供了以下4种策略
                                1。CallerRunsPolicy：提交任务的线程自己去执行该任务。
                                2。AbortPolicy：默认的拒绝策略，会throws RejectedExecutionException。
                                3。DiscardPolicy：直接丢弃任务，没有任何异常抛出。
                                4。DiscardOldestPolicy：丢弃最老的任务，其实就是把最早进入工作队列的任务丢弃，然后把新任务加入到工作队列。
                        注意：
                            Java在1.6版本还增加了 allowCoreThreadTimeOut(boolean value) 方法
                            作用：
                                可以让所有线程都支持超时
                                案例：
                                    如果项目很闲，就会将项目组的成员都撤走

    使用线程池要注意些什么
        背景：
            Java并发包里提供了一个线程池的静态工厂类Executors，利用Executors你可以快速创建线程池
            原因：
                ThreadPoolExecutor的构造函数实在是有些复杂
            现象：
                大厂的编码规范中基本上都不建议使用Executors了
                最重要的原因：
                    Executors提供的很多方法默认使用的都是无界的LinkedBlockingQueue
                    缺点：
                        1。高负载情境下，无界队列很容易导致OOM
                        2。而OOM会导致所有请求都无法处理，这是致命问题
                    方法：
                        强烈建议使用有界队列
                            概念：
                                1。使用有界队列，当任务过多时，线程池会触发执行拒绝策略
                                2。线程池默认的拒绝策略会throw RejectedExecutionException(运行时异常)
                            注意：
                                1。默认拒绝策略要慎重使用
                                    原因：
                                        对于运行时异常编译器并不强制catch它，开发人员很容易忽略
                                2。如果线程池处理的任务非常重要
                                    方法：
                                        建议自定义自己的拒绝策略；
                                3。异常处理的问题
                                    案例：
                                        1。通过ThreadPoolExecutor对象的execute()方法提交任务时
                                        2。如果任务在执行的过程中出现运行时异常，会导致执行任务的线程终止
                                        3。不过，最致命的是任务虽然异常了，但是你却获取不到任何通知，这会让你误以为任务都执行得很正常
                                    方法：
                                        最稳妥和简单的方案还是捕获所有异常并按需处理
                                    代码示例
                                        try {
                                            //业务逻辑
                                        } catch (RuntimeException x) {
                                            //按需处理
                                        } catch (Throwable x) {
                                            //按需处理
                                        }

23-Future：如何用多线程实现最优的“烧水泡茶”程序？
    背景：
        ThreadPoolExecutor的 void execute(Runnable command) 方法，利用这个方法虽然可以提交任务
        问题：
            没有办法获取任务的执行结果（execute()方法没有返回值）
            思考：
                如何获取返回结果
    如何获取任务执行结果
        背景：
            1。Java通过ThreadPoolExecutor提供的3个submit()方法
            2。和1个FutureTask工具类来支持获得任务执行结果的需求
                3个submit()方法：
                    1.提交Runnable任务
                        方法：
                            Future<?> submit(Runnable task);
                        概念：
                            参数是一个Runnable接口，Runnable接口的run()方法是没有返回值的
                        作用：
                            返回的Future仅可以用来断言任务已经结束了，类似于Thread.join()。
                    2.提交Callable任务
                        方法：
                            <T> Future<T> submit(Callable<T> task);
                        概念：
                            1。这个方法的参数是一个Callable接口
                            2。它只有一个call()方法，并且这个方法是有返回值的
                        优点：
                            返回的Future对象可以通过调用其get()方法来获取任务的执行结果。
                    3.提交Runnable任务及结果引用
                        方法：
                            <T> Future<T> submit(Runnable task, T result);
                        思考：
                            假设这个方法返回的Future对象是f，f.get()的返回值就是传给submit()方法的参数result
                            问题：
                                这个方法该怎么用呢？
                            示例：
                                参考代码
                                    profit.jikeshijian.bingfabiancheng.ThreadPoolExecutorOne23
                                    优点：
                                        result相当于主线程和子线程之间的桥梁，通过它主子线程可以共享数据。

                    分析：
                        它们的返回值都是Future接口
                            Future接口五个方法：
                                概念：
                                    提交的任务不但能够获取任务执行结果，还可以取消任务
                                1。取消任务
                                    方法：
                                        boolean cancel(boolean mayInterruptIfRunning);
                                2。判断任务是否已取消
                                    方法：
                                        boolean isCancelled();

                                3。判断任务是否已结束
                                    方法：
                                        boolean isDone();
                                4。获得任务执行结果
                                    方法：
                                        get();
                                    注意：
                                        1。阻塞式的，如果被调用的时候，任务还没有执行完
                                        2。那么调用get()方法的线程会阻塞，直到任务执行完才会被唤醒。
                                5。获得任务执行结果，支持超时
                                    方法：
                                        get(long timeout, TimeUnit unit);
                                    注意：
                                        1。阻塞式的，如果被调用的时候，任务还没有执行完
                                        2。那么调用get()方法的线程会阻塞，直到任务执行完才会被唤醒。
                    FutureTask工具类
                        背景：
                            Future是一个接口，而FutureTask是一个实实在在的工具类
                        概念
                            这个工具类有两个构造函数，它们的参数和前面介绍的submit()方法类似
                        方法：
                            1。FutureTask(Callable<V> callable);
                            2。FutureTask(Runnable runnable, V result);
                        问题：
                            如何使用？
                                思路：
                                    1。FutureTask实现了Runnable和Future接口
                                    2。由于实现了Runnable接口，所以可以将FutureTask对象作为任务提交给ThreadPoolExecutor去执行，也可以直接被Thread执行
                                    3。又因为实现了Future接口，所以也能用来获得任务的执行结果
                                示例一：
                                    将FutureTask对象提交给ThreadPoolExecutor去执行。
                                        1。创建FutureTask
                                            FutureTask<Integer> futureTask= new FutureTask<>(()-> 1+2);
                                        2。创建线程池
                                            ExecutorService es =Executors.newCachedThreadPool();
                                        3。提交FutureTask
                                            es.submit(futureTask);
                                        4。获取计算结果
                                            Integer result = futureTask.get();
                                示例二：
                                    FutureTask对象直接被Thread执行
                                        1。创建FutureTask
                                            FutureTask<Integer> futureTask = new FutureTask<>(()-> 1+2);
                                        2。创建并启动线程
                                            Thread T1 = new Thread(futureTask);
                                            T1.start();
                                        3。获取计算结果
                                            Integer result = futureTask.get();
    实现最优的“烧水泡茶”程序
        背景：
            并发编程可以总结为三个核心问题：分工、同步和互斥
        流程：
            1。编写并发程序，首先要做的就是分工
                分工：
                    概念：
                        指的是如何高效地拆解任务并分配给线程
                案例：
                    对于烧水泡茶这个程序
                        一种最优的分工方案：
                            1。用两个线程T1和T2来完成烧水泡茶程序
                                1。T1负责洗水壶、烧开水、泡茶这三道工序
                                    注意：
                                        其中T1在执行泡茶这道工序时需要等待T2完成拿茶叶的工序
                                    问题：
                                        如何让T1处于等待中
                                    方法：
                                        1。Thread.join()、CountDownLatch，甚至阻塞队列都可以解决
                                        2。Future
                                            参考代码：
                                                profit.jikeshijian.bingfabiancheng.FutureTaskThreeBoilWaterThree23
                                2。T2负责洗茶壶、洗茶杯、拿茶叶三道工序

24-CompletableFuture：异步编程没那么难
    背景：
        1。用多线程优化性能，其实不过就是将串行操作变成并行操作
        2。在串行转换成并行的过程中，一定会涉及到异步化，
    问题一：
        如何将串行代码，转成异步的？
        示例代码：
            //以下两个方法都是耗时操作
                doBizA();
                doBizB();
        方法：
            创建两个线程就可以啦：
                new Thread(()->doBizA()).start();
                new Thread(()->doBizB()).start(); 
    异步化：
        概念：
            是并行方案得以实施的基础，更深入地讲其实就是：利用多线程优化性能这个核心方案得以实施的基础
        方案：
            Java在1.8版本提供了CompletableFuture来支持异步编程
    CompletableFuture的核心优势
        案例：
            用CompletableFuture重新实现前面曾提及的烧水泡茶程序
            过程：
                1。首先还是需要先完成分工方案
                2。下面的程序中，分了3个任务
                    任务1：负责洗水壶、烧开水
                    任务2：负责洗茶壶、洗茶杯和拿茶叶
                    任务3：负责泡茶
                3。任务3要等待任务1和任务2都完成后才能开始
            参考代码
                profit.jikeshijian.bingfabiancheng.CompletableFutureBoilWater24
                问题：
                    如何使用CompletableFuture

    创建CompletableFuture对象
        概念：
            1。默认情况下CompletableFuture会使用公共的ForkJoinPool线程池
            2。这个线程池默认创建的线程数是CPU的核数
                备注：
                    通过JVM option:-Djava.util.concurrent.ForkJoinPool.common.parallelism来设置ForkJoinPool线程池的线程数
        注意：
            如果所有CompletableFuture共享一个线程池；
            问题：
                会导致线程池中所有线程都阻塞在I/O操作上，从而造成线程饥饿
                原因：
                    一旦有任务执行一些很慢的I/O操作
                现象：
                    影响整个系统的性能
                方法：
                    根据不同的业务类型创建不同的线程池，以避免互相干扰。
    如何创建CompletableFuture对象
        四种方法
            //使用默认线程池
                1。static CompletableFuture<Void> runAsync(Runnable runnable)
                    特点：
                        Runnable 接口的run()方法没有返回值
                2。static <U> CompletableFuture<U>supplyAsync(Supplier<U> supplier)
                    特点：
                        Supplier接口的get()方法是有返回值的。
            //可以指定线程池
                3。static CompletableFuture<Void> runAsync(Runnable runnable, Executor executor)
                4。static <U> CompletableFuture<U> supplyAsync(Supplier<U> supplier, Executor executor)  
        过程：
            1。创建完CompletableFuture对象之后
            2。会自动地异步执行runnable.run()方法或者supplier.get()方法，对于一个异步操作
        关注两个问题"
            1。一个是异步操作什么时候结束
            2。一个是如何获取异步操作的执行结果
            方法：
                可以通过Future接口来解决
                原因：
                    CompletableFuture类实现了Future接口
    如何理解CompletionStage接口
        背景：
            1。CompletableFuture类还实现了CompletionStage接口
            2。在1.8版本里有40个方法
        概念：
            可以清晰地描述任务之间的这种时序关系
        思考：
            1。分工的角度类比一下工作流
            2。任务是有时序关系的，比如有串行关系、并行关系、汇聚关系等
            案例：烧水泡茶的例子
                1。串行关系：洗水壶和烧开水
                2。并行关系：洗水壶、烧开水和洗茶壶、洗茶杯这两组任务之间
                3。汇聚关系：而烧开水、拿茶叶和泡茶
                    例子：
                        f3 = f1.thenCombine(f2, ()->{}) 
                    AND聚合：
                        概念：
                            指的是所有依赖的任务（烧开水和拿茶叶）都完成后才开始执行当前任务（泡茶）
                        例子：
                            烧水泡茶程序中的汇聚关系
                    OR聚合关系
                        概念：
                            指的是依赖的任务只要有一个完成就可以执行当前任务。
                异常处理：
                    CompletionStage接口也可以方便地描述异常处理。
    CompletionStage接口如何描述串行关系、AND聚合关系、OR聚合关系以及异常处理。
        1。描述串行关系
            thenApply：
                代码示例：
                    CompletionStage<R> thenApply(fn);
                概念：
                    1。thenApply系列函数里参数fn的类型是接口Function<T, R>
                    2。thenApply系列方法返回的是CompletionStage<R>。
                        原因：
                            R apply(T t)：这个方法既能接收参数也支持返回值
            thenAccept：
                代码示例：
                    CompletionStage<Void> thenAccept(consumer);
                    概念：
                        1。thenAccept系列方法里参数consumer的类型是接口Consumer<T>
                        2。thenAccept系列方法返回的是CompletionStage<Void>
                        原因：
                            void accept(T t)：这个方法虽然支持参数，但却不支持回值
            thenRun：
                代码示例：
                    CompletionStage<Void> thenRun(action);
                    概念：
                        1。action的参数是Runnable
                        2。thenRun系列方法返回的也是CompletionStage<Void>。
                        原因：
                            action既不能接收参数也不支持返回值
            thenCompose：
                示例代码
                    CompletionStage<R> thenCompose(fn);
                概念：
                    新创建出一个子流程，最终结果和thenApply系列是相同的
            注意：
                Async代表的是异步执行fn、consumer或者action

            代码示例：
                CompletableFuture<String> f0 = 
                    CompletableFuture.supplyAsync(
                    () -> "Hello World")      //①
                    .thenApply(s -> s + " QQ")  //②
                    .thenApply(String::toUpperCase);//③

                System.out.println(f0.join());
                //输出结果
                HELLO WORLD QQ

        2。描述AND汇聚关系
            背景：
                CompletionStage接口里面描述AND汇聚关系
            概念：
                主要是thenCombine、thenAcceptBoth和runAfterBoth系列的接口
            区别：
                这些接口的区别也是源自fn、consumer、action这三个核心参数不同
            代码示例：
                CompletionStage<R> thenCombine(other, fn);
                CompletionStage<R> thenCombineAsync(other, fn);
                CompletionStage<Void> thenAcceptBoth(other, consumer);
                CompletionStage<Void> thenAcceptBothAsync(other, consumer);
                CompletionStage<Void> runAfterBoth(other, action);
                CompletionStage<Void> runAfterBothAsync(other, action);

        3。描述OR汇聚关系
            背景：
                CompletionStage接口里面描述OR汇聚关系
            概念：
                主要是applyToEither、acceptEither和runAfterEither系列的接口
            区别：
                这些接口的区别也是源自fn、consumer、action这三个核心参数不同
            示例代码：
                CompletionStage applyToEither(other, fn);
                CompletionStage applyToEitherAsync(other, fn);
                CompletionStage acceptEither(other, consumer);
                CompletionStage acceptEitherAsync(other, consumer);
                CompletionStage runAfterEither(other, action);
                CompletionStage runAfterEitherAsync(other, action);
            参考代码
                profit.jikeshijian.bingfabiancheng.CompletableFutureApplyToEitherTwo24
        4。异常处理
            概念：
                1。fn、consumer、action它们的核心方法都不允许抛出可检查异常
                2。但是却无法限制它们抛出运行时异常
            案例：
                执行 7/0 就会出现除零错误这个运行时异常
                背景：
                    非异步编程里面，我们可以使用try{}catch{}来捕获并处理异常
                问题：
                    那在异步编程里面，异常该如何处理呢？
            示例代码：
                CompletableFuture<Integer> f0 = CompletableFuture.
                    .supplyAsync(()->(7/0))
                    .thenApply(r->r*10);
                System.out.println(f0.join());
            CompletionStage解决的异常方案：
                1。CompletionStage exceptionally(fn)：
                    概念：
                        类似于try{}catch{}中的catch{}
                    优点：
                        但是由于支持链式编程方式，所以相对更简单
                2。CompletionStage<R> whenComplete(consumer);
                    概念：
                        1。类似于try{}finally{}中的finally{}
                        2。无论是否发生异常，都会执行whenComplete()中的回调函数consumer
                    缺点：
                        不支持返回结果
                3。CompletionStage<R> whenCompleteAsync(consumer);
                    概念：
                4。CompletionStage<R> handle(fn);
                    概念：
                        1。类似于try{}finally{}中的finally{}
                        2。无论是否发生异常都会执行whenComplete()中的handle()中的回调函数fn
                    优点：
                        支持返回结果
                5。CompletionStage<R> handleAsync(fn);
                示例代码：
                    CompletableFuture<Integer>
                        f0 = CompletableFuture
                        .supplyAsync(()->7/0))
                        .thenApply(r->r*10)
                        .exceptionally(e->0);
                    System.out.println(f0.join());

25-CompletionService：如何批量执行异步任务？
    案例：
        如何优化一个询价应用的核心代码？
    方法：
        采用“ThreadPoolExecutor+Future”的方案
    示例代码：
        参考：com/suixingpay/profit/jikeshijian/bingfabiancheng/ThreadPoolExecutorFutureOne25.java
    分析：
        如果获取电商S1报价的耗时很长，那么即便获取电商S2报价的耗时很短
        问题：
            也无法让保存S2报价的操作先执行
            原因：
                因为这个主线程都阻塞在了 f1.get() 操作上
                思考：
                    1。增加一个阻塞队列，获取到S1、S2、S3的报价都进入阻塞队列
                    2。然后在主线程中消费阻塞队列
                优点：
                    这样就能保证先获取到的报价先保存到数据库了
                演进一：
                    profit.jikeshijian.bingfabiancheng.ThreadPoolExecutorBlockingTwo25
                    现象：
                        不建议按演进一这样做：
                        原因：
                            Java SDK并发包里已经提供了设计精良的CompletionService。
                            优点：
                                解决先获取到的报价先保存到数据库的问题，而且还能让代码更简练。

    利用CompletionService实现询价系统
    CompletionService
        原理
            1。内部维护了一个阻塞队列
            2。把任务执行结果的Future对象加入到阻塞队列中
        问题：
            如何创建如何创建CompletionService呢？
        方法：
            CompletionService接口的实现类是ExecutorCompletionService，两个构造方法
            示例代码：
                1。ExecutorCompletionService(Executor executor)；
                2。ExecutorCompletionService(Executor executor, BlockingQueue<Future<V>> completionQueue)。
        案例一：
            如何利用CompletionService来实现高性能的询价系统
            演进二：
                参考代码
                    profit.jikeshijian.bingfabiancheng.ThreadPoolExecutorCompletionServiceThree25

        接口说明：
            5个方法：
                1。Future<V> submit(Callable<V> task);
                    概念：
                        方法参数是Callable<V> task
                2。Future<V> submit(Runnable task, V result);
                    概念：
                        方法有两个参数，分别是Runnable task和V result
                    类似于：
                        ThreadPoolExecutor的 <T> Future<T> submit(Runnable task, T result)
                3。Future<V> take() throws InterruptedException;
                    概念：
                        从阻塞队列中获取并移除一个元素
                    特点：
                        如果阻塞队列是空的，那么调用 take() 方法的线程会被阻塞
                4。Future<V> poll();
                    概念：
                        从阻塞队列中获取并移除一个元素
                    特点：
                        如果阻塞队列是空的，而 poll() 方法会返回 null 值
                5。Future<V> poll(long timeout, TimeUnit unit) throws InterruptedException;
                    概念：
                        支持以超时的方式获取并移除阻塞队列头部的一个元素
                    例子：
                        如果等待了 timeout unit时间，阻塞队列还是空的，那么该方法会返回 null 值。
        案例二：
            利用CompletionService实现Dubbo中的Forking Cluster
                背景：
                    Dubbo中有一种叫做Forking的集群模式
                        作用：
                            并行地调用多个查询服务，只要有一个成功返回结果，整个服务就可以返回了
                        例子：
                            1。提供一个地址转坐标的服务，为了保证该服务的高可用和性能
                            2。你可以并行地调用3个地图服务商的API
                            3。然后只要有1个正确返回了结果r，那么地址转坐标这个服务就可以直接返回r了
                            优点：
                                可以容忍2个地图服务商服务异常
                            缺点：
                                消耗的资源偏多
                            代码示例：
                                geocoder(addr) {
                                    //并行执行以下3个查询服务，
                                    r1=geocoderByS1(addr);
                                    r2=geocoderByS2(addr);
                                    r3=geocoderByS3(addr);
                                    //只要r1,r2,r3有一个返回
                                    //则返回
                                    return r1|r2|r3;
                                }
                利用CompletionService可以快速实现 Forking 这种集群模式:
                    代码参考：
                        profit.jikeshijian.bingfabiancheng.ThreadPoolExecutorCompletionServiceForkingFour25
26-ForkJoin：单机版的MapReduce
    背景：
         1。并发编程可以分为三个层面的问题：分别是分工、协作和互斥
            问题：
                1。当我们关注于任务的时候，会发现我们的视角已经从并发编程的细节中跳出来了
                2。应用的更多的是现实世界的思维模式，类比的往往是现实世界里的分工()
                    1。前面介绍了线程池、Future、CompletableFuture和CompletionService(都是讲的分工)
                    2。这些工具类都是在帮助我们站在任务的视角来解决并发问题
                    3。不是让我们纠缠在线程之间如何协作的细节上（比如线程之间如何实现等待、通知等）
                    例子：
                        1。并行：对于简单的并行任务，你可以通过“线程池+Future”的方案来解决
                        2。聚合：如果任务之间有聚合关系，无论是AND聚合还是OR聚合，都可以通过CompletableFuture来解决
                        3。批量：而批量的并行任务，则可以通过CompletionService来解决。
        2。并行、聚合、批量并行这三种任务模型，基本上能够覆盖日常工作中的并发场景了
        3。但还是不够全面，因为还有一种“分治”的任务模型没有覆盖到
            分治：
                概念：
                    1。把一个复杂的问题分解成多个相似的子问题
                    2。然后再把子问题分解成更小的子问题
                    3。直到子问题简单到可以直接求解
                现象：
                    对于问题的分治，实际上就是对于任务的分治。
                    原因：
                        解决每一个问题都对应着一个任务
                例子：
                    算法领域：分治算法(归并排序、快速排序都属于分治算法，二分法查找也是一种分治算法）
                    大数据领域:计算框架MapReduce背后的思想也是分治
                    Java并发包:提供了一种叫做Fork/Join的并行计算框架，就是用来支持分治这种任务模型的
    分治任务模型
        两个阶段：
            1。任务分解
                过程：
                    1。将任务迭代地分解为子任务
                    2。直至子任务可以直接计算出结果
            2。结果合并
                过程：
                    1。逐层合并子任务的执行结果
                    2。直至获得最终结果
        概念：
            在这个分治任务模型里，任务和分解后的子任务具有相似性
                哪方面相似：
                    体现在任务和子任务的算法是相同的
                不同的地方：
                    数据规模是不同的
                例子：
                    具备这种相似性的问题，往往采用递归算法

    Fork/Join的使用
        概念：
            1。Fork/Join是一个并行计算的框架，主要就是用来支持分治任务模型的
                1。Fork对应的是分治任务模型里的任务分解
                2。Join对应的是结果合并
            2。Fork/Join计算框架主要包含两部分
                1。一部分是分治任务的线程池ForkJoinPool
                2。另一部分是分治任务ForkJoinTask
                    类似：ThreadPoolExecutor和Runnable的关系，只不过分治任务有自己独特类型ForkJoinTask。
        ForkJoinTask：
            概念：
                是一个抽象类，它的方法有很多，最核心的是fork()方法和join()方法
                    fork()方法：会异步地执行一个子任务
                    Join()方法：会阻塞当前线程来等待子任务的执行结果
            两个子类：
                概念：
                    1。用递归的方式来处理分治任务的
                    2。两个子类都定义了抽象方法compute()
            具体子类
                1。RecursiveAction
                    特点：
                        定义的compute()没有返回值
                2。RecursiveTask
                    特点：
                        定义的compute()方法是有返回值
            案例一：
                如何用Fork/Join这个并行计算框架计算斐波那契数列
                    参考代码：
                        profit.jikeshijian.bingfabiancheng.ForkJoinFibonacciOne26

    ForkJoinPool工作原理
        背景：
            1。ThreadPoolExecutor本质上是一个生产者-消费者模式的实现，内部有一个任务队列
                任务队列作用：是生产者和消费者通信的媒介
            2。ThreadPoolExecutor可以有多个工作线程，但是这些工作线程都共享一个任务队列。
            3。ForkJoinPool本质上也是一个生产者-消费者的实现，但是更加智能
        概念：
            Fork/Join并行计算的核心组件是ForkJoinPool
        原理：
            1。ThreadPoolExecutor内部只有一个任务队列，而ForkJoinPool内部有多个任务队列
            2。当我们通过ForkJoinPool的invoke()或者submit()方法提交任务时
            3。ForkJoinPool根据一定的路由规则把任务提交到一个任务队列中
            4。如果任务在执行过程中会创建出子任务，那么子任务会提交到工作线程对应的任务队列中。
            问题：
                如果工作线程对应的任务队列空了，是不是就没活儿干了呢？
                答案：
                    不是的
                    原因：
                        ForkJoinPool支持一种叫做“任务窃取”的机制。
                        机制过程：
                            如果工作线程空闲了，那它可以“窃取”其他工作任务队列里的任务
                        例子：
                            线程T2对应的任务队列已经空了，它可以“窃取”线程T1对应的任务队列的任务
        注意：
            1。ForkJoinPool中的任务队列采用的是双端队列
            2。工作线程正常获取任务和“窃取任务”分别是从任务队列不同的端消费
                目的：
                    避免很多不必要的数据竞争。
            3。ForkJoinPool的实现远比我们这里介绍的复杂，如果你感兴趣，建议去看它的源码
        案例二
            模拟MapReduce统计单词数量
            参考代码
                profit.jikeshijian.bingfabiancheng.MapReduceForkJoinTwo26
        注意：
        1。默认情况下所有的并行流计算都共享一个ForkJoinPool，这个共享的ForkJoinPool默认的线程数是CPU的核数
        2。如果所有的并行流计算都是CPU密集型计算的话，完全没有问题
        3。如果存在I/O密集型的并行流计算，那么很可能会因为一个很慢的I/O计算而拖慢整个系统的性能
        4。建议用不同的ForkJoinPool执行不同类型的计算任务

27-并发工具类模块热点问题答疑
    1. while(true) 总不让人省心
        参考代码：
            profit.jikeshijian.bingfabiancheng.AccountWhile27
            profit.jikeshijian.bingfabiancheng.SafeWMWhile27
    2. signalAll() 总让人省心
        背景
            Dubbo最近已经把signal()改成signalAll()了
        原因：
            用signalAll()会更安全。
        示例代码
            // RPC结果返回时调用该方法   
            private void doReceived(Response res) {
                lock.lock();
                try {
                    response = res;
                    done.signalAll();
                } finally {
                    lock.unlock();
                }
            }
    3. Semaphore需要锁中锁
        Semaphore
            优点：
                允许多个线程访问一个临界区，
            缺点：
                当多个线程进入临界区时，如果需要访问共享变量就会存在并发问题，所以必须加锁
            问题：
                对象池的例子中Vector能否换成ArrayList？
                答案：
                    不可以
                原因：
                    1。Semaphore可以允许多个线程访问一个临界区
                    2。那就意味着可能存在多个线程同时访问ArrayList
                            ArrayList不是线程安全的
            参考代码：
                profit.jikeshijian.bingfabiancheng.ObjPoolSemaphoreN16

28-Immutability模式：如何利用不变性解决并发问题？
    背景：
        1。多个线程同时读写同一共享变量存在并发问题
        2。这里的必要条件之一是读写，如果只有读，而没有写，是没有并发问题的。
    不变性（Immutability）模式：
        概念：
            1。就是对象一旦被创建之后，状态就不再发生变化
            2。变量一旦被赋值，就不允许修改了（没有写操作）；没有修改操作，也就是保持了不变性。
    快速实现具备不可变性的类
    方法：
        1。将一个类所有的属性都设置成final的
        2。并且只允许存在只读方法，那么这个类基本上就具备不可变性了
        3。这个类本身也是final的，也就是不允许继承
            原因：
                子类可以覆盖父类的方法，有可能改变不可变性
        例子：
            String和Long、Integer、Double等基础类型的包装类都具备不可变性
        特点：
            不可变类的三点要求：类和属性都是final的，所有方法均是只读的。
        问题：
            Java的String方法也有类似字符替换操作，怎么能说所有方法都是只读的呢？
            答案：
                1。String这个类以及它的属性value[]都是final的
                2。而replace()方法的实现，就的确没有修改value[]，而是将替换后的字符串作为返回值返回了。
            参考代码：
                profit.jikeshijian.bingfabiancheng.StringImmutabilityOne28
                问题：
                    如果具备不可变性的类，需要提供类似修改的功能，具体该怎么操作呢？
                方法：
                    创建一个新的不可变对象，这是与可变对象的一个重要区别，可变对象往往是修改自己的属性。
                    思考：
                        所有的修改操作都创建一个新的不可变对象
                    问题：
                        是不是创建的对象太多了，有点太浪费内存呢？
                        答案：
                            是的
                            问题：
                                这样做的确有些浪费，那如何解决呢？
                            方法：
                                利用享元模式避免创建重复对象
    利用享元模式避免创建重复对象
        享元模式
            概念：
                享元模式本质上其实就是一个对象池，利用享元模式创建对象的逻辑也很简单
            作用：
                可以减少创建对象的数量，从而减少内存占用
            例子；
                Java语言里面Long、Integer、Short、Byte等这些基本数据类型的包装类都用到了享元模式。
            原理：
                1。创建之前，首先去对象池里看看是不是存在
                    1。如果已经存在，就利用对象池里的对象
                    2。如果不存在，就会新创建一个对象，并且把这个新创建出来的对象放进对象池里。
            案例：
                1。Long这个类并没有照搬享元模式，Long内部维护了一个静态的对象池，仅缓存了[-128,127]之间的数字，
                2。这个对象池在JVM启动的时候就创建好了，而且这个对象池一直都不会变化，也就是说它是静态的
                问题：
                    为什么这样设计？
                    原因：
                        Long这个对象的状态共有 2的64次方 种，实在太多，不宜全部缓存，而[-128,127]之间的数字利用率最高
                    注意：
                        基本上所有的基础类型的包装类都不适合做锁
                    原因：
                        它们内部用到了享元模式，这会导致看上去私有的锁，其实是共有的。
                    例子：
                        Integer 和 String 类型的对象不适合做锁
                    示例代码：
                        class A {
                            Long al=Long.valueOf(1);
                            public void setAX(){
                                synchronized (al) {
                                    //省略代码无数
                                }
                            }
                            }
                        class B {
                            Long bl=Long.valueOf(1);
                            public void setBY(){
                                synchronized (bl) {
                                    //省略代码无数
                                }
                            }
                        }
                        分析：
                            1。本意是A用锁al，B用锁bl，各自管理各自的，互不影响。
                            2。但实际上al和bl是一个对象，结果A和B共用的是一把锁
    使用Immutability模式的注意事项
        注意：
            1。对象的所有属性都是final的，并不能保证不可变性；
                概念：
                    1。在Java语言中，final修饰的属性一旦被赋值，就不可以再修改
                    2。但是如果属性的类型是普通对象，那么这个普通对象的属性是可以被修改的
                        案例一：

                            示例代码：
                                Bar的属性foo虽然是final的，依然可以通过setAge()方法来设置foo的属性age
                                class Foo{
                                    int age=0;
                                    int name="abc";
                                }
                                final class Bar {
                                    final Foo foo;
                                    void setAge(int a){
                                        foo.age=a;
                                    }
                                }

                            结论：
                                在使用Immutability模式的时候一定要确认保持不变性的边界在哪里，是否要求属性对象也具备不可变性。
            2。不可变对象也需要正确发布。
                概念：
                    不可变对象虽然是线程安全的，但是并不意味着引用这些不可变对象的对象就是线程安全的
                示例代码：
                    1。Foo具备不可变性，线程安全，但是类Bar并不是线程安全的
                    2。类Bar中持有对Foo的引用foo，对foo这个引用的修改在多线程中并不能保证可见性和原子性。
                    //Foo线程安全
                        final class Foo{
                            final int age=0;
                            final int name="abc";
                        }
                        //Bar线程不安全
                        class Bar {
                            Foo foo;
                            void setFoo(Foo f){
                            this.foo=f;
                            }
                        }
                    思考：
                        1。如果你的程序仅仅需要foo保持可见性，无需保证原子性，那么可以将foo声明为volatile变量(保证可见性)
                        2。如果你的程序需要保证原子性，那么可以通过原子类来实现
                    示例代码：
                        合理库存的原子化实现，你应该很熟悉了，其中就是用原子类解决了不可变对象引用的原子性问题。
                        public class SafeWM {
                            class WMRange{
                                final int upper;
                                final int lower;
                                WMRange(int upper,int lower){
                                    //省略构造函数实现
                                }
                            }
                            final AtomicReference<WMRange>
                                rf = new AtomicReference<>(new WMRange(0,0)
                                );
                            // 设置库存上限
                            void setUpper(int v){
                                while(true){
                                    WMRange or = rf.get();
                                    // 检查参数合法性
                                    if(v < or.lower){
                                        throw new IllegalArgumentException();
                                    }
                                    WMRange nr = new WMRange(v, or.lower);
                                    if(rf.compareAndSet(or, nr)){
                                        return;
                                    }
                                }
                            }
                        }
    总结：
        1。具备不变性的对象，只有一种状态，这个状态由对象内部所有的不变属性共同决定
        2。其实还有一种更简单的不变性对象，那就是无状态
            无状态对象：内部没有属性，只有方法
                优点：
                    在多线程领域，无状态对象没有线程安全问题，无需同步处理，自然性能很好
                例子：无状态的服务、无状态的协议
        无状态：
            优点：
                1。有很多好处，最核心的一点就是性能
                2。在分布式领域，无状态意味着可以无限地水平扩展，所以分布式领域里面性能的瓶颈一定不是出在无状态的服务节点上

29-Copy-on-Write模式：不是延时策略的COW
    背景：
        1。Java里String这个类在实现replace()方法的时候
        2。并没有更改原字符串里面value[]数组的内容，而是创建了一个新字符串
    引出：
        本质上是一种Copy-on-Write方法：写时复制
    Copy-on-Write模式的应用领域
        例子一：
            java领域
            CopyOnWriteArrayList和CopyOnWriteArraySet这两个Copy-on-Write容器
            概念：
                这两个容器实现的读操作是无锁的，由于无锁，所以将读操作的性能发挥到了极致。
            分析：
                1。Java提供的Copy-on-Write容器，由于在修改的同时会复制整个容器
                2。所以在提升读操作性能的同时，是以内存复制为代价的
            优点：
                如果是修改非常少、数组数量也不大，并且对读性能要求苛刻的场景，使用Copy-on-Write容器效果就非常好了
            缺点：
                在修改的时候会复制整个数组，所以如果容器经常被修改或者这个数组本身就非常大的时候，是不建议使用的
        例子二：
            操作系统领域
                Unix的操作系统中创建进程的API是fork()，传统的fork()函数会创建父进程的一个完整副本
                例子：
                    1。父进程的地址空间现在用到了1G的内存
                    2。那么fork()子进程的时候要复制父进程整个进程的地址空间（占有1G内存）给子进程，这个过程是很耗时的
                    3。而Linux中的fork()函数就聪明得多了，fork()子进程的时候，并不复制整个进程的地址空间，而是让父子进程共享同一个地址空间
                    4。只用在父进程或者子进程需要写入的时候才会复制地址空间，从而使父子进程拥有各自的地址空间。
                分析：
                    1。父子进程的地址空间以及数据都是要隔离的
                    2。使用Copy-on-Write更多地体现的是一种延时策略，只有在真正需要复制的时候才复制，而不是提前复制好
                    3。同时Copy-on-Write还支持按需复制，所以Copy-on-Write在操作系统领域是能够提升性能的。
        其他例子：
            1。Docker容器镜像的设计是Copy-on-Write
            2。分布式源码管理系统Git背后的设计思想都有Copy-on-Write……
        例子四：
            在函数式编程领域
                概念：
                    函数式编程的基础是不可变性（Immutability），所以函数式编程里面所有的修改操作都需要Copy-on-Write来解决
                优点：
                    Copy-on-Write也是可以按需复制的，
    案例：
        参考代码：
        profit.jikeshijian.bingfabiancheng.RouterImmutability29
    总结：
        1。Copy-on-Write才是最简单的并发解决方案
            例子：
                Java中的基本数据类型String、Integer、Long等都是基于Copy-on-Write方案实现的。
        缺点：
            消耗内存，每次修改都需要复制一个新的对象出来
        方法：
            随着自动垃圾回收（GC）算法的成熟以及硬件的发展，这种内存消耗已经渐渐可以接受
        场景：
            在实际工作中，如果写操作非常少，那你就可以尝试用一下Copy-on-Write，效果还是不错的。

30-线程本地存储模式：没有共享，就没有伤害
    背景：
        1。多个线程同时读写同一共享变量存在并发问题
        2。前面两篇文章我们突破的是写，没有写操作自然没有并发问题了
        3。其实还可以突破共享变量，没有共享变量也不会有并发问题，正所谓是没有共享，就没有伤害。
    问题一：
        如何避免共享呢
        思路：
            每个线程都拥有自己的变量，彼此之间不共享，也就没有并发问题了
            例子一：
                多个人争一个球总容易出矛盾，那就每个人发一个球。
            例子二：
                线程封闭，其本质上就是避免共享。你已经知道通过局部变量可以做到避免共享
    问题二：
        有没有其他方法可以做到呢？
        答案：
            Java语言提供的线程本地存储（ThreadLocal）就能够做到
    ThreadLocal的使用方法
        案例一：
            静态类ThreadId会为每个线程分配一个唯一的线程Id
            参考代码：
                profit.jikeshijian.bingfabiancheng.ThreadLocalThreadId30
        案例二：
            SimpleDateFormat不是线程安全的，那如果需要在并发场景下使用它，你该怎么办呢？
            参考代码：
                profit.jikeshijian.bingfabiancheng.ThreadLocalSafeDateFormat30
    ThreadLocal的工作原理
    问题一：
        如果让你来实现ThreadLocal的功能，你会怎么设计呢？
        分析：
            1。ThreadLocal的目标是让不同的线程有不同的变量V，那最直接的方法就是创建一个Map
            2。它的Key是线程，Value是每个线程拥有的变量V，ThreadLocal内部持有这样的一个Map就可以了
        代码示例：
            profit.jikeshijian.bingfabiancheng.MyThreadLocal30
        问题二：
            那Java的ThreadLocal是这么实现的吗？
            答案：
                我们的设计思路和Java的实现差异很大
            java实现原理：
                1。Java的实现里面也有一个Map，叫做ThreadLocalMap
                2。不过持有ThreadLocalMap的不是ThreadLocal，而是Thread
                3。Thread这个类内部有一个私有属性threadLocals，其类型就是ThreadLocalMap
                4。ThreadLocalMap的Key是ThreadLocal
                分析：
                    1。我们的设计里面Map属于ThreadLocal
                    2。而Java的实现里面ThreadLocalMap则是属于Thread
                问题：
                    这两种方式哪种更合理呢？
                    答案：
                        Java的实现更合理一些
                    原因：
                        1。在Java的实现方案里面，ThreadLocal仅仅是一个代理工具类
                        2。内部并不持有任何与线程相关的数据，所有和线程相关的数据都存储在Thread里面，这样的设计容易理解
                        3。而从数据的亲缘性上来讲，ThreadLocalMap属于Thread也更加合理。
                        更深层次的原因
                            不容易产生内存泄露。
                    区别：
                        我们的设计方案中：
                            ThreadLocal持有的Map会持有Thread对象的引用，ThreadLocal对象存在，那么Map中的Thread对象就永远不会被回收
                            缺点：
                                ThreadLocal的生命周期往往都比线程要长，所以这种设计方案很容易导致内存泄露
                        Java的实现中
                            1。Thread持有ThreadLocalMap，而且ThreadLocalMap里对ThreadLocal的引用还是弱引用（WeakReference）
                            2。所以只要Thread对象可以被回收，那么ThreadLocalMap就能被回收
                        优点：
                            Java的这种实现方案虽然看上去复杂一些，但是更加安全。
                        缺点：
                            线程池中使用ThreadLocal，如果不谨慎就可能导致内存泄露。
    ThreadLocal与内存泄露
        问题一：
            在线程池中使用ThreadLocal为什么可能导致内存泄露呢
            原因：
                1。在线程池中线程的存活时间太长，往往都是和程序同生共死的
                2。意味着Thread持有的ThreadLocalMap一直都不会被回收
                3。再加上ThreadLocalMap中的Entry对ThreadLocal是弱引用（WeakReference）
                4。即便Value的生命周期结束了，Value也是无法被回收的，从而导致内存泄露。
        问题二
            如何正确使用ThreadLocal呢？
            方法：
                既然JVM不能做到自动释放对Value的强引用，那我们手动释放就可以了
            问题三：
                如何能做到手动释放呢？
            方法：
                try{}finally{}方案了，手动释放资源的利器
                示例代码：
                    ExecutorService es;
                    ThreadLocal tl;
                    es.execute(()->{
                        //ThreadLocal增加变量
                        tl.set(obj);
                        try {
                            // 省略业务逻辑代码
                        }finally {
                            //手动清理ThreadLocal
                            tl.remove();
                        }
                    });
    InheritableThreadLocal与继承性
        背景：
            通过ThreadLocal创建的线程变量，其子线程是无法继承的。
            例子：
                1。在线程中通过ThreadLocal创建了线程变量V
                2。而后该线程创建了子线程，你在子线程中是无法通过ThreadLocal来访问父线程的线程变量V的。
                问题
                    需要子线程继承父线程的线程变量，那该怎么办呢？
                方法：
                    Java提供了InheritableThreadLocal来支持这种特性
                        InheritableThreadLocal是ThreadLocal子类，所以用法和ThreadLocal相同
                不建议：InheritableThreadLocal
                    原因：
                        1。具有ThreadLocal相同的缺点——可能导致内存泄露
                        2。线程池中线程的创建是动态的，很容易导致继承关系错乱
                        3。如果你的业务逻辑依赖InheritableThreadLocal，那么很可能导致业务逻辑计算错误，而这个错误往往比内存泄露更要命。


31-GuardedSuspension模式：等待唤醒机制的规范实现
    案例：
        1。Web版的文件浏览器，通过它用户可以在浏览器里查看服务器上的目录和文件
        2。这个项目依赖运维部门提供的文件浏览服务，而这个文件浏览服务只支持消息队列（MQ）方式接入
        消息队列：
            作用：
                在互联网大厂中用的非常多，主要用作流量削峰和系统解耦。
            特点：
                发送消息和消费结果这两个操作之间是异步的，
            参考代码：
                profit.jikeshijian.bingfabiancheng.Message30
    Guarded Suspension模式
        概念：
            保护性地暂停
        问题：
            如何模拟大堂经理进行保护性地暂停的
        思路:
            1。一个对象GuardedObject,前面提到的大堂经理，受保护对象就是餐厅里面的包间
                1.内部有一个成员变量——受保护的对象
                2.以及两个成员方法——get(Predicate<T> p)和onChanged(T obj)方法

            2。受保护对象的get()方法对应的是我们的就餐，就餐的前提条件是包间已经收拾好了，参数p就是用来描述这个前提条件的
            3。受保护对象的onChanged()方法对应的是服务员把包间收拾好了
            4。通过onChanged()方法可以fire一个事件，而这个事件往往能改变前提条件p的计算结果
            参考代码
                profit.jikeshijian.bingfabiancheng.GuardedObjectOne30
    扩展Guarded Suspension模式
        参考代码：
            profit.jikeshijian.bingfabiancheng.GuardedObjectTwo31

32-Balking模式:再谈线程安全的单例模式
    背景：
        1。“多线程版本的if”来理解Guarded Suspension模式
        2。不同于单线程中的 if，这个“多线程版本的if”是需要等待的，而且还很执着，必须要等到条件为真
    案例：
        各种编辑器提供的自动保存功能
        具体逻辑：
            1。自动保存功能的实现逻辑一般都是隔 一定时间自动执行存盘操作，存盘操作的前提是文件做过修改
            2。如果文件没有执行过修改操作，就需要快速 放弃存盘操作。
            参考代码：
                profit.jikeshijian.bingfabiancheng.AutoSaveEditorOne32
            优化之后：
                profit.jikeshijian.bingfabiancheng.AutoSaveEditorTwo32

    Balking模式的经典实现
        概念：
            Balking模式本质上是一种规范化地解决“多线程版本的if”的方案
        优点：
            将并发处理逻辑和业务逻辑分开。
        参考代码：
            profit.jikeshijian.bingfabiancheng.AutoSaveEditorThree32
    用volatile实现Balking模式
        背景：
            1。前面我们用synchronized实现了Balking模式，这种实现方式最为稳妥
            2。不过在某些特定场景下，也可以使用volatile来实现，但使用volatile的前提是对原子性没有要求。
        案例一：
            参考代码：
                自动保存路由表
                    profit.jikeshijian.bingfabiancheng.RouterTableFour32
        案例二：
            单次初始化
                profit.jikeshijian.bingfabiancheng.BalkingInitTest32
        案例三：
            线程安全的单例模式本质上其实也是单次初始化
                profit.jikeshijian.bingfabiancheng.BalkingSingleton32
        案例四：
            双重检查机制：
                profit.jikeshijian.bingfabiancheng.BalkingSingletonTwo32

33-Thread-Per-Message模式：最简单实用的分工方法
    背景：
        1。把并发编程领域的问题总结为三个核心问题：分工、同步和互斥
        2。其中，同步和互斥相关问题更多地源自微观，而分工问题则是源自宏观
        3。我们解决问题，往往都是从宏观入手，在编程领域，软件的设计过程也是先从概要设计开始，而后才进行详细设计
        4。解决并发编程问题，首要问题也是解决宏观的分工问题。
    分工问题也有一系列的设计模式：
        并发编程领域里，比较常用的主要有Thread-Per-Message模式、Worker Thread模式、生产者-消费者模式等等
    如何理解Thread-Per-Message模式
        概念：
            1。这种委托他人办理的方式，在并发编程领域被总结为一种设计模式
            2。简言之就是为每个任务分配一个独立的线程
            3。这是一种最简单的分工方法，实现起来也非常简单。
        现实世界：
            例子一：
                比如教育小朋友，搞不定怎么办呢？只能委托学校老师了；
            例子二：
                另一方面受限于我们的时间，比如忙着写Bug，哪有时间买别墅呢？只能委托房产中介了
            优点：
                委托他人代办有一个非常大的好处，那就是可以专心做自己的事了。
        编程领域：
            例子：
                1。比如写一个HTTP Server，很显然只能在主线程中接收请求
                2。而不能处理HTTP请求，因为如果在主线程中处理HTTP请求的话
                3。那同一时间只能处理一个请求，太慢了！怎么办呢？
                4。可以利用代办的思路，创建一个子线程，委托子线程去处理HTTP请求。

    用Thread实现Thread-Per-Message模式
        最经典的应用场景是网络编程里服务端的实现：
            流程：
                1。服务端为每个客户端请求创建一个独立的线程
                2。当线程处理完请求后，自动销毁
                3。这是一种最简单的并发处理网络请求的方法。
            案例：
                echo程序的服务端会原封不动地将客户端的请求发送回客户端。
            例子：
                客户端发送TCP请求"Hello World"，那么服务端也会返回"Hello World"。
            参考到代码：
                profit.jikeshijian.bingfabiancheng.ThreadPerMessage33
        轻量级线程：
            优点：
                1。创建的成本很低，基本上和创建一个普通对象的成本相似
                1。并且创建的速度和内存占用相比操作系统线程至少有一个数量级的提升
                2。轻量级线程实现Thread-Per-Message模式就完全没有问题了。
            案例：
                参考代码：
                    profit.jikeshijian.bingfabiancheng.ThreadPerMessageTwo33

34-WorkerThread模式：如何避免重复创建线程？
    背景：
        Thread-Per-Message模式，对应到现实世界，其实就是委托代办
        缺点：
            非常影响性能，同时无限制地创建线程还可能导致OOM，所以在Java领域使用场景就受限了
        原因：
            用Java Thread实现，频繁地创建、销毁线程
        思路：
            如何有效避免线程的频繁创建、销毁以及OOM问题
        方法：
            Worker Thread模式。
    Worker Thread模式及其实现
        概念：
            Worker Thread对应到现实世界里，其实指的就是车间里的工人。
            注意；
                车间里的工人数量往往是确定的
            例子：
                车间里的工人，有活儿了，大家一起干，没活儿了就聊聊天等着
        问题：
            那在编程领域该如何模拟车间的这种工作模式呢？或者说如何去实现Worker Thread模式呢？
            思考：
                1。用阻塞队列做任务池
                2。然后创建固定数量的线程消费阻塞队列中的任务
            方法：
                线程池
            优点：
                能够避免重复创建、销毁线程，同时能够限制创建线程的上限等等
                背景：
                    Java的Thread实现Thread-Per-Message模式难以应对高并发场景
                原因：
                    在于频繁创建、销毁Java线程的成本有点高，而且无限制地创建线程还可能导致应用OOM
                   方法：
                     线程池
    案例：
        以echo程序为例，看看如何用线程池来实现。
        示例一：
            profit.jikeshijian.bingfabiancheng.ThreadPerMessageOne33
        示例二：
            创建有界的队列来接收任务。在创建线程池时，清晰地指明拒绝策略。
            profit.jikeshijian.bingfabiancheng.ThreadPerMessageThreadPoolTwo34

    避免线程死锁
        概念：
            如果提交到相同线程池的任务不是相互独立的，而是有依赖关系的，那么就有可能导致线程死锁。
        现象：
            1。应用每运行一段时间偶尔就会处于无响应的状态
            2。监控数据看上去一切都正常，但是实际上已经不能正常工作了。
        案例：
            提交到相同线程池中的任务一定是相互独立的，否则就一定要慎重
        示例：
            profit.jikeshijian.bingfabiancheng.ThreadPoolDeadLockThree34

35-两阶段终止模式:如何优雅地终止线程?
    背景：
        前面两篇文章我们讲述的内容，从纯技术的角度看，都是启动多线程去执行一个异步任务
    问题：
        既启动，那又该如何终止呢?
    思路：
        优雅地终止线程
            优雅：指的是给T2一个机会料理后事，而不是被一剑封喉。
        1。不是自己终止自己，而是在一个线程T1中，终止线程T2

    如何理解两阶段终止模式
    概念：
        将终止过 程分成两个阶段
            第一阶段：主要是线程T1向线程T2发送终止指令
            第二阶段：是线程T2响应终止 指令。
            背景：
                Java线程进入终止状态的前提是线程进入RUNNABLE状态
                问题一：
                    线程也可能处在休眠状态
                    思路：
                        首先要把线程的状态从休眠状态转换到RUNNABLE状态
                    问题二：
                        如何做到呢?
                    方法：
                        靠Java Thread类提供的interrupt()方法，它可以将休眠状态的线程转换到RUNNABLE状态。
                      问题三：
                        线程转换到RUNNABLE状态之后，我们如何再将其终止呢?
                        方法：
                            1。设置一个标志位，然后线程会在合适的时机 检查这个标志位
                            2。如果发现符合终止条件，则自动退出run()方法
                            3。这个过程其实就是我们前面提到的第二 阶段:响应终止指令。
                终止指令：interrupt()方法和线程终止的标志位

    用两阶段终止模式终止监控操作
        案例：
            参考代码：
                profit.jikeshijian.bingfabiancheng.ProxyThreadStopOne35
                profit.jikeshijian.bingfabiancheng.ProxyThreadStopTwo35

    如何优雅地终止线程池
        背景：
            Java领域用的最多的还是线程池，而不是手动地创建线程
            问题一：
                那我们该如何优雅地终止线程池呢?
                方法：
                    线程池提供了两个方法:shutdown()和shutdownNow()
                    问题二：
                        这两个方法有什么区别呢?
                        背景：
                            线程池的原理：
                                1。是生产者-消费者模式的一种实现
                                2。提交给线程池的任务，
                                3。首先是进入一个阻塞队列中
                                4。之后线程池中的线程从阻塞队列中取出任务执行
                        答案：
                            shutdown()方法：
                                1。很保守的关闭线程池的方法
                                2。线程池执行shutdown()后，就会拒绝接收新的任务，
                                3。但是会等待线程池中正在执行的任务和已经进入阻塞队列的任务都执行完之后才最终关闭线程池
                            shutdownNow()方法：
                                1。相对就激进一些了
                                2。线程池执行shutdownNow()后，会拒绝接收新的任务，同时 还会中断线程池中正在执行的任务
                                3。已经进入阻塞队列的任务也被剥夺了执行的机会，不过这些被剥夺执行机会的任务会作为shutdownNow()方法的返回值返回
                                    原因：
                                        shutdownNow()方法会中断正在执行的线程， 所以提交到线程池的任务，如果需要优雅地结束，就需要正确地处理线程中断。

                        分析：
                            1。shutdown()和shutdownNow()方法你会发现，它们实质上使用的也是两阶段终止模式
                            2。只是终 止指令的范围不同而已，前者只影响阻塞队列接收任务，后者范围扩大到线程池中所有的任务。

36-生产者-消费者模式:用流水线思想提高效率
    背景：
        Worker Thread模式类比的是 工厂里车间工人的工作模式
    例子：
        1。Java线程池本质上就是用生产者-消 费者模式实现的
        2。Log4j2中异步Appender内部也用到了生产者-消费者模式

    生产者-消费者模式的优点
    概念：
        生产者-消费者模式的核心是一个任务队列
        过程：
            1。生产者线程生产任务，并将任务添加到任务队列中
            2。消费者线程从任务队列中获取任务并执行
    优点：
        1。解耦。
            概念：
                1。解耦对于大型系统的设计非常重要
                2。解耦的一个关键就是组件之间的依赖关系和通信方式必须受限
            例子：
                在生产者-消费者模式中，生产者和消费者没有任何依赖关系，他们彼此之间的通信只能通过任务队列
        2。支持异步：
            概念：
                1。在生产者-消费者模式中，生产者线程只需要将任务添加到任务队列而无需等待任务被消费者线程执行完
                2。与传统的方法之间调用的本质区别，传统的方法之间调用是同步的。
            问题：
                异步化处理最简单的方式就是创建一个新的线程去处理，那中间增加一个“任务队列”究竟有什么用呢?
            回答：
                平衡生产者和消费者的速度差异
                例子一：
                    1。假设生产者的速率很慢，而消费者的速率很高，比如是1:3,如果生产者有3个线程,采用创建新的线程的方式
                    2。那么会创建3个子线程，而采用生产者-消费者模式，消费线程只需要1个就可以了
                例子二：
                    1。Java语言里，Java线程和操作系统线程是一一对应的，线程创建得太多
                    2。会增加上下文切换的成本，所以Java线程不是越多越好，适量即可
                注意：
                    生产者-消费者模式恰好能支持你用适量的线程。
        3。能够平衡生产者和消费者的速度差异。

    支持批量执行以提升性能
        背景：
            如果使用轻量级线程，就没有必要平衡生产者和消费者的速度差异了
            原因：
                轻量级线程本身就是廉价的
            问题：
                生产者-消费者模式在性能优化方面就无用武之地了呢?
            答案：
                不是，有一类并发场景应用生产者-消费者模式就有奇效，那就是批量执行任务。
                例子
                    我们要在数据库里INSERT1000条数据，
                    方案一：
                        用1000个线程并发执行，每个线程INSERT一条数据;
                    方案二：
                        用1个线程，执行一个批量的SQL，一次性把1000条数据INSERT进去
                示例代码一：
                    profit.jikeshijian.bingfabiancheng.ProductConsumerOne36

    支持分阶段提交以提升性能
        参考代码
            profit.jikeshijian.bingfabiancheng.LoggerProductConsumerTwo36










































