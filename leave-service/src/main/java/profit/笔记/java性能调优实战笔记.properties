第00篇文章：怎样才能做好性能调优
    1. 扎实的计算机基础
    需要储备计算机组成原理、操作系统、网络协议以及数据库等基础知识。
    具体的性能问题往往还与传输、计算、存储数据等相关，那我们还需要储备数据结构、算法以及数学等基础知识。

    2. 习惯透过源码了解技术本质
    需要深入源码，通过分析来学习、总结一项技术的实现原理和优缺点，这样我们就能更客观地去学习一项技术，还能透过源码来学习牛人的思维方式，收获更好的编码实现方式。

    3. 善于追问和总结
    为什么这项技术可以提升系统性能？对比其他技术它好在哪儿？
    实现的原理又是什么呢？事实上，“知其然且知所以然”才是我们积累经验的关键。知道了一项技术背后的实现原理，我们才能在遇到性能问题时，做到触类旁通。

01篇如何制定性能调优标准？
   我们为什么要做性能调优？什么时候开始做？做性能调优是不是有标准可参考？
    为什么做性能调优：
        所有的系统在开发完之后，多多少少都会有性能问题，我们首先要做的就是想办法把问题暴露出来，
        例如进行压力测试、模拟可能的操作场景等等，再通过性能调优去解决这些问题。
        好的系统性能调优不仅仅可以提高系统的性能，还能为公司节省资源。
    什么时候开始做
        初期没必要介入：不仅不会给性能带来提升，还会影响开发进度
            只需要在代码层面保证，减少磁盘io，降低竞争锁的使用，以及使用高效的算法，遇到复杂的业务，我们可以充分
            利用设计模式来优化业务代码，例如，设计商品价格的时候，往往会有折扣活动，红包活动，可以用装饰器模式去设计

        编码完成后：就可以进行性能测试，产品经理一般会提供上线预期数据，进行压测，统计工具来统计各项性能指标，看是否在预期范围之内；
        上线之后：根据线上的实际情况，依照日志监控以及性能统计日志，来观测性能问题，就要对日志进行分析并修复问题；

     性能瓶颈
       CPU：代码递归导致的无限循环，正则表达式引起的回溯，JVM频繁的 FULL GC，以及多线程编程造成的大量上下文切换导致cpu繁忙
       内存：JVM对内存管理，堆内存存储java对象，不存在读写性能瓶颈，内存空间有限，占满，无法回收时，就会导致内存溢出，内存泄漏问题
       磁盘 I/O：存储空间大，SSD固态硬盘读写优化，但还是无法与内存的读写速度相提并论
       网络:网络带宽低，对于传输数据比较大，或者并发量比较大的系统，网路很容易成为性能瓶颈
       异常:抛出异常需要构建异常栈，对异常进行捕获和处理，高并发下，持续的进行异常处理，性能会明显下降
       数据库:数据库的操作往往涉及到磁盘I/O的读写，大量数据库读写操作，会导致I/O性能瓶颈
       锁竞争:多个线程共享一个资源，锁的使用会带来上下文的切换JVM 内部锁已经做了多次优化，例如，新增了偏向锁、自旋锁、轻量级锁、
         锁粗化、锁消除等。而如何合理地使用锁资源，优化锁资源，就需要你了解更多的操作系统知识、Java 多线程编程基础，积累项目经验

    指标：响应时间，吞吐量，计算机资源分配使用率，负载承受能力
        响应时间：
           数据库响应时间：数据库操作所消耗的时间，往往是整个请求链中最耗时的；
           服务端响应时间：服务端包括 Nginx 分发的请求所消耗的时间以及服务端程序执行所消耗的时间；
           网络响应时间：这是网络传输时，网络硬件需要对传输的请求进行解析等操作所消耗的时间；
           客户端响应时间：对于普通的 Web、App 客户端来说，消耗时间是可以忽略不计的，但如果你的客户端嵌入了大量的逻辑处理，消耗的时间就有可能变长，从而成为系统的瓶颈。
        吞吐量：磁盘吞吐量和网络吞吐量
                磁盘吞吐量：IOPS和数据吞吐量
        计算机资源分配使用率
           CPU 占用率、内存使用率、磁盘 I/O、网络 I/O 来表示资源使用率，任何一项分配不合理，对整个系统性能都是毁灭性的
        负载承受能力
            系统响应时间的上升曲线是否平缓。这项指标能直观地反馈给你，系统所能承受的负载压力极限，当你对系统进行压测时，系统的响应时间会随着系统并发数的增加而延长，直到系统无法处理这么多请求，抛出大量错误时，就到了极限

03。字符串性能优化不容小觑，百M内存轻松存储几十G数据
    String myStr = "aa" + "bb" + "cc" + "dd";在jdk1.8的时候会转化为
            String str=“abc” :，JVM 首先会检查该对象是否在字符串常量池中，如果在，就返回该对象引用，否则新的字符串将在常量池中被创建。这种方式可以减少同一个值的字符串对象的重复创建，节约内存
            String str = new String(“abc”) :首先在编译类文件时，"abc"常量字符串将会放入到常量结构中，在类加载时，“abc"将会在常量池中创建；其次，在调用 new 时，JVM 命令将会调用 String 的构造函数，同时引用常量池中的"abc” 字符串，在堆内存中创建一个 String 对象；最后，str 将引用 String 对象。
            String.intern：节省内存：如果常量池中有相同值，就会重复使用该对象，返回对象引用，这样一开始的对象就可以被回收掉。这种方式可以使重复性非常高的地址信息存储大小从 20G 降到几百兆
            String.intern参考下java核心技术，不推荐使用：一般使用Java 6这种历史版本，并不推荐大量使用intern，为什么呢?魔鬼存在于细节中，被缓存的字符串是存在所
            谓PermGen里的，也就是臭名昭著的“永久代”，这个空间是很有限的，也基本不会被FullGC之外的垃圾收集照顾到。所以，如果使用不当，OOM就会光顾。
            在后续版本中，这个缓存被放置在堆中，这样就极大避免了永久代占满的问题，甚至永久代在JDK 8中被MetaSpace(元数据区)替代了。而且，默认缓存大小也在不断地扩大中， \
              从最初的1009，到7u40以后被修改为60013。你可以使用下面的参数直接打印具体数字，可以拿自己的JDK立刻试验一下。
            String str ="hello"
            对象：指的是存储在内存中的，没有被改变
            对象引用：并不是对象本身。对象在内存中是一块内存地址，str是对象引用；
            对象的拼接：StringBuilder.append这样可以提高程序的效率。不安全
            StringBuffer：是线程安全的，涉及到锁竞争，所以从性能上来说，要比 StringBuilder 差一些。
    Immutable：https://www.jianshu.com/p/0ba3ea41af22
04。慎重使用正则表达式
        方法：排除法：
        流程：
            1。首先将方法里面的业务代码全部注释，留一个空方法在这里，再看性能如何。
            2。我快速定位到了是业务代码问题，就马上逐一查看代码查找原因。
            3。我将插入数据库操作代码加上之后，TPS 稍微下降了，但还是没有找到原因。
            4。最后，就只剩下 Split() 方法操作了，果然，我将 Split() 方法加入之后，TPS 明显下降了。
        优点：这种方式能够很好地区分是框架性能问题，还是业务代码性能问题。
        问题：正则表达式回溯
                方法：使用懒惰模式和独占模式。
        正则表达式组成部分：
            普通字符：
                概念：字母[a-zA-Z],数字[0-9],下划线[_],汉字，标点符号
                例子：regex=[a-z]匹配到a到z 26个字母的任意一个
            标准字符：
                概念：能够与"多种普通字符"匹配的简单表达式
                例子：\d,\w,\s,匹配数字0到9的任意数字可以用普通字符实现regex=[0-9],也可以用标准字符实现regex=\d
            限定字符（量词）：
                概念：用于表示匹配的字符数量
                例子：*，+，？，{n}等，匹配任意1到3位的数字可以用regex=\d{1,3}表示
            定位字符（边界字符）：
                概念："零宽"，标记匹配符合某种条件的位置
                例子：$,^,等字符，匹配以Hello开头的字符串可以使用regex=^Hello表示
        如何避免回溯问题？
            正则表达式的优化：
                1少用贪婪模式，多用独占模式
                2。减少分支选择
                3。减少捕获嵌套
            1。贪婪模式
                概念：在数量匹配中，如果单独使用 +、 ? 、* 或{min,max} 等量词，正则表达式会匹配尽可能多的内容
                例子：
                    text=“abbc”
                    regex=“ab{1,3}c”:NFA 自动机读取了最大的匹配范围，即匹配 3 个 b 字符
                现象：匹配发生了一次失败，就引起了一次回溯
                推论：如果匹配结果是“abbbc”，就会匹配成功。
            2。懒惰模式
                概念：正则表达式会尽可能少地重复匹配字符。如果匹配成功，它会继续匹配剩余的字符串。
                例子：在上面例子的字符后面加一个“？”，就可以开启懒惰模式。
                    text=“abc”
                    regex=“ab{1,3}?c”
                    结果：匹配结果是“abc”
                    过程：该模式下 NFA 自动机首先选择最小的匹配范围，即匹配 1 个 b 字符，因此就避免了回溯问题。
            3。独占模式
                概念：
                    1。同贪婪模式一样，独占模式一样会最大限度地匹配更多内容
                    2。不同的是，在独占模式下，匹配失败就会结束匹配，不会发生回溯问题。
                    例子：在字符后面加一个“+”，就可以开启独占模式。
                        text=“abbc”
                        regex=“ab{1,3}+bc”
                        结果：不匹配，结束匹配，不会发生回溯问题


05。ArrayList还是LinkedList？使用不当性能差千倍
    背景：
        1。ArrayList、Vector、LinkedList 集合类继承了 AbstractList 抽象类
        2。AbstractList 实现了 List 接口，同时也继承了 AbstractCollection 抽象类
        3。ArrayList、Vector、LinkedList 又根据自我定位，分别实现了各自的功能。
    扩展：
        1。ArrayList 和 Vector 使用了数组实现，这两者的实现原理差不多
        2。LinkedList 使用了双向链表实现
    ArrayList
        问题一：ArrayList为什么可以序列化
            现象：对象数组 elementData 使用了 transient 修饰
                transient：修饰属性，表示该属性不会被序列化
        问题二：ArrayList 在大量新增元素的场景下效率就一定会变慢呢？
        问题三： for 循环和迭代遍历选择哪种方式？原因？
        1。类介绍：
            1。实现了 List 接口，继承了 AbstractList 抽象类
                原理：底层是数组实现的，并且实现了自增扩容数组大小。
            2。实现了 Cloneable 接口和 Serializable 接口，
                作用：所以他可以实现克隆和序列化
            3。实现了 RandomAccess 接口
                作用：能实现快速随机访问
                原因： RandomAccess 接口是一个标志接口，他标志着“只要实现该接口的 List 类"
                     RandomAccess：是一个空接口，什么也没有实现
        2。属性介绍
            1。size：数组长度
            2。default_capacity：初始化容量，默认大小是10
            3。elementData：对象数组，被transient 关键字修饰
                背景：
                    ArrayList 的数组是基于动态扩增的，所以并不是所有被分配的内存空间都存储了数据。
                方案一：(不采用)
                    如果采用外部序列化法实现数组的序列化，会序列化整个数组
                    缺点：浪费时间和空间
                方案二：（采用）（回答了问题一）
                    内部提供了两个私有方法 writeObject 以及 readObject 来自我完成序列化与反序列化
                    优点：
                        序列化与反序列化数组时节省了空间和时间
                transient 修饰数组的作用：
                        防止对象数组被其他外部方法序列化
        3。构造函数
            1。传入一个初始化值
                背景：
                    1。当 ArrayList 新增元素时，如果所存储的元素已经超过其已有大小
                    2。它会计算元素大小后再进行动态扩容
                现象：
                    数组的扩容会导致整个数组进行一次内存复制
                问题：影响性能
                方法：合理指定数组初始大小，这样有助于减少数组的扩容次数
                优点：提高系统性能
            2。默认创建一个空数组对象
            3。传入一个集合类型进行初始化
        4。新增元素：
            1。直接将元素加到数组的末尾
                特点：没有发生扩容的前提下，是不会有元素复制排序过程的。
                优点：（回答了问题二）
                    1。ArrayList 在大量新增元素的场景下，性能并不会变差，反而比其他 List 集合的性能要好。
                        前提：初始化时就比较清楚存储数据的大小，就可以在 ArrayList 初始化时指定数组容量大小

            2。添加元素到任意位置
                    特点：导致在该位置后的所有元素都需要重新排列
                共同点：
                    1。先确认容量大小，如果容量够大，就不用进行扩容；
                    2。如果容量不够大，就会按照原来数组的 1.5 倍大小进行扩容，在扩容之后需要将数组复制到新分配的内存地址。
        5。删除元素
            原理：
                1。ArrayList 在每一次有效的删除元素操作之后，都要进行数组的重组
            缺点：
                并且删除的元素位置越靠前，数组重组的开销就越大。
        6。遍历元素
            背景：
                基于数组实现的，所以在获取元素的时候是非常快捷的。
        问题：下面哪一个删除正确
            1。迭代删除
            2。for(;;)删除：
            for(int i=0;i<list.size();i++)
            答案：第一个正确，第二个错误,第三个正确
                for(:)循环处理流程：
                    1。for(:)循环[这里指的不是for(;;)]是一个语法糖，这里会被解释为迭代器，在使用迭代器遍历时，
                    2。ArrayList内部创建了一个内部迭代器iterator，在使用next()方法来取下一个元素时
                    3。会使用ArrayList里保存的一个用来记录List修改次数的变量modCount
                    4。与iterator保存了一个expectedModCount来表示期望的修改次数进行比较，如果不相等则会抛出异常；
                    5。在foreach循环中调用list中的remove()方法，会走到fastRemove()方法，
                    6。该方法不是iterator中的方法，而是ArrayList中的方法，在该方法只做了modCount++，而没有同步到expectedModCount。
                    7。当再次遍历时，会先调用内部类iteator中的hasNext(),再调用next(),在调用next()方法时
                    8。会对modCount和expectedModCount进行比较，此时两者不一致，就抛出了ConcurrentModificationException异常。
                    关键：ArrayList的remove还是iterator中的remove。

    LinkedList
        结构：
            1。是基于双向链表数据结构实现的
            2。定义了一个 Node 结构，Node 结构中包含了 3 个部分
                元素内容 item、前指针 prev 以及后指针 next
        扩展：
            JDK1.7 之前
                1。只包含了一个 Entry 结构的 header 属性
                2。并在初始化的时候默认创建一个空的 Entry，用来做 header，前后指针指向自己，形成一个循环双向链表。
            JDK1.7 之后
                1。链表的 Entry 结构换成了 Node，内部组成基本没有改变
                2。但 LinkedList 里面的 header 属性去掉了，新增了一个 Node 结构的 first 属性和一个 Node 结构的 last 属性
                优点：
                    1。first/last 属性能更清晰地表达链表的链头和链尾概念；
                    2。first/last 方式可以在初始化 LinkedList 的时候节省 new 一个 Entry；
                    3。first/last 方式最重要的性能优化是链头和链尾的插入删除操作更加快捷了。
        1。类：
            1。实现了 List 接口、Deque 接口，同时继承了 AbstractSequentialList 抽象类
                扩展：实现了 List 类型又有 Queue 类型的特点
            2。实现了 Cloneable 和 Serializable 接口
                作用：实现克隆和序列化
            3。没有实现RandomAccess 接口
                原因：
                    LinkedList 存储数据的内存地址是不连续的，而是通过指针来定位不连续地址
                推论
                    LinkedList 不支持随机快速访问
        2。属性：
            1。first：transient修饰 不会序列化
            2。last：transient修饰 不会序列化
            3。size：transient修饰 不会序列化
            原因：
                自行实现 readObject 和 writeObject 进行序列化与反序列化。

        3。新增元素
            1。默认的 add (Ee) 方法
                概念：是将添加的元素加到队尾
                过程
                    1。首先是将 last 元素置换到临时变量中
                    2。生成一个新的 Node 节点对象
                    3。然后将 last 引用指向新节点对象
                    4。之前的 last 对象的前指针指向新节点对象
            2。add(int index, E element)
                概念：添加元素到任意位置的方法
                原理：
                    1。如果我们是将元素添加到任意两个元素的中间位置
                    2。添加元素操作只会改变前后元素的前后指针
                    3。指针将会指向添加的新元素
                优点：
                    相比 ArrayList 的添加操作来说，LinkedList 的性能优势明显
        4。删除元素
            过程
                1。首先要通过循环找到要删除的元素
                2。如果要删除的位置处于 List 的前半段，就从前往后找
                3。若其位置处于后半段，就从后往前找
            优点：
                无论要删除较为靠前或较为靠后的元素都是非常高效的
            缺点：
                如果 List 拥有大量元素，移除的元素又在 List 的中间段，那效率相对来说会很低。
        5。遍历元素
            方式一：for循环
                1。通过分前后半段来循环查找到对应的元素
                2。特别是在 for 循环遍历的情况下，每一次循环都会去遍历半个 List。
                缺点：
                    查询元素是非常低效的
            方式二： iterator 方式
                优点：直接拿到我们的元素，不需要通过循环查找 List。

    性能消耗对比（新增，删除，遍历）
        新增元素
            注意以下是时间对比，时间大的，性能反而低
            1。从集合头部位置新增元素：ArrayList>LinkedList
                背景：
                    1。 ArrayList 是数组实现的，而数组是一块连续的内存空间
                    2。LinkedList 是基于链表实现，在添加元素的时候，首先会通过循环查找到添加元素的位置
                原因：
                    1。ArrayList需要对头部以后的数据进行复制重排，所以效率很低
                    2。 LinkedList添加的位置处于 List 的前半段，就从前往后找，若其位置处于后半段，就从后往前找，添加元素到头部是非常高效的。

            2。从集合中间位置新增元素：ArrayList<LinkedList
                原因：
                    1。ArrayList 在添加元素到数组中间时，同样有部分数据需要复制重排，效率也不是很高
                    2。LinkedList 将元素添加到中间位置，是添加元素最低效率的，因为靠近中间位置，在添加元素之前的循环查找是遍历元素最多的操作。
            3。从集合尾部位置新增元素：ArrayList<LinkedList
                前提：
                    ArrayList在没有扩容的情况下
                原因：
                    1。ArrayList 在添加元素到尾部的时候，不需要复制重排数据，效率非常高
                    2。LinkedList 虽然也不用循环查找元素，但 LinkedList 中多了 new 对象以及变换指针指向对象的过程
        删除元素
            1。从集合头部位置删除元素：ArrayList>LinkedList
            2。从集合中间位置删除元素：ArrayList<LinkedList
            3。从集合尾部位置删除元素：ArrayList<LinkedList
            注意：删除元素操作测试的结果和添加元素操作测试的结果很接近
        遍历元素
            1。for(;;) 循环：ArrayList<LinkedList
                原因：
                    1。 LinkedList 基于链表实现的，在使用 for 循环的时候，每一次 for 循环都会去遍历半个 List，所以严重影响了遍历的效率
                    2。ArrayList 则是基于数组实现的，并且实现了 RandomAccess 接口标志，意味着 ArrayList 可以实现快速随机访问
            2。迭代器迭代循环：ArrayList～LinkedList
                性能相当
            建议：遍历linkedlist，切忌使用 for 循环遍历

06。Stream如何提高遍历集合效率？
        背景：
            在 Java8 中，Collection 新增了两个流方法，分别是 Stream() 和 parallelStream()。
        猜测：
            1。这两个方法肯定和 Stream 有关
            2。是不是和我们熟悉的 InputStream 和 OutputStream 也有关系呢？
            3。新增的两个方法有什么作用？
    什么是 Stream？
        背景：
            1。现在的电商业务，常常使用ID的hash值来实现分库分表
                优点（减少单个表的数据量，优化订单查询的速度）
            2。查询的时候，需要将各个数据源的数据在应用层进行合并操作；
        传统方法：
            例子：
                1。通过 for 循环或者 Iterator 迭代来重新排序合并数据
                2。通过重新定义 Collections.sorts 的 Comparator 方法来实现
            缺点：
                对于大数据量系统来说，效率并不是很理想。
        优化方法：引入Stream
        概念：
            1。与接触的字节流概念不太一样，Java8 集合中的 Stream 相当于高级版的 Iterator
            2。可以通过 Lambda 表达式对集合进行各种非常便利、高效的聚合操作（Aggregate Operation）
            3。或者大批量数据操作 (Bulk Data Operation)。
            4。Stream 的聚合操作与数据库 SQL 的聚合操作 sorted、filter、map 等
            5。数据操作方面，Stream 不仅可以通过串行的方式实现数据操作
            6。还可以通过并行的方式处理大批量数据，提高数据的处理效率。
        例子：
            1。串行实现
                Map<String, List<Student>> stuMap = stuList.stream().filter((Student s) -> s.getHeight() > 160) .collect(Collectors.groupingBy(Student ::getSex));
            2。并行实现
                Map<String, List<Student>> stuMap = stuList.parallelStream().filter((Student s) -> s.getHeight() > 160) .collect(Collectors.groupingBy(Student ::getSex)); 

    Stream 如何优化遍历？
        1。Stream 操作分类
            作用：是实现高效迭代大数据集合的重要原因之一
            分类：
                1。中间操作（懒操作）
                    概念：只对操作进行了记录，即只会返回一个流，不会进行计算操作
                    分类：
                        1。无状态（Stateless）操作：
                                概念：元素的处理不受之前元素的影响
                                例子：unordered(),filter,map,peek等
                        2。有状态（Stateful）操作：
                                概念：指该操作只有拿到所有元素之后才能继续下去。
                                例子：distinct，sorted，limit，skip
                2。终结操作
                    概念：实现了计算操作
                    分类：
                        1。短路（Short-circuiting）操作：
                                概念：遇到某些符合条件的元素就可以得到最终结果
                                例子：foreach，foreachOrdered,toArray,reduce,collect,max,min,count
                        2。非短路（Unshort-circuiting）操作：
                                概念：指必须处理完所有元素才能得到最终结果
                                例子：anyMatch,allMatch,findFirst,findAny,noneMatch
            原理：
                懒操作(中间操作)结合终结操作、数据源构成的处理管道（Pipeline）
        2。Stream 源码实现
            Stream：
                概念：定义了一些流的常用操作方法，例如，map、filter 等。
                主要结构类
                    1。BaseStream：
                        概念：
                            1。和 Stream 为最顶端的接口类
                            2。主要定义了流的基本接口方法
                                例如：spliterator、isParallel 等
                    2。ReferencePipeline：
                        概念：
                            1。是一个结构类
                            2。通过定义内部类组装了各种操作流
                                例如：
                                    1。Head：主要用来定义数据源操作
                                    2。StatelessOp：无状态中间操作 StatelessOp 对象
                                    3。StatefulOp：有状态操作 StatefulOp 对象
                            3。实现了 BaseStream 与 Stream 的接口方法
                            4。会将整个 Stream 流操作组装成一个调用链
                    3。Sink：
                        概念：
                            1。是一个接口，定义每个 Stream 操作之间关系的协议
                                包含了：begin()、end()、cancellationRequested()、accpt() 四个方法
                            2。调用链上的各个 Stream 操作的上下关系，通过Sink 接口协议来定义实现的。
                                调用链：ReferencePipeline 最终会将整个 Stream 流操作组装成一个调用链
        3。Stream 操作叠加（串行处理）
            背景：
                1。一个 Stream 的各个操作是由处理管道组装，并统一完成数据处理的
                2。在 JDK 中每次的中断操作会以使用阶段（Stage）命名。
            流程：
                1。初次调用 names.stream() 方法时，会初次加载 Head 对象，此时为加载数据源操作
                2。接着加载的是中间操作
                    1。无状态中间操作 StatelessOp 对象
                    2。和有状态操作 StatefulOp 对象
                3。此时的 Stage 并没有执行，而是通过 AbstractPipeline 生成了一个中间操作 Stage 链表；
                4。当我们调用终结操作时，会生成一个最终的 Stage
                5。通过这个 Stage 触发之前的中间操作，从最后一个 Stage 开始，递归产生一个 Sink 链。
            例子：
                需求：是查找出一个长度最长，并且以张为姓氏的名字
                代码实现
                    List<String> names = Arrays.asList(" 张三 ", " 李四 ", " 王老五 ", " 李三 ", " 刘老四 ", " 王小二 ", " 张四 ", " 张五六七 ");

                    String maxLenStartWithZ = names.stream()
                                                .filter(name -> name.startsWith(" 张 "))
                                                .mapToInt(String::length)
                                                .max()
                                                .toString();
                猜测运行流程：
                    1。首先遍历一次集合，得到以“张”开头的所有名字
                    2。然后遍历一次 filter 得到的集合，将名字转换成数字长度
                    3。最后再从长度集合中找到最长的那个名字并且返回。
                具体代码实现流程：
                    1。 names.stream() 方法将会调用集合类基础接口 Collection 的 Stream 方法
                        原因：names 是 ArrayList 集合
                    2。Stream 方法就会调用 StreamSupport 类的 Stream 方法
                         方法：初始化了一个 ReferencePipeline 的 Head 内部类对象
                    3。再调用 filter 和 map 方法，分别创建了一个 Stage 来标识用户的每一次操作
                        背景：
                            filter 和 map 方法是无状态的中间操作
                        推出
                            所以执行 filter 和 map 操作时，并没有进行任何的操作
                        一个完整的 Stage的组成部分：
                            1。数据来源
                            2。操作
                            3。回调函数
                    4。new StatelessOp 将会调用父类 AbstractPipeline 的构造函数
                        作用：将前后的 Stage 联系起来，生成一个 Stage 链表
                        注意：
                            1。在创建每一个 Stage 时，都会包含一个 opWrapSink() 方法
                            2。该方法会把一个操作的具体实现封装在 Sink 类中，Sink 采用（处理 -> 转发）的模式来叠加操作
                    5。当执行 max 方法时，会调用 ReferencePipeline 的 max 方法。创建一个 TerminalOp 操作
                    6。同时创建一个 ReducingSink，并且将操作封装在 Sink 类中。
                    7。调用 AbstractPipeline 的 wrapSink 方法，该方法调用 opWrapSink 生成一个 Sink 链表
                        Sink 链表：每一个 Sink 都封装了一个操作的具体实现。
                    8。当 Sink 链表生成完成后，Stream 开始执行，通过 spliterator 迭代集合，执行 Sink 链表中的具体操作
                简单流程
                    1。Java8 中的 Spliterator 的 forEachRemaining 会迭代集合，每迭代一次，都会执行一次 filter 操作
                    2。如果 filter 操作通过，就会触发 map 操作
                    3。然后将结果放入到临时数组 object 中，再进行下一次的迭代。
                    4。完成中间操作后，就会触发终结操作 max。
                以上这个例子，实串行模式
        Stream 并行处理
            背景：
                Stream 处理数据的方式有两种，串行处理和并行处理。要实现并行处理
            例子：
                上面的例子新增一个 Parallel() 方法
            过程：
                1。Stream 的并行处理在执行终结操作之前，跟串行处理的实现是一样的
                2。而在调用终结方法之后，实现的方式就有点不太一样，会调用 TerminalOp 的 evaluateParallel 方法进行并行处理。
                    并行处理：
                        1。Stream 结合了 ForkJoin 框架，对 Stream 处理进行了分片
                        2。Splititerator 中的 estimateSize 方法会估算出分片的数据量
                    原理：
                        1。通过预估的数据量获取最小处理单元的阀值，如果当前分片大小大于最小处理单元的阀值，就继续切分集合
                        2。每个分片将会生成一个 Sink 链表，当所有的分片操作完成后，ForkJoin 框架将会合并分片任何结果集。
        合理使用 Stream
            1。多核 CPU 服务器配置环境下，对比长度 100 的 int 数组的性能：
                结果：
                    常规的迭代 <Stream 并行迭代 <Stream 串行迭代
                推论：在循环迭代次数较少的情况下，常规的迭代方式性能反而更好；

            2。多核 CPU 服务器配置环境下，对比长度 1.00E+8 的 int 数组的性能：
                结果：
                    Stream 并行迭代 < 常规的迭代 <Stream 串行迭代
                推论：
                    在大数据循环迭代中，如果服务器是多核 CPU 的情况下，Stream 的并行迭代优势明显
            3。多核 CPU 服务器配置环境下，对比长度 1.00E+8 对象数组过滤分组的性能：
                结果：
                    Stream 并行迭代 < 常规的迭代 <Stream 串行迭代
                推论：
                    在大数据循环迭代中，如果服务器是多核 CPU 的情况下，Stream 的并行迭代优势明显
            4。单核 CPU 服务器配置环境下，对比长度 1.00E+8 对象数组过滤分组的性能：
                结果：
                    常规的迭代 <Stream 串行迭代 <Stream 并行迭代
                推论：
                    在单核 CPU 服务器配置环境中，也是常规迭代方式更有优势；
            建议：
                1。平时处理大数据的集合时，应该尽量考虑将应用部署在多核 CPU 环境下
                2。并且使用 Stream 的并行迭代方式进行处理。
            结论：
                其实使用 Stream 未必可以使系统性能更佳，结合应用场景进行选择，也就是合理地使用 Stream。

07 | 深入浅出HashMap的设计与优化
    背景：
        除了 Collection 接口，那么在 Java 容器类中，还定义了 Map 接口，主要用来存储键值对数据。
    常用的数据结构
        1。数组
            概念：采用一段连续的存储单元来存储数据
            操作：
                1。查找：指定下标的查找，时间复杂度为 O(1)
                2。插入：数组中间以及头部插入数据时，需要复制移动后面的元素。
        2。链表
            概念：
                1。一种在物理存储单元上非连续、非顺序的存储结构
                2。数据元素的逻辑顺序是通过链表中的指针链接次序实现的。
            组成：
                1。由一系列结点（链表中每一个元素）组成，结点可以在运行时动态生成
                    结点：“存储数据单元的数据域”和“存储下一个结点地址的指针域”
            操作：
                1。查找：查找一个结点或者访问特定编号的结点需要 O(n) 的时间。
                2。插入：插入的时候可以达到 O(1) 的复杂度

        3。哈希表
            概念：
                根据关键码值（Key value）直接进行访问的数据结构
            原理：
                将键的 Hash 值映射到内存地址，即根据键获取对应的值，并将其存储到内存地址
            操作：
                把关键码值映射到表中一个位置来访问记录
                映射函数叫做哈希函数
            优点：
                查找速度快
            定义：
                存放记录的数组
        4。树
            概念：
                由 n（n≥1）个有限结点组成的一个具有层次关系的集合，就像是一棵倒挂的树

    HashMap 的实现结构
        概念：
            1。作为最常用的 Map 类，它是基于哈希表实现的
            2。继承了 AbstractMap 并且实现了 Map 接口。
        原理
            根据键的 Hash 值来决定对应值的存储位置。
        优点：
            获取数据的速度会非常快
        问题：哈希冲突
            解决方法：
                1。开放定址法（不建议）
                    概念：
                        1。当发生哈希冲突时，如果哈希表未被装满
                        2。说明在哈希表中必然还有空位置，那么可以把 key 存放到冲突位置的空位置上去
                    缺点：
                        加大了查找、扩容的复杂度

                2。再哈希函数法：
                    概念：
                        1。在同义词产生地址冲突时再计算另一个哈希函数地址
                        2。直到冲突不再发生
                    缺点：
                        不易产生“聚集”，但却增加了计算时间
                    场景：
                        不考虑添加元素的时间成本，且对查询元素的要求极高
                3。链地址法
                    概念：存储相同 Hash 值的数据。
        hashmap对应的解决方法：
            方法：
                采用链地址法解决哈希冲突问题。
            实现：
                1。采用了数组（哈希表）+ 链表的数据结构
                2。当发生哈希冲突时，就用一个链表结构存储相同 Hash 值的数据。
        重要属性
            1。由一个 Node 数组构成；
                代码： transient Node<K,V>[] table;
                Node：
                    1。HashMap 中的一个内部类
                    2。含了一个 key-value 键值对
                    3。一个 next 指针
                        next 指针的作用：
                                1。当有哈希冲突时，HashMap 会用之前数组当中相同哈希值对应存储的 Node 对象
                                2。通过指针指向新增的相同哈希值的 Node 对象的引用
            2。加载因子（loadFactor）
                概念：
                    1。间接设置 Entry 数组（哈希表）的内存空间大小
                    2。在初始 HashMap 不设置参数的情况下，默认 LoadFactor 值为 0.75
                        设置0。75的原因：
                            1。加载因子越大
                                优点：对空间的利用就越充分。
                                缺点：链表的长度越长，查找效率也就越低
                                    查询复杂度：对于使用链表法的哈希表来说，查找一个元素的平均时间是 O(1+n)
                                            n：遍历链表的长度
                            2。加载因子太小
                                现象：那么哈希表的数据将过于稀疏
                                缺点：对空间造成严重浪费。
            3。边界值（threshold）
                概念：
                    1。通过初始容量和 LoadFactor 计算所得
                    2。初始 HashMap 不设置参数的情况下，默认边界值为 12
                问题：初始化容量设置比较小时，会有什么问题；
                过程：
                    1。HashMap 中 Node 的数量超过边界值
                    2。HashMap 就会调用 resize() 方法重新分配 table 数组
                    3。会导致 HashMap 的数组复制，迁移到另一块内存中去
                缺点：
                    影响 HashMap 的效率
        添加元素优化
            概念：
                初始化完成后，HashMap 就可以使用 put() 方法添加键值对了
            大致流程
                1。会根据该 key 的 hashCode() 返回值

                2。再通过 hash() 方法计算出 hash 值
                    原理：
                        1。将 hashCode 值右移 16 位（h >>> 16 代表无符号右移 16 位
                        2。使用位异或运算（如果两个数对应的位置相反，则结果为 1，反之为 0）
                    假设没有这个步骤：
                        例子：
                            1。添加两个对象 a 和 b，如果数组长度是 16，这时对象 a 和 b 通过公式 (n - 1) & hash 运算
                            2。也就是 (16-1)＆a.hashCode 和 (16-1)＆b.hashCode，
                            3。15 的二进制为 0000000000000000000000000001111
                            4。假设对象 A 的 hashCode 为 1000010001110001000001111000000
                            5。假设对象 B 的 hashCode 为 0111011100111000101000010100000
                            6。上述与运算结果都是 0
                            问题：hash冲突
                    作用：
                        1。尽量打乱 hashCode 真正参与运算的低 16 位。
                    优点：
                        降低hash冲突
                3。再通过 putVal 方法中的 (n - 1) & hash 决定该 Node 的存储位置
                    n：代表哈希表的长度
                        背景：哈希表习惯将长度设置为 2 的 n 次方
                        目的：恰好可以保证 (n - 1) & hash 的计算得到的索引值总是位于 table 数组的索引之内
                        例子：hash=15，n=16 时，结果为 15；hash=17，n=16 时，结果为 1。
                4。获得 Node 的存储位置后，如果判断 Node 不在哈希表中，就新增一个 Node，并添加到哈希表中，
                        注意：在 JDK1.8 中，HashMap 引入了红黑树数据结构来提升链表的查询效率。
                            背景：
                                当链表超过 8 时，HashMap 就会将链表转换为红黑树
                                    原因：
                                        因为链表的长度超过 8 后，红黑树的查询效率要比链表高，
                                    转换过程：存在左旋、右旋
                                    缺点：效率会降低
                                    解决：因链表过长而导致的查询时间复杂度高

            put 的实现源码:
                1。当table为nul或者tab的长度为0时，resize初始化
                2。(n - 1) & hash计算存储的下标
                3。判断这个下标是不是链表的第一个节点
                    如果是：
                        1。new 第一个 Node 节点，
                        2。返回新节点赋值给 tab[i]
                    如果不是：
                        存在四种情况
                            1。p 为链表节点：
                                    直接更新值
                            2。p 为红黑树节点：
                                1。肯定插入后仍然是红黑树节点
                                2。直接强制转型 p 后调用 TreeNode.putTreeVal 方法
                            3。p。插入后还是链表
                                背景：
                                    binCount计数器：计算当前链表的元素个数
                                    用这个来判断插入p结点之后是否超过指定的边界，从而判断是否要转换为红黑树

                            4。p插入后变成红黑树
                                插入后，调用 treeifyBin 方法，将该链表转换为红黑树
        获取元素优化
            概念：
                1。只存在数组，数组中没有 Node 链表
                    优点：查询数据性能最好的时候
                2。发生大量的哈希冲突，就会产生 Node 链表
                    缺点：每次查询元素都可能遍历 Node 链表，从而降低查询数据的性能
                       方法：
                            1。链表过长时，会转化为红黑树
                                优点：
                                    1。平均复杂度降低到了 O(log(n))
                                    2。链表越长，使用黑红树替换后的查询效率提升就越明显。
        扩容优化
            背景：
                HashMap 也是数组类型的数据结构，所以一样存在扩容的情况
            JDK1.7 扩容
                过程：
                    1。分别取出数组元素，一般该元素是最后一个放入链表中的元素
                    2。遍历以该元素为头的单向链表元素，依据每个被遍历元素的 hash 值
                    3。计算其在新数组中的下标，然后进行交换
                现象：
                    会将原来哈希冲突的单向链表尾部变成扩容后单向链表的头部。
            JDK 1.8 扩容
                1。判断原来的 hash 值和左移动的一位（newtable 的值）按位与操作是 0 或 1 就行
                    左移一位原因：
                        1。扩容数组的长度是 2 倍关系
                        例子：
                            假设初始 tableSize = 4 要扩容到 8 来说就是 0100 到 1000 的变化（左移一位就是 2 倍）
                    按与操作重新分配索引的原因
                        1。 hash 值本来就是随机的
                        2。而 hash 按位与上 newTable 得到的 0和 1就是随机的
                2。0 的话索引不变，1 的话索引变成原索引加上扩容前数组。
        建议：
            1。使用 HashMap 时，可以结合自己的场景来设置初始容量和加载因子两个参数。
            2。当查询操作较为频繁时，我们可以适当地减少加载因子
            3。如果对内存利用率要求比较高，我可以适当的增加加载因子。
            4。预知存储数据量的情况下，提前设置初始容量
                初始容量： = 预知数据量 / 加载因子
                了，优点：可以减少 resize() 操作，提高 HashMap 的效率

08 | 网络通信优化之I/O模型：如何解决高并发下I/O瓶颈？
    作用：
        1。I/O 操作读写文件
        2。Socket 的信息传输
    什么是 I/O
    概念：
        机器获取和交换信息的主要渠道，而流是完成 I/O 操作的主要方式。
        流：
            概念：
                1。一种信息的转换
                2。一种数据的载体，通过它可以实现数据交换和传输
            输入流：InputStream
                    概念：机器或者应用程序接收外界的信息
            输出流：OutputStream
                    概念：机器或者应用程序向外输出的信息称为输出流
            机器---应用程序：
                背景：
                    1。进行信息交换或者数据交换，总是先将对象或数据转换为某种形式的流
                    2。再通过流的传输，到达指定机器或程序后，再将流转换为对象数据

    Java 的 I/O 操作类
        4个基本类
            问题一：不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，为什么 I/O 流操作要分为字节流操作和字符流操作呢？
            回答：字符到字节必须经过转码，这个过程非常耗时，如果我们不知道编码类型就很容易出现乱码问题
            1。字节流
                概念：
                    1。InputStream/OutputStream 是字节流的抽象类，这两个抽象类又派生出了若干子类
                    2。子类分别处理不同的操作类型
                1。InputStream
                    FileInputStream：文件的读写操作
                    ByteArrayInputStream：数组的读写操作
                    BufferedInputStream：普通字符串的读写操作
                2。OutputStream
                    FileOutputStream：文件的读写操作
                    ByteArrayOutputStream：数组的读写操作
                    BufferedOutputStream：普通字符串的读写操作

            2。字符流
                概念：
                    1。Reader/Writer 是字符流的抽象类，这两个抽象类也派生出了若干子类
                    2。子类分别处理不同的操作类型
                1。Reader
                    FileReader：文件的读写操作
                    CharArrayReader：数组的读写操作
                    StringReader：普通字符串的读写操作
                2。Writer
                    FileWriter：文件的读写操作
                    CharArrayWriter：数组的读写操作
                    StringWriter：普通字符串的读写操作

    传统 I/O 的性能问题
        背景：
            I/O 操作的分类
                1。磁盘 I/O 操作
                    概念：
                        1。从磁盘中读取数据源输入到内存中
                        2。之后将读取的信息持久化输出在物理磁盘上
                2。网络 I/O 操作
                    概念：
                        1。从网络中读取信息输入到内存
                        2。最终将信息输出到网络中
        1。多次内存复制
            例子：
                1。通过 InputStream 从源数据中读取数据流输入到缓冲区里
                2。通过 OutputStream 将数据输出到外部设备（包括磁盘、网络）
            输入操作在操作系统中的具体流程：
                1。JVM 会发出 read() 系统调用，并通过 read 系统调用向内核发起读请求
                2。内核向硬件发送读指令，并等待读就绪
                3。内核把将要读取的数据复制到指向的内核缓存中；
                4。操作系统内核将数据复制到用户空间缓冲区，然后 read 系统调用返回
            分析：
                1。数据先从外部设备复制到内核空间
                2。再从内核空间复制到用户空间
            结论：
                发生了两次内存复制操作
            现象：
                导致不必要的数据拷贝和上下文切换
            缺点：
                降低 I/O 的性能。

        2. 阻塞
            背景：
            1。在传统 I/O 中，InputStream 的 read() 是一个 while 循环操作，它会一直等待数据读取，直到数据就绪才会返回
                说明：如果没有数据就绪，这个读取操作将会一直被挂起，用户线程将会处于阻塞状态。
                情况一：在少量连接请求的情况下，使用这种方式没有问题，响应速度也很高
                情况二：
                    1。但在发生大量连接请求时，就需要创建大量监听线程
                    2。这时如果线程没有数据就绪就会被挂起，然后进入阻塞状态。
                    3。一旦发生线程阻塞，这些线程将会不断地抢夺 CPU 资源，从而导致大量的 CPU 上下文切换，增加系统的性能开销
    如何优化 I/O 操作
        背景：
            1。编程语言对上面两个问题做出了性能优化
            2。操作系统也进一步优化了 I/O
        例子：
            1。JDK1.4 发布了 java.nio 包（new I/O 的缩写）
                作用：NIO 的发布优化了内存复制以及阻塞导致的严重性能问题
            2。JDK1.7 又发布了 NIO2，
                作用：从操作系统层面实现的异步 I/O
        方案一：
            概念：使用缓冲区优化读写流操作
            NIO与传统I/O的区别
                1。基于块（Block）的，它以块为基本单位处理数据
                    NIO中的组件：
                        1。缓冲区（Buffer）
                            概念：是一块连续的内存块
                            作用：NIO 读写数据的中转地
                            例子：可以将文件一次性读入内存再做后续处理，而传统的方式是边读文件边处理数据
                        2。通道（Channel）
                            概念：缓冲数据的源头或者目的地
                            作用：读取缓冲或者写入数据，是访问缓冲的接口
                2。传统 I/O 是面向流，NIO 是面向 Buffer
        方案二：
            概念：使用 DirectBuffer 减少内存复制
            背景：除了做了缓冲块优化之外，还提供了一个可以直接访问物理内存的类 DirectBuffer
                区别：
                    1。普通的 Buffer 分配的是 JVM 堆内存
                    2。而 DirectBuffer 是直接分配物理内存。
                以前的方式：
                    数据要输出到外部设备，必须先从用户空间复制到内核空间，再复制到输出设备，
                DirectBuffer方式：
                    直接将步骤简化为从内核空间复制到外部设备
                优点：减少了数据拷贝
                扩展：
                    1。申请的是非 JVM 的物理内存，所以创建和销毁的代价很高
                    2。申请的内存并不是直接由 JVM 负责垃圾回收
                    3。 DirectBuffer 包装类被回收时，会通过 Java Reference 机制来释放该内存块。
        方案三：
            概念：避免阻塞，优化 I/O 操作
            背景：
                传统的 I/O 
                    1。即使使用了缓冲块，依然存在阻塞问题
                        原因：
                            1。线程池线程数量有限，一旦发生大量并发请求
                            2。超过最大数量的线程就只能等待，直到线程池中有空闲的线程可以被复用
                            3。而对 Socket 的输入流进行读取时，读取流会一直阻塞
                                解除阻塞的三种情况：
                                    1。有数据可读；
                                    2。连接释放
                                    3。空指针或 I/O 异常。
                    2。阻塞问题，就是传统 I/O 最大的弊端。
        NIO：
            概念：非阻塞I/0的两个基本组件
            1。通道（Channel）
                原始方式：
                    应用程序调用操作系统 I/O 接口时，是由 CPU 完成分配
                    问题：“发生大量 I/O 请求时，非常消耗 CPU“
                    方法：
                        操作系统引入了 DMA（直接存储器存储），内核空间与磁盘之间的存取完全由 DMA 负责，
                        问题：依然需要向 CPU 申请权限，且需要借助 DMA 总线来完成数据的复制操作，如果 DMA 总线过多，就会造成总线冲突。
                            方法：通道解决的
                                1。Channel 有自己的处理器，可以完成内核空间和磁盘之间的 I/O 操作
                                2。读写数据都要通过Channel，双向的，所以读、写可以同时进行。
            2。多路复用器（Selector）
                概念：
                    1.Selector 是 Java NIO 编程的基础
                    2.基于事件驱动实现的
                    流程：
                        1。在 Selector 中注册 accpet、read 监听事件，
                        2。Selector 会不断轮询注册在其上的 Channel
                            细节：
                                1。一个线程使用一个 Selector，通过轮询的方式，可以监听多个 Channel 上的事件
                                2。我们可以在注册 Channel 时设置该通道为非阻塞
                                3。当 Channel 上没有 I/O 操作时，该线程就不会一直等待了
                                4。而是会不断轮询所有 Channel，从而避免发生阻塞。
                        3。如果某个 Channel 上面发生监听事件这个 Channel 就处于就绪状态
                        4。然后进行 I/O 操作。
                    作用：用于检查一个或多个 NIO Channel 的状态是否处于可读、可写。
                    注意：
                        目前操作系统的 I/O 多路复用机制都使用了 epoll
                        优点：
                            1。相比传统的 select 机制，epoll 没有最大连接句柄 1024 的限制
                            2。所以 Selector 在理论上可以轮询成千上万的客户端
                例子：
                    情况一：
                        概念：最早没有实现线程池的 I/O 操作
                        流程：
                            1。可以把监听多个 I/O 连接请求比作一个火车站的进站口
                            2。以前检票只能让搭乘就近一趟发车的旅客提前进站，而且只有一个检票员
                            3。这时如果有其他车次的旅客要进站，就只能在站口排队
                    情况二：
                        概念：
                            多线程创建了多个监听线程，同时监听各个客户端的 I/O 请求。
                        流程：
                            1。火车站升级了，多了几个检票入口，允许不同车次的旅客从各自对应的检票入口进站
                    情况三
                        例子：
                            流程：
                                1。火车站进行了升级改造，可以容纳更多旅客了，每个车次载客更多了
                                2。而且车次也安排合理，乘客不再扎堆排队，可以从一个大的统一的检票口进站了
                                3。这一个检票口可以同时检票多个车次
                            分析：
                                1。大的检票口就相当于 Selector
                                2。车次就相当于 Channel
                                3。旅客就相当于 I/O 流。

09 | 网络通信优化之序列化：避免使用Java序列化
    背景：
        1。大部分后端服务都是基于微服务架构实现
        2。服务按照业务划分被拆分，实现了服务的解偶
        问题：
            不同业务之间通信需要通过接口实现调用
                过程：
                    1。两个服务之间要共享一个数据对象，就需要从对象转换成二进制流
                    2。通过网络传输，传送到对方服务，再转换回对象，供服务方法调用
                汇总：编码和解码过程我们称之为序列化与反序列化。
                    问题：
                        1。大量并发请求的情况下，如果序列化的速度慢，会导致请求响应时间增加；
                        2。序列化后的传输数据体积大，会导致网络吞吐量下降
                方案：
                    1。java序列化：Java 提供了 RMI 框架可以实现服务与服务之间的接口暴露和调用（主流框架没有用）
                            RMI 中对数据对象的序列化采用的是 Java 序列化。
                    2。json序列化：SpringCloud 用的是 Json 序列化
                    3。Hessian序列化：Dubbo 虽然兼容了 Java 序列化，但默认使用的是 Hessian 序列化

    为什么没有使用java序列化：
        背景：Java 提供了一种序列化机制
        序列化机制：
            1。将一个对象序列化为二进制形式（字节数组）
                作用：用于写入磁盘或输出到网络
            2。也能从网络或磁盘中读取字节数组，反序列化成对象
                作用：在程序中使用。
        实现：
        jdk提供了两个对象：
            ObjectInputStream
                概念：
                    1。实现了 Serializable 接口的类的对象进行序列化
                         对象：实现了 Serializable 
                            概念：会生成一个 serialVersionUID 的版本号
                                serialVersionUID作用：
                                    1。会在反序列化过程中来验证序列化对象是否加载了反序列化的类
                                    2。如果是具有相同类名的不同版本号的类，在反序列化中是无法获取对象的
                    2。默认序列化方式，仅对对象的非 transient 的实例变量进行序列化
                    3。不会序列化对象的 transient 的实例变量，也不会序列化静态变量。
            ObjectOutputStream
                概念：
                    实现了 Serializable 接口的类的对象进行序列化
        具体实现方法：
            writeObject：默认
            readObject：默认
            扩展：
                在实现 Serializable 接口的类中对其进行重写，定制一套属于自己的序列化与反序列化机制。
            还定义了两个重写方法
                writeReplace：序列化之前替换序列化对象
                readResolve：反序列化之后对返回对象
        缺点：
            1。无法跨语言
                背景：
                    现在的系统设计越来越多元化，很多系统都使用了多种语言来编写应用程序
                例子：
                    C++ 写游戏服务，Java/Go 写周边服务，Python 写一些监控应用。
                副作用
                    两个基于不同语言编写的应用程序相互通信，则无法实现两个应用服务之间传输对象的序列化与反序列化。
            2。易被攻击
                背景：
                    对不信任数据的反序列化，从本质上来说是危险的，应该予以避免
                实现：
                    ObjectInputStream 上调用 readObject() 方法进行反序列化的
                readObject()的特点：
                    1。是一个神奇的构造器
                    2。可以将类路径上几乎所有实现了 Serializable 接口的对象都实例化。
                        说明了：
                            反序列化字节流的过程中，该方法可以执行任意类型的代码，这是非常危险的。
                        流程：
                            1。需要长时间进行反序列化的对象，不需要执行任何代码，也可以发起一次攻击
                            2。攻击者可以创建循环对象链，然后将序列化后的对象传输到程序中反序列化
                            3。这种情况会导致 hashCode 方法被调用次数呈次方爆发式增长, 从而引发栈溢出异常
                        例子：
                            1。通过 Apache Commons Collections，Java 反序列化漏洞可以实现攻击
                                Apache Commons Collections：
                                    1。第三方基础库
                                    2。扩展了 Java 标准库里的 Collection 结构，提供了很多强有力的数据结构类型
                                    3。实现了各种集合工具类。
                            2。一度横扫了 WebLogic、WebSphere、JBoss、Jenkins、OpenNMS 的最新版
                            攻击的原理：
                                1。Apache Commons Collections 允许链式的任意的类函数反射调用
                                2。攻击者通过“实现了 Java 序列化协议”的端口，把攻击代码上传到服务器上
                                3。再由 Apache Commons Collections 里的 TransformedMap 来执行。
                        解决方法：
                            1。制定了一套数据结构来保存和获取对象
                            例子：JSON 序列化、ProtocolBuf
                                原因：只支持一些基本类型和数组数据类型
                                目的：避免反序列化创建一些不确定的实例
                                特点：它们的设计简单，但足以满足当前大部分系统的数据传输需求
                            2。重写 resolveClass 方法，并在该方法中校验对象名字。
                                目的：通过反序列化对象白名单来控制反序列化对象

            3。序列化后的流太大
                序列化的性能：
                    指标一：序列化后的二进制流大小
                    问题一：
                        1。存储硬件的成本就越高
                            原因：序列化后的二进制数组越大
                        2。影响到系统的吞吐量
                            原因：进行网络传输，则占用的带宽就更多
                例子：
                    1。Java 序列化实现的二进制编码完成的二进制数组大小  
                    2。NIO 中的 ByteBuffer 实现的二进制编码
                    区别：
                        编码完成的二进制数组大小要大上几倍
                    缺点：
                        Java 序列后的流会变大，最终会影响到系统的吞吐量。
            4。序列化性能太差
                序列化性能：
                    指标二：序列化的速度
                    问题：
                        序列化的速度慢，就会影响网络通信的效率，从而增加系统的响应时间
                    例子：
                        Java 序列化中的编码耗时要比 ByteBuffer 长很多

    使用 Protobuf 序列化替换 Java 序列化
        优秀的序列化框架
            FastJson、Kryo、Protobuf、Hessian
        推荐：
            Protobuf：
                概念：
                    1。是由 Google 推出且支持多语言的序列化框架
                    2。无论是编解码耗时，还是二进制流压缩大小，都名列前茅
                        在主流序列化框架中
                形式：
                    以一个 .proto 后缀的文件为基础
                         这个文件描述了字段以及字段类型
                扩展：
                    1。通过工具可以生成不同语言的数据结构文件
                    2。在序列化该数据对象的时候，Protobuf 通过.proto 文件描述来生成 Protocol Buffers 格式的编码。
                Protocol Buffers
                    1。存储格式
                        概念：
                            1。一种轻便高效的结构化数据存储格式
                                存储格式： T-L-V（标识 - 长度 - 字段值）
                            T：
                                概念：代表字段的正数序列 (tag)
                                原理：Protocol Buffers 将对象中的每个字段和正数序列对应起来，对应关系的信息是由生成的代码来保证
                                作用：传输流量就可以大幅缩减
                            L：代表 Value 的字节长度，一般也只占一个字节
                            V：代表字段值经过编码后的值
                        优势：
                            1。不需要分隔符
                            2。也不需要空格
                            3。同时减少了冗余字段名
                    2。实现原理
                        原理：
                            1。定义了一套自己的编码方式，几乎可以映射 Java/Python 等语言的所有基础数据类型
                            2。不同的编码方式对应不同的数据类型，还能采用不同的存储格式
                        数据类型
                            1。存储 Varint 编码数据
                                原理：
                                    1。不需要存储字节长度 Length
                                         原因：数据占用的存储空间是固定的
                                    2。实际上 Protocol Buffers 的存储方式是 T - V
                                优点：减少了一个字节的存储空间。

                            2。Protobuf 定义的 Varint 编码方式
                                概念：
                                1。一种变长的编码方式
                                2。每个数据类型一个字节的最后一位是一个标志位 (msb)，用 0 和 1 来表示
                                    0：表示当前字节已经是最后一个字节
                                    1：表示这个数字后面还有一个字节
                                3。对于 int32 类型数字，一般需要 4 个字节表示
                                    Varint 编码方式实现：
                                        1。对于很小的 int32 类型数字，就可以用 1 个字节来表示。
                                            优点：
                                                起到很好地压缩数据的效果
                                            原因：
                                                对于大部分整数类型数据来说，一般都是小于 256
                                2。int32正负数的实现
                                    背景：
                                        1。int32一般最后一位是用来表示正负值
                                        2。Varint 编码方式将最后一位用作了标志位
                                    使用 int32/int64 表示负数就需要多个字节来表示
                                    过程：
                                        1。通过 Zigzag 编码进行转换，将负数转换成无符号数
                                        2。再采用 sint32/sint64 来表示负数
                                    优点：
                                        大大地减少编码后的字节数
                                优点：
                                    1。压缩存储数据的效果好
                                    2。编码和解码的性能方面也很高效
                                    3。整体性能非常优秀。
                                过程：
                                    1。Protobuf 的编码和解码过程结合.proto 文件格式，加上 Protocol Buffer 独特的编码格式
                                    2。只需要简单的数据运算以及位移等操作就可以完成编码与解码
    思考题：
        单例模式实现的类实现了序列化，还是单例？写一个实现了 Java 的 Serializable 接口的单例，你会怎么写呢？
            public class Singleton implements Serializable{
                private final static Singleton singleInstance = new Singleton();
                private Singleton(){}
                public static Singleton getInstance(){
                    return singleInstance; 
                }
            }
        回答：
            1。序列化会通过反射调用无参构造器返回一个新对象，破坏单例模式。
            2。解决方法是添加readResolve()方法，自定义返回对象策略。

10 | 网络通信优化之通信协议：如何优化RPC网络通信？
    RPC 通信是大型服务框架的核心
        核心：
            1。远程通信：
                概念：提供了服务之间通信的桥梁
                主要因素：远程通信的性能需求就是技术选型的主要影响因素
                例子：
                    1。SpringCloud：
                        概念：基于 Feign 组件实现的 RPC 通信（基于 Http+Json 序列化实现）
                    2。Dubbo：
                        概念：是基于 SPI 扩展了很多 RPC 通信框架，包括 RMI、Dubbo、Hessian 等 RPC 通信框架（默认是 Dubbo+Hessian 序列化）
                        优点：
                            可以支持抢购类的高并发，在这个业务场景中，请求的特点是瞬时高峰、请求量大和传入、传出参数数据包较小。

            2。服务治理：
                概念：提供了服务的后勤保障

    什么是 RPC 通信
        背景：
            1。MVC-》RPC-》SOA-》微服务
            2。微服务、SOA、还是 RPC 架构，它们都是分布式服务架构，都需要实现服务之间的互相通信，我们通常把这种通信统称为 RPC 通信。
    概念：
        1。远程服务调用，是通过网络请求远程计算机程序服务的通信技术。
        2。框架封装好了底层网络通信、序列化等技术
            客户端如何使用：
                引入各个服务的接口包，就可以实现在代码中调用 RPC 服务同调用本地方法一样
                优点：方便、透明
        3。通信包括了建立通信、实现报文、传输协议以及传输数据编解码等操作
    RMI：
        概念：
            1。是 JDK 中最先实现了 RPC 通信的框架之一
            2。实现对建立分布式 Java 应用程序至关重要
            3。是 Java 体系非常重要的底层技术，很多开源的 RPC 通信框架也是基于 RMI 实现原理设计出来的
                例如： Dubbo 框架中也接入了 RMI 框架
        应用：
            EJB 以及 Spring 框架中
        特点：
            是纯 Java 网络分布式应用系统的核心解决方案
        作用；
            1。实现了一台虚拟机应用对远程方法的调用可以同对本地方法的调用一样
            2。RMI 帮我们封装好了其中关于远程通信的内容
        实现原理：
            概念：
                1。远程代理对象是 RMI 中最核心的组件
                2。除了对象本身所在的虚拟机，其它虚拟机也可以调用此对象的方法
                3。这些虚拟机可以不在同一个主机上，通过远程代理对象，远程应用可以用网络协议与服务进行通信
        通信过程
            服务端：
                第一步：创建对象
                第二步：注册对象，到RMI Registry
                第五步：调用远程对象的方法
                第七步：骨架代理调用实现方法
                第八步：返回执行结果
                第九步：骨架返回结果给存根

            客户端
                第三步：从RMI Registry先查找注册的远程对象
                第四步：根据name返回远程对象
                第六步：客户端本地存根和服务端骨架通信
                第十步：存根把结果返回客户端
        高并发下场景下的性能瓶颈
            1。Java 默认序列化
              缺点
                1。性能不是很好
                    原因：采用的是 Java 默认的序列化方式
                2。其它语言框架也暂时不支持 Java 序列化。
            2。TCP 短连接
                缺点：非常消耗性能
                    原因：高并发情况下，大量请求会带来大量连接的创建和销毁
            3。阻塞式网络 I/O
                缺点：性能将会大打折扣
                原因：
                    1。网络通信存在 I/O 瓶颈，如果在 Socket 编程中使用传统的 I/O 模型
                    2。在高并发场景下基于短连接实现的网络通信就很容易产生 I/O 阻塞

    一个高并发场景下的 RPC 通信优化路径
        SpringCloud：
            概念：基于 Http 通信协议（短连接）和 Json 序列化实现的
            缺点：
                1。高并发场景下并没有优势
                2。RPC 通信和 RMI 通信的性能瓶颈就非常相似
        1。选择合适的通信协议
            网络通信
                概念：
                    两台设备之间实现数据流交换的过程，是基于网络传输协议和传输数据的编解码来实现的
                    网络传输协议：
                        TCP协议：
                            原理：
                                1。实现的 Socket 通信是有连接的
                                2。传输数据是要通过三次握手来实现数据传输的可靠性，且传输数据是没有边界的，采用的是字节流模式
                        UDP协议：
                            原理：
                                1。实现的 Socket 通信，客户端不需要建立连接
                                2。只需要创建一个套接字发送数据报给服务端
                                    缺点：不能保证数据报一定会达到服务端，具有不可靠性
                                3。UDP 发送的数据采用的是数据报模式，每个 UDP 的数据报都有一个长度，该长度将与数据一起发送到服务端。
                        选择：
                            要求数据可靠性：采用 TCP 协议
                            不要求数据可靠性：考虑使用 UDP 协议，效率要比 TCP 协议高
        2。使用单一长连接
            短连接：
                前提：连接的客户端数量多
                作用：可以避免长时间地占用连接
                优点：导致系统资源浪费
            长连接：
                前提：连接的客户端数量少，向服务端请求的数量却一样多
                作用：可以省去大量的 TCP 建立和关闭连接的操作，
                优点：减少系统的性能消耗，节省时间

        3。优化 Socket 通信
            背景
                1。传统的 Socket 通信主要存在 I/O 阻塞、线程模型缺陷以及内存拷贝等问题。
            推荐：
                比如 Netty。Netty4 对 Socket 通信编程做了很多方面的优化
            1。实现非阻塞 I/O：
                优势：多路复用器 Selector 实现了非阻塞 I/O 通信。
            2。高效的 Reactor 线程模型
                Netty：
                    概念：
                        1。主从 Reactor 多线程模型
                            优点：解决在高负载、高并发的情况下，由于单个 NIO 线程无法监听海量客户端和满足大量 I/O 操作造成的问题。
                        2。服务端接收客户端请求连接是用了一个主线程
                            主线程：用于客户端的连接请求操作，一旦连接建立成功，将会监听 I/O 事件，监听到事件后会创建一个链路请求。
                                链路请求：
                                    作用：将会注册到负责 I/O 操作的 I/O 工作线程上
                                        I/O 工作线程：
                                            作用：负责后续的 I/O 操作。
            3。串行设计：
                概念：Netty 采用了串行无锁化完成链路操作，提供了 Pipeline 实现链路的各个操作在运行期间不进行线程切换。
                采用串行的原因：
                    1。服务端在接收消息之后，存在着编码、解码、读取和发送等链路操作
                    2。如果这些操作都是基于并行去实现，无疑会导致严重的锁竞争，进而导致系统的性能下降
            4。零拷贝：
                概念：
                    1。NIO 提供的 ByteBuffer 可以使用 Direct Buffers 模式，直接开辟一个非堆物理内存
                    2。不需要进行字节缓冲区的二次拷贝，可以直接将数据写入到内核空间。
                一般情况下：
                    1。一个数据从内存发送到网络中，存在着两次拷贝动作
                    2。先是从用户空间拷贝到内核空间，再是从内核空间拷贝到网络 I/O 中
            5。提高网络吞吐量，Netty 可以基于 ChannelOption 来设置这些参数。
                1。TCP_NODELAY：
                    作用：用来控制是否开启 Nagle 算法
                    Nagle 算法原理：
                        1。通过缓存的方式将小的数据包组成一个大的数据包
                        2。从而避免大量的小数据包发送阻塞网络，提高网络传输的效率。
                    建议：
                        关闭该算法，优化对于时延敏感的应用场景
                2。SO_RCVBUF 和 SO_SNDBUF：
                    作用：根据场景调整套接字发送缓冲区和接收缓冲区的大小。
                    原因：
                        1。服务端处理客户端连接请求是按顺序处理的
                        2。所以同一时间只能处理一个客户端连接，当有多个客户端进来的时候
                        3。服务端就会将不能处理的客户端连接请求放在队列中等待处理
                3。SO_BACKLOG：
                    作用：指定了客户端连接请求缓冲队列的大小
                4。SO_KEEPALIVE：
                    作用：连接会检查长时间没有发送数据的客户端的连接状态，检测到客户端断开连接后，服务端将回收该连接
                    建议：
                        可以将该时间设置得短一些，来提高回收连接的效率。
        4. 量身定做报文格式
            目的：提高传输的效率
            角度：
                1。自己的业务和架构来考虑设计
                2。尽量实现报体小、满足功能、易解析等特性。
            做法：设计一套报文，用于描述具体的校验、操作、传输数据等内容。

        5。编码、解码
            思路：
                如果只是单纯的数据对象传输，我们可以选择性能相对较好的 Protobuf 序列化，有利于提高网络通信的性能
        6。调整 Linux 的 TCP 参数设置选项
            建议：
                如果 RPC 是基于 TCP 短连接实现的，我们可以通过修改 Linux TCP 配置项来优化网络通信
            三次握手和四次握手：
                参考趣谈网络协议的第11讲
            修改TCP参数：
                1。通过 sysctl -a | grep net.xxx 命令运行
                    概念：查看 Linux 系统默认的的 TCP 参数设置
                2。vim/etc/sysctl.conf，加入需要修改的配置项
                3。sysctl -p 命令
                    作用：运行生效修改后的配置项设置

11 | 答疑课堂：深入了解NIO的优化实现原理（TODO）（）补充
    背景：
        1。Tomcat 8.5 版本之前，默认情况下使用的是 BIO 线程模型
        2。如果在高负载、高并发的场景下，可以通过设置 NIO 线程模型，来提高系统的网络通信性能。
        3。通过测试，Tomcat 在 I/O 读写操作比较多的情况下，使用 NIO 线程模型有明显的优势。
    网络 I/O 模型优化
    《UNIX 网络编程》的物种I/O模型
        1。阻塞式 I/O
            概念：
                1。每一个连接创建时，都需要一个用户线程来处理
                2。在 I/O 操作没有就绪或结束时，线程会被挂起，进入阻塞等待状态
            问题：阻塞到底发生在套接字（socket）通信的哪些环节呢？
                套接字（socket）：
                    1。流式套接字（TCP）
                    2。数据报套接字（UDP）
                假设一次最简单的 TCP 数据传输）：
                    1。应用程序通过系统调用 socket 创建一个套接字，它是系统分配给应用程序的一个文件描述符；
                    2。应用程序会通过系统调用 bind，绑定地址和端口号，给套接字命名一个名称
                    3。系统会调用 listen 创建一个队列用于存放客户端进来的连接；
                    4。应用服务会通过系统调用 accept 来监听客户端的连接请求。
            注意：
                1。有一个客户端连接到服务端之后，服务端就会调用 fork 创建一个子进程
                2。通过系统调用 read 监听客户端发来的消息，再通过 write 向客户端返回信息。
            流程：
                1。整个 socket 通信工作流程中，socket 的默认状态是阻塞的
                    说明：当发出一个不能立即完成的套接字调用时，其进程将被阻塞，被系统挂起，进入睡眠状态，一直等待相应的操作响应
                        三种阻塞：
                            1。connect 阻塞：
                                过程：
                                    1。当客户端发起 TCP 连接请求，通过系统调用 connect 函数
                                    2。TCP 连接的建立需要完成三次握手过程，客户端需要等待服务端发送回来的 ACK 以及 SYN 信号
                                    3。同样服务端也需要阻塞等待客户端确认连接的 ACK 信号
                                结论：
                                    TCP 的每个 connect 都会阻塞等待，直到确认连接。

                            2。accept 阻塞：
                                概念：如果没有新的连接到达，调用进程将被挂起，进入阻塞状态
                                原因：一个阻塞的 socket 通信的服务端接收外来连接，会调用 accept 函数
                            3。read、write 阻塞：
                                过程
                                    1。当一个 socket 连接创建成功之后，服务端用 fork 函数创建一个子进程
                                    2。调用 read 函数等待客户端的数据写入，如果没有数据写入
                                    3。调用子进程将被挂起，进入阻塞状态。

        2。非阻塞式 I/O
            概念：
                1。使用 fcntl 可以把以上三种操作都设置为非阻塞操作。
                2。如果没有数据返回，就会直接返回一个 EWOULDBLOCK 或 EAGAIN 错误，此时进程就不会一直被阻塞
                3。当我们把以上操作设置为了非阻塞状态，我们需要设置一个线程对该操作进行轮询检查
                缺点：
                    大量请求的情况下不实用
                原因：
                    使用用户线程轮询查看一个 I/O 操作的状态，对于 CPU 的使用率无疑是种灾难
        3。I/O 复用
            概念：
                1。I/O 复用函数 select/poll/epoll，进程将一个或多个读操作通过系统调用函数，阻塞在函数操作上
                2。系统内核就可以帮我们侦测多个读操作是否处于就绪状态
            1。select() 函数：
                背景：
                    Linux 操作系统的内核将所有外部设备都看做一个文件来操作，对一个文件的读写操作会调用内核提供的系统命令
                作用：
                    在超时时间内，监听用户感兴趣的文件描述符上的可读可写和异常事件的发生

            2。poll() 函数：
            3。epoll() 函数：

        4。信号驱动式 I/O 
            概念：
                内核就是一个观察者，信号回调则是通知
        5。异步 I/O
        意义：每一种 I/O 模型的出现，都是基于前一种 I/O 模型的优化升级。

12 | 多线程之锁优化（上）：深入了解Synchronized同步锁的优化方法
    背景：
        1。并发编程中，多个线程访问同一个共享资源时，我们必须考虑如何维护数据的原子性
    Lock：jdk1。5版本：
        概念：
            1。并发包中新增了 Lock 接口来实现锁功能
            2。提供了与 Synchronized 关键字类似的同步功能，只是在使用时需要显示获取和释放锁。
            3。同步锁是基于 Java 实现的
    Synchronized
        jdk1.5之前
            概念：
            1.Java 是依靠 Synchronized 关键字实现锁功能来做到这点的
            2.Synchronized 是 JVM 实现的一种内置锁，锁的获取和释放是由 JVM 隐式实现。
            3.基于底层操作系统的 Mutex Lock 实现的，每次获取和释放锁操作都会带来用户态和内核态的切换
            缺点：
                增加系统性能开销
            例子：
                1。单个线程重复申请锁的情况下，JDK1.5 版本的 Synchronized 锁性能要比 Lock 的性能差很多
                2。在 Dubbo 基于 Netty 实现的通信中，消费端向服务端通信之后，由于接收返回消息是异步，所以需要一个线程轮询监听返回信息
                3。在接收消息时，就需要用到锁来确保 request session 的原子性
                4。如果我们这里使用 Synchronized 同步锁，那么每当同一个线程请求锁资源时，都会发生一次用户态和内核态的切换
        同步锁的方式：
            1。修饰方法
                public synchronized void method1() {
                    // code
                }
                原理：
                    1。当方法调用时，调用指令将会检查该方法是否被设置 ACC_SYNCHRONIZED 访问标志
                        ACC_SYNCHRONIZED：
                            作用：访问标志来区分一个方法是否是同步方法
                    2。如果设置了该标志，执行线程将先持有 Monitor 对象，然后再执行方法。
                    3。在该方法运行期间，其它线程将无法获取到该 Mointor 对象，当方法执行完成后，再释放该 Monitor 对象


            2。修饰同步代码块
                // 关键字在代码块上，锁为括号里面的对象
                    public void method2() {
                        Object o = new Object();
                            synchronized (o) {
                        // code
                        }
                    }
                原理：
                    1。是由 monitorenter 和 monitorexit 指令来实现同步的
                    2。进入 monitorenter 指令后，线程将持有 Monitor 对象
                    3。退出 monitorenter 指令后，线程将释放该 Monitor 对象。

            锁原理：
                概念：
                    1。JVM 中的同步是基于进入和退出管程（Monitor）对象实现的
                    2。每个对象实例都会有一个 Monitor，Monitor 可以和对象一起创建、销毁
                    3。Monitor 是由 ObjectMonitor 实现，而 ObjectMonitor 是由 C++ 的 ObjectMonitor.hpp 文件实现
                具体过程：
                    1。当多个线程同时访问一段同步代码时，多个线程会先被存放在 EntryList 集合中，处于 block 状态的线程，都会被加入到该列表
                    2。接下来当线程获取到对象的 Monitor 时，Monitor 是依靠底层操作系统的 Mutex Lock 来实现互斥
                    3。线程申请 Mutex 成功，则持有该 Mutex，其它线程将无法获取到该 Mutex。
                    4。如果线程调用 wait() 方法，就会释放当前持有的 Mutex
                    5。并且该线程会进入 WaitSet 集合中，等待下一次被唤醒。如果当前线程顺利执行完方法，也将释放 Mutex。
                结论：
                    1。Monitor 是依赖于底层的操作系统实现，存在用户态与内核态之间的切换，所以增加了性能开销。

    JDK1。6之后
        对象实例：
            1。对象头
                1。Mark Word：
                    1。记录了对象和锁有关的信息
                    2。Mark Word 在 64 位 JVM 中的长度是 64bit
                    注意：锁升级功能主要依赖于 Mark Word 中的锁标志位和释放偏向锁标志位
                2。指向类的指针
                3。数组长度
            2。实例数据
            3。对齐填充
        概念：
            引入了偏向锁、轻量级锁、重量级锁概念
        目的：
            来减少锁竞争带来的上下文切换，提升性能
        实现：
            新增的 Java 对象头实现了锁升级功能
                升级过程：
                    1。Synchronized 同步锁就是从偏向锁开始的，随着竞争越来越激烈
                    2。偏向锁升级到轻量级锁，最终升级到重量级锁
        1。偏向锁：
            目的：
                用来优化同一线程多次申请同一个锁的竞争
            例子：
                1。创建一个线程并在线程中执行循环监听的场景下，或单线程操作一个线程安全集合时
                2。同一线程每次都需要获取和释放锁，每次操作都会发生用户态与内核态的切换。
            作用：
                1。当一个线程再次访问这个同步代码或方法时
                2。该线程只需去对象头的 Mark Word 中去判断一下是否有偏向锁指向它的 ID
                3。无需再进入 Monitor 去竞争对象了
            流程：
                1。当对象被当做同步锁并有一个线程抢到了锁时，锁标志位还是 01
                2。“是否偏向锁”标志位设置为 1，并且记录抢到锁的线程 ID，表示进入偏向锁状态
                3。一旦出现其它线程竞争锁资源时，偏向锁就会被撤销
                4。偏向锁的撤销需要等待全局安全点，暂停持有该锁的线程，同时检查该线程是否还在执行该方法，
                5。如果是，则升级锁，反之则被其它线程抢占。
            缺点（高并发）：
                开启偏向锁无疑会带来更大的性能开销
                原因：
                    高并发场景下，当大量线程同时竞争同一个锁资源时，偏向锁就会被撤销，发生 stop the word
                方法：
                    关闭偏向锁，提高性能
                    -XX:-UseBiasedLocking // 关闭偏向锁（默认打开）
                    -XX:+UseHeavyMonitors  // 设置重量级锁
        2。轻量级锁
            流程：
                1。当有另外一个线程竞争获取这个锁时，由于该锁已经是偏向锁
                2。当发现对象头 Mark Word 中的线程 ID 不是自己的线程 ID，就会进行 CAS 操作获取锁
                3。如果获取成功，直接替换 Mark Word 中的线程 ID 为自己的 ID，该锁会保持偏向锁状态
                4。如果获取锁失败，代表当前锁有一定的竞争，偏向锁将升级为轻量级锁。
            场景：
                用于线程交替执行同步块的场景，绝大部分的锁在整个同步周期内都不存在长时间的竞争。

        3。自旋锁与重量级锁：
            自旋锁：
                背景：
                    1。轻量级锁 CAS 抢锁失败，线程将会被挂起进入阻塞状态
                    2。如果正在持有锁的线程在很短的时间内释放资源，那么进入阻塞状态的线程无疑又要申请锁资源。
                目的：
                    可以通过自旋方式不断尝试获取锁，从而避免线程被挂起阻塞
                    原因：
                        大多数情况下，线程持有锁的时间都不会太长，毕竟线程被挂起阻塞可能会得不偿失
                概念：
                    1。从 JDK1.7 开始，自旋锁默认启用
                    2。自旋次数由 JVM 设置决定
                建议：
                    不建议设置的重试次数过多
                原因：
                    CAS 重试操作意味着长时间地占用 CPU。
                优点：
                    锁竞争不激烈且锁占用时间非常短的场景下，自旋锁可以提高系统性能
                缺点：
                    锁竞争激烈或锁占用的时间过长，自旋锁将会导致大量的线程一直处于 CAS 重试状态，占用 CPU 资源，反而会增加系统性能开销
                解决方法：
                    高负载、高并发的场景下，我们可以通过设置 JVM 参数来关闭自旋锁，优化系统性能
                    -XX:-UseSpinning // 参数关闭自旋锁优化 (默认打开) 
                    -XX:PreBlockSpin // 参数修改默认的自旋次数。JDK1.7 后，去掉此参数，由 jvm 控制

        动态编译实现锁消除 / 锁粗化
            锁消除：
                背景：Java7 之后的版本就不需要手动配置了，该操作可以自动实现。
                概念：不会生成 synchronized 所表示的锁的申请与释放的机器码，即消除了锁的使用
                实现：
                    借助了一种被称为逃逸分析的技术，来判断同步块使用的锁对象是否只能够被一个线程访问，而没有被发布到其它线程

            锁粗化：
                流程：
                    1。在 JIT 编译器动态编译时，如果发现几个相邻的同步块使用的是同一个锁实例
                    2。那么 JIT 编译器将会把这几个同步块合并为一个大的同步块
                    3。从而避免一个线程“反复申请、释放同一个锁“所带来的性能开销。

        减小锁粒度
            背景：
                锁对象是一个数组或队列时，集中竞争一个对象的话会非常激烈，锁也会升级为重量级锁。
            方法：
                可以考虑将一个数组和队列对象拆成多个小对象，来降低锁竞争，提升并行度
            例子
                1。JDK1.8 之前实现的 ConcurrentHashMap 版本
                1。并发读写操作集合时，存在激烈的锁资源竞争，也因此性能会存在瓶颈
                3。ConcurrentHashMap 就很很巧妙地使用了分段锁 Segment 来降低锁资源竞争

13 | 多线程之锁优化（中）：深入了解Lock同步锁的优化方法
    背景
        1。在并发量不高、竞争不激烈的情况下，Synchronized 同步锁由于具有分级锁的优势，性能上与 Lock 锁差不多
        2。但在高负载、高并发的情况下，Synchronized 同步锁由于竞争激烈会升级到重量级锁，性能则没有 Lock 锁稳定
    Lock 锁的实现原理
        概念：
            锁是基于 Java 实现的锁，Lock 是一个接口类
        例子
            ReentrantLock，ReentrantReadWriteLock
        实现：
            都是依赖 AbstractQueuedSynchronizer（AQS）类实现的。
        原理：
            1。AQS 类结构中包含一个基于链表实现的等待队列（CLH 队列）
                作用：用于存储所有阻塞的线程，
            2。AQS 中还有一个 state 变量
                作用：对 ReentrantLock 来说表示加锁状态。
    锁分离优化 Lock 同步锁
    背景：影响数据不一致
        1。一个线程在读数据，另一个线程在写数据
        2。一个线程在写数据，另一个线程也在写数据
        3。大部分业务场景中，读业务操作要远远大于写业务操作
    1。读写锁 ReentrantReadWriteLock
        场景：读多写少的场景
        背景：
            1。允许多个读线程同时访问
            2。不允许写线程和读线程
            3。写线程和写线程同时访问
            4。内部维护了两个锁，一个是用于读操作的 ReadLock，一个是用于写操作的 WriteLock。
        如何实现锁分离来保证共享资源的原子性呢？
            概念：
                1。基于 AQS 实现的
                    自定义同步器（继承 AQS）：
                        需要在同步状态 state 上维护多个读线程和一个写线程的状态
                2。使用了高低位，来实现一个整型控制两种状态的功能
                        1。高 16 位表示读，
                        2。低 16 位表示写
            原理：
                1。一个线程尝试获取写锁时，会先判断同步状态 state 是否为 0
                    1。state==0，说明暂时没有其它线程获取锁
                        1。说明暂时没有其它线程获取锁则进入 CLH 队列进行阻塞等待
                        2。判断是否为首节点且CAS获取锁状态：
                            成功：CAS进行修改低16为写锁同步状态，得到锁
                            失败：休眠当前线程
                    2。state!=0，则说明有其它线程获取了锁
                        判断同步状态 state 的低 16 位（w）是否为 0
                            1。w==0，说明其它线程获取了读锁，此时进入 CLH 队列进行阻塞等待
                            2。w!=0，说明其它线程获取了写锁，此时要判断获取了写锁的是不是当前线程
                                1。不是，是就进入 CLH 队列进行阻塞等待
                                2。是，再判断当前线程获取写锁是否超过了最大次数
                                    1。超过，抛异常
                                    2。没有超过，更新同步状态(判断是否是公平锁)
                                        1。是，将当前没有获取到锁的线程放入CLH列表中
                                            判断是否为首节点且CAS获取锁状态：
                                            成功：CAS进行修改低16为写锁同步状态，得到锁
                                            失败：休眠当前线程
                                        2。否：CAS进行修改低16为写锁同步状态，得到锁
                2。一个线程尝试获取读锁时，会先判断同步状态 state 是否为 0
                    1。state==0，说明暂时没有其它线程获取锁
                        1。说明暂时没有其它线程获取锁则进入 CLH 队列进行阻塞等待
                        2。判断是否为首节点且CAS获取锁状态：
                            成功：CAS进行修改高16为写锁同步状态，得到锁
                            失败：休眠当前线程
                    2。state!=0，则说明有其它线程获取了锁
                        1。会判断同步状态低 16 位是否为0，是否存在写锁
                            1。存在，则获取读锁失败，进入 CLH 阻塞队列
                                2。判断是否为首节点且CAS获取锁状态：
                                    成功：CAS进行修改高16为写锁同步状态，得到锁
                                    失败：休眠当前线程
                            2。不存在，判断读锁是否为当前线程
                                否：则获取读锁失败，进入 CLH 阻塞队列
                                    2。判断是否为首节点且CAS获取锁状态：
                                        成功：CAS进行修改高16为写锁同步状态，得到锁
                                        失败：休眠当前线程
                                是：获取高16位读锁状态，判断当前获取读锁是否超过最大次数
                                    1。超过：抛异常
                                    2。没有超过，更新同步状态(判断是否是公平锁)
                                        1。是，将当前没有获取到锁的线程放入CLH列表中
                                            判断是否为首节点且CAS获取锁状态：
                                                成功：CAS进行修改低16为写锁同步状态，得到锁
                                                失败：休眠当前线程
                                        2。否：CAS进行修改低16为写锁同步状态，得到锁
        缺点：
            读取很多、写入很少的情况下，RRW 会使写入线程遭遇饥饿（Starvation）问题。
                说明：写入线程会因迟迟无法竞争到锁而一直处于等待状态。
        方法：
            jdk1。8引入StampedLock
    2。读写锁再优化之 StampedLock
        背景：
            1。RRW 被很好地应用在了读大于写的并发场景中，然而 RRW 在性能上还有可提升的空间
            2。在读取很多、写入很少的情况下，RRW 会使写入线程遭遇饥饿（Starvation）问题
            3。也就是说写入线程会因迟迟无法竞争到锁而一直处于等待状态。
        概念：
            1。StampedLock 不是基于 AQS 实现的
            2。实现的原理和 AQS 是一样的，都是基于队列和锁状态实现的。
        AQS区别
            1。StampedLock控制锁有三种模式：写、悲观读以及乐观读
            2。StampedLock 在获取锁时会返回一个票据 stamp，获取的 stamp 除了在释放锁时需要校验
            3。在乐观读模式下，stamp 还会作为读取共享资源后的二次校验
        流程
            写锁：
                概念：
                    1。WriteLock 是一个独占锁，同时只有一个线程可以获取该锁
                    2。当一个线程获取该锁后，其它请求的线程必须等待，当没有线程持有读锁或者写锁的时候才可以获取到该锁
                流程：
                    1。首先是通过 WriteLock 获取一个票据 stamp
                    2。请求该锁成功后会返回一个 stamp 票据变量，用来表示该锁的版本
                    3。当释放该锁的时候，需要 unlockWrite 并传递参数 stamp。
            读锁：
                1。首先线程会通过乐观锁 tryOptimisticRead 操作获取票据 stamp 
                2。如果当前没有线程持有写锁，则返回一个非 0 的 stamp 版本信息
                3。线程获取该 stamp 后，将会拷贝一份共享资源到方法栈
                4。在这之前具体的操作都是基于方法栈的拷贝数据。
                5。之后方法还需要调用 validate，验证之前调用 tryOptimisticRead 返回的 stamp 在当前是否有其它线程持有了写锁
                    1。是，那么 validate 会返回 0，升级为悲观锁；
                    2。不是，就可以使用该 stamp 版本的锁对数据进行操作。
        优点：
            1。StampedLock 获取读锁只是使用与或操作进行检验，不涉及 CAS 操作
            2。即使第一次乐观锁获取失败，也会马上升级至悲观锁
            3。这样就可以避免一直进行 CAS 操作带来的 CPU 占用性能的问题，因此 StampedLock 的效率更高。
        缺点：
            1。不支持重入
            2。不支持条件变量
            3。线程中断时可能导致CPU暴涨

14 | 多线程之锁优化（下）：使用乐观锁优化并行操作
    悲观锁
        例子：Synchronized 和 Lock 实现的同步锁机制
        缺点：
            1。悲观锁在高并发的场景下，激烈的锁竞争会造成线程阻塞
            2。大量阻塞线程会导致系统的上下文切换，增加系统的性能开销。
        解决：
            乐观锁
    什么是乐观锁
        概念：
            1。操作共享资源时，它总是抱着乐观的态度进行，它认为自己可以成功地完成操作。
            2。当多个线程同时操作一个共享资源时，只有一个线程会成功
                其他失败的线程：
                    1。不会像悲观锁一样在操作系统中挂起，而仅仅是返回
                    2。并且系统允许失败的线程重试，也允许自动放弃退出操作。
        优点：
            1。乐观锁相比悲观锁来说，不会带来死锁、饥饿等活性故障问题
            2。线程间的相互影响也远远比悲观锁要小。
            3。乐观锁没有因竞争造成的系统开销，所以在性能上也是更胜一筹。
    乐观锁的实现原理
        CAS：三个参数
            V：需要更新的变量
            E：预期值
            N：最新值
        概念：
            1。是实现乐观锁的核心算法
        过程：
            1。只有当需要更新的变量等于预期值时，需要更新的变量才会被设置为最新值
            2。如果更新值和预期值不同，则说明已经有其它线程更新了需要更新的变量
            3。此时当前线程不做操作，返回 V 的真实值。
        1。CAS 如何实现原子操作
            背景：
                1。JDK 中的 concurrent 包中，atomic 路径下的类都是基于 CAS 实现的
            例子
                AtomicInteger，AtomicLong等
            原理：
                1。AtomicInteger 的自增方法 getAndIncrement 是用了 Unsafe 的 getAndAddInt 方法
                2。显然 AtomicInteger 依赖于本地方法 Unsafe 类
                3。Unsafe 类中的操作方法会调用 CPU 底层指令实现原子操作。
        2。处理器如何实现原子操作
            背景：
                1。处理器有自己的内部缓存。
                    原因：处理器和物理内存之间的通信速度要远慢于处理器间的处理速度
                    优点：在执行操作时，频繁使用的内存数据会缓存在处理器的 L1、L2 和 L3 高速缓存中，以加快频繁读取的速度。
            单核处理器：
                1。自我保证基本的内存操作是原子性的
                2。当一个线程读取一个字节时，所有进程和线程看到的字节都是同一个缓存里的字节
                3。其它线程不能访问这个字节的内存地址。
            多核处理器：
                1。每个处理器都是多核的
                2。每个处理器维护了一块字节的内存，每个内核维护了一块字节的缓存
                问题：
                    这时候多线程并发就会存在缓存不一致的问题，从而导致数据不一致。
                方法
                    1。总线锁定：
                            原理：
                                1。当处理器要操作一个共享变量的时候，其在总线上会发出一个 Lock 信号
                                2。这时其它处理器就不能操作共享变量了，该处理器会独享此共享内存中的变量
                            缺点：
                                阻塞其它处理器获取该共享变量的操作请求时，可能会导致大量阻塞，从而增加系统的性能开销
                    2。缓存锁定：(最新的处理器都支持缓存锁定机制。)
                            1。某个处理器对缓存中的共享变量进行了操作
                            2。就会通知其它处理器放弃存储该共享资源或者重新读取该共享资源

    优化 CAS 乐观锁
        背景：
            1。乐观锁在并发性能上要比悲观锁优越，但是在写大于读的操作场景下
            2。CAS 失败的可能性会增大，如果不放弃此次 CAS 操作，就需要循环做 CAS 重试，这无疑会长时间地占用 CPU。
        java7中
            AtomicInteger 的 getAndSet 方法中使用了 for 循环不断重试 CAS 操作，如果长时间不成功，就会给 CPU 带来非常大的执行开销。
        java8
            for 循环虽然被去掉了，但我们反编译 Unsafe 类时就可以发现该循环其实是被封装在了 Unsafe 类中，CPU 的执行开销依然存在
            解决方法： LongAdder
                优点：LongAdder 在高并发场景下会比 AtomicInteger 和 AtomicLong 的性能更好
                缺点：代价就是会消耗更多的内存空间
        原理：
            1。就是降低操作共享变量的并发数，
                说明：也就是将对单一共享变量的操作压力分散到多个变量值上，将竞争的每个写线程的 value 值分散到一个数组中
            2。不同线程会命中到数组的不同槽中，各个线程只对自己槽中的 value 值进行 CAS 操作
            3。最后在读取值的时候会将原子操作的共享变量与各个分散在数组的 value 值相加，返回一个近似准确的数值
                组成：base 变量和一个 cell[] 数组组成
                    一个线程：
                        没有竞争的情况下，LongAdder 会直接使用 base 变量作为原子操作变量，通过 CAS 操作修改变量
                    多个线程：
                        除了占用 base 变量的一个写线程之外，其它各个线程会将修改的变量写入到自己的槽 cell[] 数组中
        缺点：
            1。LongAdder 在操作后的返回值只是一个近似准确的数值
            2。在一些对实时性要求比较高的场景下，LongAdder 并不能取代 AtomicInteger 或 AtomicLong。

15 | 多线程调优（上）：哪些操作导致了上下文切换？
    背景：
        1。并发程序中，并不是启动更多的线程就能让程序最大限度地并发执行。
        2。线程数量设置太小，会导致程序不能充分地利用系统资源；
        3。线程数量设置太大，又可能带来资源的过度竞争，导致上下文切换带来额外的系统开销。

    初识上下文切换
        上下文切换：
            概念：
                一个线程被暂停剥夺使用权，另外一个线程被选中开始或者继续运行的过程
            过程：
                1。当一个线程的时间片用完了，或者因自身原因被迫暂停运行了
                    CPU 时间片：
                        概念：
                            1。CPU 分配给每个线程执行的时间段，一般为几十毫秒
                            2。决定了一个线程可以连续占用处理器运行的时长。
                2。这个时候，另外一个线程（可以是同一个线程或者其它进程的线程）就会被操作系统选中，来占用处理器
            上下文：
                概念：
                    切出切入的过程中，操作系统需要保存和恢复相应的进度信息
                        切出：一个线程被剥夺处理器的使用权而被暂停运行
                        切入：一个线程被选中占用处理器开始或者继续运行
                内容：
                    1。寄存器的存储内容
                        作用：负责存储已经、正在和将要执行的任务
                    2。程序计数器存储的指令内容
                        作用：负责存储 CPU 正在执行的指令位置以及即将执行的下一条指令的位置。
                            cpu数量多的情况下：
                                1。操作系统将 CPU 轮流分配给线程任务，此时的上下文切换就变得更加频繁了
                                2。并且存在跨 CPU 上下文切换，比起单核上下文切换，跨核切换更加昂贵。
    多线程上下文切换诱因

        背景：
            1。在操作系统中，上下文切换的类型还可以分为进程间的上下文切换和线程间的上下文切换
            2。在多线程编程中，我们主要面对的就是线程间的上下文切换导致的性能问题
        线程的的五种状态
            1。“新建”（NEW）
            2。“就绪”（RUNNABLE）
            3。“运行”（RUNNING）
            4。“阻塞”（BLOCKED）
            5。“死亡”（DEAD）
        提示：这个运行过程中，线程由 RUNNABLE 转为非 RUNNABLE 的过程就是线程上下文切换
        一个上下文切换的过程例子：
            1。一个线程的状态RUNNING-》BLOCKED
                概念：
                    1。称为一个线程的暂停
                过程：
                    1。线程暂停被切出之后，操作系统会保存相应的上下文
                        目的：
                            以便这个线程稍后再次进入 RUNNABLE 状态时能够在之前执行进度的基础上继续执行。
            2。再由BLOCKED-》RUNNABLE
                概念：
                    1。称为一个线程的暂停
                过程：
                    1。此时线程将获取上次保存的上下文继续完成执行。
            3。然后再被调度器选中执行
        什么诱发的切换：
            1。自发性上下文切换
                概念：
                    1。程序本身触发的切换
                        说明：指线程由 Java 程序调用导致切出
                例子：多线程编程中，执行调用以下方法或关键字
                    1.sleep()
                    2.wait()
                    3.yield()
                    4.join()
                    4.park()
                    5.synchronized
                    6.lock
            2。非自发性上下文切换
                概念：
                    1.由系统或者虚拟机诱发
                        说明：指线程由于调度器的原因被迫切出
                例子：
                    1。线程被分配的时间片用完
                    2。虚拟机垃圾回收导致
                        原因：
                            stop-the-world 事件的发生
                        什么时候发生：
                            堆内存将很快被耗尽
                                原因：
                                    1。在 Java 虚拟机中，对象的内存都是由虚拟机中的堆分配的，在程序运行过程中，新的对象将不断被创建
                                    2。如果旧的对象使用后不进行回收，堆内存将很快被耗尽。
                        作用：
                            对创建后不再使用的对象进行回收，从而保证堆内存的可持续性分配
                    3。执行优先级的问题

    发现上下文切换
        参考代码：profit.jikeshijian.xiengnengyouhua.DemoApplication15
        运行结果：
            multi thread exce time: 4632s
            counter: 100000000
            serial exec time: 650s
            counter: 100000000
        通过数据对比我们可以看到
        1。串联的执行速度比并发的执行速度要快
            原因：
                1。线程的上下文切换导致了额外的开销，
                2。使用 Synchronized 锁关键字，导致了资源竞争，从而引起了上下文切换
                3。即使不使用 Synchronized 锁关键字，并发的执行速度也无法超越串联的执行速度
                    原因：
                        多线程同样存在着上下文切换。
            例子：
                Redis、NodeJS
            查看上下文命令：
                1。vmstat：监视 Java 程序运行过程中系统的上下文切换频率
                2。pidstat：监控指定进程的 Context Switch 上下文切换。
            系统开销的具体环节：
                1。操作系统保存和恢复上下文
                2。调度器进行线程调度
                3。处理器高速缓存重新加载
                4。上下文切换也可能导致整个高速缓存区被冲刷，从而带来时间开销

16 | 多线程调优（下）：如何优化多线程上下文切换？
    背景：
        1。多线程中如果使用了竞争锁，当线程由于等待竞争锁而被阻塞
        2。JVM 通常会将这个锁挂起，并允许它被交换出去
        3。如果频繁地发生阻塞，CPU 密集型的程序就会发生更多的上下文切换。
    竞争锁优化
        概念：
            锁其实不是性能开销的根源，竞争锁才是。
        原因：
            1。多线程对锁资源的竞争会引起上下文切换
            2。锁竞争导致的线程阻塞越多，上下文切换就越频繁，系统的性能开销也就越大
    方法：
    1。减少锁的持有时间
        概念：
            1。锁的持有时间越长，就意味着有越多的线程在等待该竞争资源释放
            2。如果是 Synchronized 同步锁资源，就不仅是带来线程间的上下文切换，还有可能会增加进程间的上下文切换。
        方法：
            将一些与锁无关的代码移出同步代码块，尤其是那些开销较大的操作以及可能被阻塞的操作。
        例子：
            优化前：
                public synchronized void mySyncMethod(){  
                    businesscode1();
                    mutextMethod();
                    businesscode2();
                }
            优化后：
                public void mySyncMethod(){  
                    businesscode1();
                    synchronized(this)
                    {
                        mutextMethod();
                    }
                    businesscode2();
                }
    2。降低锁的粒度
        概念：
            1。同步锁可以保证对象的原子性，我们可以考虑将锁粒度拆分得更小一些
            2。以此避免所有线程对一个锁资源的竞争过于激烈
        方式
            1。锁分离
                传统锁：没有实现；
                    现象：
                        读读互斥、读写互斥、写写互斥
                读写锁：实现了
                    概念：
                        1。是由“读锁”和“写锁”两个锁实现的，
                        2。其规则是可以共享读，但只有一个写。
                    现象：
                        1。读读是不互斥的，读写是互斥的，写写是互斥的
                    优点：
                        读远大于写的多线程场景中，锁分离避免了在高并发读情况下的资源竞争，从而避免了上下文切换。
            2。锁分段
                前面讲的 ConcurrentHashMap
    3。非阻塞乐观锁替代竞争锁
        volatile：
            优点：：
                读写操作不会导致上下文切换，因此开销比较小
            缺点：
                不能保证操作变量的原子性，因为没有锁的排他性。
            作用：保障可见性及有序性
        CAS：
            概念：
                1。是一个原子的 if-then-act 操作
                2。CAS 是一个无锁算法实现，保障了对一个共享变量读写操作的一致性
            组成：
                V：内存值
                A：旧的预期值
                B：要修改的新值
            流程：
                1。当且仅当 A 和 V 相同时，将 V 修改为 B，否则什么都不做
            优点：
                不会导致上下文切换。
            例子：
                Java 的 Atomic 包就使用了 CAS 算法来更新数据，就不需要额外加锁
            JVM 内部对Synchronized进行了优化：
                1。偏向锁——》轻量级锁--》重量级锁，
                2。JIT 编译器在动态编译同步块的时候，也会通过锁消除、锁粗化的方式来优化该同步锁。

        wait/notify 优化
            概念：
                1。配合调用 Object 对象的 wait() 方法和 notify() 方法或 notifyAll() 方法来实现线程间的通信。
            wait() ：
                概念：阻塞等待其它线程的通知
            notify() ：
                概念：通知其它线程从 wait() 方法处返回。
            例子：
                profit.jikeshijian.xiengnengyouhua.WaitNotifyTest16
                流程：
                    消费者：
                        1。在消费者第一次申请到锁之前，发现没有商品消费
                        2。此时会执行 Object.wait() 方法，这里会导致线程挂起，进入阻塞状态，这里为一次上下文切换
                    生产者：
                        1。获取到锁并执行 notifyAll() 之后，会唤醒处于阻塞状态的消费者线程，此时这里又发生了一次上下文切换。
                            notifyAll分析：
                                概念：如果有多个消费者线程同时被阻塞，用 notifyAll() 方法，将会唤醒所有阻塞的线程
                                缺点：
                                1。被唤醒的等待线程在继续运行时，也可能会导致上下文切换。
                                    原因：
                                        需要再次申请相应对象的内部锁，此时等待线程可能需要和其它新来的活跃线程争用内部锁
                                2。可能会导致线程再次进入阻塞状态，从而引起不必要的上下文切换。
                                    原因：
                                        某些商品依然没有库存，过早地唤醒这些没有库存的商品的消费线程

        优化 wait/notify 的使用，减少上下文切换
            流程
                1：使用 Object.notify() 替代 Object.notifyAll()
                    原因：
                        Object.notify() 只会唤醒指定线程，不会过早地唤醒其它未满足需求的阻塞线程
                    优点：
                        可以减少相应的上下文切换。
                2。尽快地释放内部锁
                    时间：在生产者执行完 Object.notify() / notifyAll() 唤醒其它线程之后
                    原因：
                        避免其它线程在唤醒之后长时间地持有锁处理业务操作
                    优点：
                        可以避免被唤醒的线程再次申请相应内部锁的时候等待锁的释放。
                3。避免长时间等待
                    例子：使用 Object.wait (long）设置等待超时时间
                    缺点：
                        增加了上下文切换。
                    原因：
                            线程无法区分其返回是由于等待超时还是被通知线程唤醒，从而导致线程再次尝试获取锁操作
                    建议：
                        Lock 锁结合 Condition 接口替代 Synchronized 内部锁中的 wait / notify，实现等待／通知。
                        优点：
                            1。解决上述的 Object.wait(long) 无法区分的问题
                            2。可以解决线程被过早唤醒的问题。

        合理地设置线程池大小，避免创建过多线程
            1。线程池的线程数量设置不宜过大
                原因：
                    一旦线程池的工作线程总数超过系统所拥有的处理器数量，就会导致过多的上下文切换
            2。线程数量设置不会直接暴露给我们
                例子：
                    Executors.newCachedThreadPool() 创建的线程池
                    原理：
                        1。该线程池会复用其内部空闲的线程来处理新提交的任务
                        2。如果没有，再创建新的线程（不受 MAX_VALUE 限制），这样的线程池如果碰到大量且耗时长的任务场景，就会创建非常多的工作线程
                    缺点：
                        导致频繁的上下文切换
                    场景：
                        这类线程池就只适合处理大量且耗时短的非阻塞任务。
        使用协程实现非阻塞等待
            协程
                概念：
                    1。一种比线程更加轻量级的东西，相比于由操作系统内核来管理的进程和线程
                    2。协程则完全由程序本身所控制，也就是在用户态执行。
                优点：
                    性能方面得到了很大的提升
                原因：
                    避免了像线程切换那样产生的上下文切换
        减少 Java 虚拟机的垃圾回收
            原因：
                垃圾回收需要暂停线程，在移动完成后需要再次唤醒该线程。
                    暂停的原因：
                        1。很多 JVM 垃圾回收器（serial 收集器、ParNew 收集器）在回收旧对象时，会产生内存碎片，从而需要进行内存整理，
                        2。在这个过程中就需要移动存活的对象。而移动内存对象就意味着这些对象所在的内存地址会发生变化

17 | 并发容器的使用：识别不同场景下最优容器
    并发场景下的 Map 容器
    1。hashMap
        概念：
            1。切忌在并发场景下使用 HashMap
        原因：
            1。JDK1.7 之前，并发场景下使用 HashMap 会出现死循环，从而导致 CPU 使用率居高不下
                原因：扩容是导致死循环的主要原因。
            2。 Java 在 JDK1.8 中在高并发场景下，依然会有数据丢失以及不准确的情况出现。
                注意：JDK1.8 中修复了 HashMap 扩容导致的死循环问题
        方法：保证线程安全
            Hashtable、ConcurrentHashMap 以及 ConcurrentSkipListMap

    2。Hashtable、ConcurrentHashMap：
        出发点：是用一个哈希表来存储商品和销量键值对，然后使用排序获得销量前十的商品
        概念：
            1。基于 HashMap 实现的
            2。对于小数据量的存取比较有优势。
        场景：
            数据不断地写入和删除，且不存在数据量累积以及数据排序的场景下
        Hashtable：
            特点：使用 Synchronized 同步锁修饰了 put、get、remove 等方法
            缺点：
                在高并发场景下，读写操作都会存在大量锁竞争，给系统带来性能开销。
            可以用在强一致性的场景
        ConcurrentHashMap：
            优点：
                保证线程安全的基础上兼具了更好的并发性能
            jdk1。7
                使用了分段锁 Segment 减小了锁粒度，最终优化了锁的并发操作。
            jdk1。8
                Java 重新启用了 Synchronized 同步锁，通过 Synchronized 实现 HashEntry 作为锁粒度
                    优点：这种改动将数据结构变得更加简单了，操作也更加清晰流畅。
            put方法：
                没有哈希冲突时：会使用 CAS 进行添加元素操作
                有哈希冲突时：通过 Synchronized 将链表锁定，再执行接下来的操作。
            缺点：
                1。在强一致的场景中 ConcurrentHashMap 就不适用
                2。该容器在数据量比较大的时候，链表会转换为红黑树。
                3。红黑树在并发情况下，删除和插入过程中有个平衡的过程，会牵涉到大量节点，因此竞争锁资源的代价相对比较高。
            原因：
                1。get、size 等方法没有用到锁，ConcurrentHashMap 是弱一致性的
                2。有可能会导致某次读无法马上获取到写入的数据。

    3。ConcurrentHashMap VS ConcurrentSkipListMap
        出发点：提醒用户手机卡实时流量不足；
            1。主要的流程是服务端先通过虚拟运营商同步用户实时流量，
            2。再通过手机端定时触发查询功能，如果流量不足，就弹出系统通知。
        分析：
            1。该功能的特点是用户量大，并发量高，写入多于查询操作
            2。这时我们就需要设计一个缓存，用来存放这些用户以及对应的流量键值对信息。
        考虑：ConcurrentHashMap 容器和ConcurrentSkipListMap容器
        博客：https://www.cnblogs.com/java-zzl/p/9767255.html
        概念：
            1。基于 TreeMap 的设计原理实现的
                TreeMap：
                    1。基于红黑树实现
                    2。线程非安全的
                    3。来存取大数据

            2。基于跳表实现
            3。针对局部，需要锁住的节点少，因此在并发场景下的性能会更好一些
            4。线程安全的
            没有SkipListMap的原因：
                1。在非线程安全的 Map 容器中，基于红黑树实现的 TreeMap 在单线程中的性能表现得并不比跳跃表差。
        特点：
            存取平均时间复杂度是 O（log（n））
        场景：
            大数据量存取的场景，最常见的是基于跳跃表实现的数据量比较大的缓存。

        什么是跳跃表
            概念：
                1。基于链表扩展实现的一种特殊链表，类似于树的实现
                2。不仅实现了横向链表，还实现了垂直方向的分层索引。
            组层：
                1。由若干层链表组成，每一层都实现了一个有序链表索引
                2。只有最底层包含了所有数据，每一层由下往上依次通过一个指针指向上层相同值的元素
                3。每层数据依次减少，等到了最顶层就只会保留部分数据了。
            原理：
                1。利用了空间换时间的方法来提高了查询效率
                2。程序总是从最顶层开始查询访问，通过判断元素值来缩小查询范围
            例子：
                查询：从最顶层查询，再依次往下
                增加：
                    1。首先新增一个节点到最底层的链表中
                    2。根据概率算出 level 值，
                    3。再根据 level 值新建索引层，最后链接索引层的新节点
                    注意：
                        新增节点和链接索引都是基于 CAS 操作实现。
                删除：
                    1。首先找到待删除结点，将其 value 值设置为 null
                    2。之后再向待删除结点的 next 位置新增一个标记结点，以便减少并发冲突
                    3。然后让待删结点的前驱节点直接越过本身指向的待删结点，直接指向后继结点
                    4。中间要被删除的结点最终将会被 JVM 垃圾回收处理掉；
                    5。最后判断此次删除后是否导致某一索引层没有其它节点了，并视情况删除该层索引 。
        场景：
            Hashtable：如果对数据有强一致要求
            ConcurrentHashMap：在大部分场景通常都是弱一致性的情况下
            ConcurrentSkipListMap：如果数据量在千万级别，且存在大量增删改操作，则可以考虑使用

    并发场景下的 List 容器
        将用户id存入黑名单中：如何实现；
            ArrayList：非线程安全容器，在并发场景下使用很可能会导致线程安全问题
        考虑线程安全的数组：Vector 和 CopyOnWriteArrayList。
            Vector：
                概念：
                    1。基于 Synchronized 同步锁实现的线程安全
                    2。Synchronized 关键字几乎修饰了所有对外暴露的方法
                缺点：
                    在读远大于写的操作场景中，Vector 将会发生大量锁竞争，从而给系统带来性能开销。

            CopyOnWriteArrayList：
                概念：
                    1。是 java.util.concurrent 包提供的方法
                    2。实现了读操作无锁，写操作则通过操作底层数组的新副本来实现，是一种读写分离的并发策略
        分析：
            1。黑名单是一个读远大于写的操作业务
            2。我们可以固定在某一个业务比较空闲的时间点来更新名单。
            3。这种场景对写入数据的实时获取并没有要求
            4。因此我们只需要保证最终能获取到写入数组中的用户 ID 就可以了
        方法：
            CopyOnWriteArrayList 这种并发数组容器无疑是最适合这类场景的了。

18 | 如何设置线程池大小？
    线程池原理
        问题一：
            Java 线程的创建与销毁将会消耗一定的计算机资源，从而增加系统的性能开销。
            原因：
                1。在 HotSpot VM 的线程模型中，Java 线程被一对一映射为内核线程
                2。Java 在使用线程执行程序时，需要创建一个内核线程
                3。当该 Java 线程被终止时，这个内核线程也会被回收

        问题二：
            大量创建线程同样会给系统带来性能问题
            原因：
                1。内存和 CPU 资源都将被线程抢占
                2。如果处理不当，就会发生内存溢出、CPU 使用率超负荷等问题。
        方法：
            线程池
                概念：
                    1。频繁创建线程的业务场景，线程池可以创建固定的线程数量
                    2。并且在操作系统底层，轻量级进程将会把这些线程映射到内核
                作用：
                    提高线程复用，又可以固定最大线程使用量，防止无限制地创建线程
                过程：
                    当程序提交一个任务需要一个线程时，会去线程池中查找是否有空闲的线程
                        1。有，则直接使用线程池中的线程工作
                        2。没有，会去判断当前已创建的线程数量是否超过最大线程数量
                            1.未超过，则创建新线程
                            2.超过，则进行排队等待或者直接抛出异常。

    线程池框架 Executor
        目的：
            更好地实现用户级的线程调度，更有效地帮助开发人员进行多线程开发
        两个核心线程池
            1。ScheduledThreadPoolExecutor：
                概念：用来定时执行任务
            2。ThreadPoolExecutor：
                概念：用来执行被提交的任务
                    不推荐使用Executors 利用工厂模式实现的四种线程池
                        原因：
                            1。需要结合生产环境下的实际场景
                            2。选择使用 Executors 提供的工厂类，将会忽略很多线程池的参数设置
                            3。工厂类一旦选择设置默认参数，就很容易导致无法调优参数设置，从而产生性能问题或者资源浪费。
                    建议你使用 ThreadPoolExecutor 自我定制一套线程池
                    代码：
                    public ThreadPoolExecutor(int corePoolSize,// 线程池的核心线程数量
                                                int maximumPoolSize,// 线程池的最大线程数
                                                long keepAliveTime,// 当线程数大于核心线程数时，多余的空闲线程存活的最长时间
                                                TimeUnit unit,// 时间单位
                                                BlockingQueue<Runnable> workQueue,// 任务队列，用来储存等待执行任务的队列
                                                ThreadFactory threadFactory,// 线程工厂，用来创建线程，一般默认即可
                                                RejectedExecutionHandler handler) // 拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务
                    概念：
                        1。线程池有两个线程数的设置，一个为核心线程数，一个为最大线程数
                        2。在创建完线程池之后，默认情况下，线程池中并没有任何线程，等到有任务来才创建线程去执行任务。
                        特殊情况下：
                            调用 prestartAllCoreThreads() 或者 prestartCoreThread() 方法的话，可以提前创建等于核心线程数的线程数量（预热方式）
                            场景：在抢购系统中
                    流程：
                        1。当创建的线程数等于 corePoolSize 时，提交的任务会被加入到设置的阻塞队列中
                            注意一：当线程池中创建的线程数量超过设置的 corePoolSize，在某些线程处理完任务后
                                1。如果等待 keepAliveTime 时间后仍然没有新的任务分配给它，那么这个线程将会被回收
                                2。线程池回收线程时，会对所谓的“核心线程”和“非核心线程”一视同仁，直到线程池中线程的数量等于设置的 corePoolSize 参数，回收过程才会停止。
                            注意二：
                                背景
                                    1。即使是 corePoolSize 线程，在一些非核心业务的线程池中
                                    2。如果长时间地占用线程数量，也可能会影响到核心业务的线程池，这个时候就需要把没有分配任务的线程回收掉。
                                方法：
                                    1。通过 allowCoreThreadTimeOut 设置项要求线程池
                                    2。将包括“核心线程”在内的，没有任务分配的所有线程，在等待 keepAliveTime 时间后全部回收掉。
                        2。当队列满了，会创建线程执行任务，直到线程池中的数量等于 maximumPoolSize。
                        3。当线程数量已经等于 maximumPoolSize 时， 新提交的任务无法加入到等待队列
                        4。也无法创建非核心线程直接执行，我们又没有为线程池设置拒绝策略
                        5。这时线程池就会抛出 RejectedExecutionException 异常，即线程池拒绝接受这个任务。

    计算线程数量
        背景：
            1。多线程执行的任务类型可以分为 CPU 密集型和 I/O 密集型
            2。根据不同的任务类型，我们计算线程数的方法也不一样。
        CPU 密集型：
            概念：
                消耗的主要是 CPU 资源
            设置：
                将线程数设置为 N（CPU 核心数）+1
            原因：
                1。多出的1指的是为了防止线程偶发的缺页中断
                2。其它原因导致的任务暂停而带来的影响
                3。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。
            例子参考：profit.jikeshijian.xiengnengyouhua.IOOrCPUTypeTest
                4 核 intel i5 CPU 机器上的运行时间变化如下：
                分析：
                    1。当线程数量太小，同一时间大量请求将被阻塞在线程队列中排队等待执行线程
                        缺点：
                            CPU 没有得到充分利用
                    2。当线程数量太大，被创建的执行线程同时在争取 CPU 资源，又会导致大量的上下文切换
                        缺点：
                            从而增加线程的执行时间，影响了整体执行效率

        I/O 密集型：
            概念：
                1。大部分的时间来处理 I/O 交互
                2。线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用
                3。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，\
            设置：
                具体的计算方法是 2N。

            例子参考：profit.jikeshijian.xiengnengyouhua.IOOrCPUTypeTest
            测试条件：
                1。由于测试代码读取 2MB 大小的文件，涉及到大内存
                2。所以在运行之前，我们需要调整 JVM 的堆内存空间：-Xms4g -Xmx4g，避免发生频繁的 FullGC，影响测试结果。
            测试结果：
                线程数量在 8 时，线程平均执行时间是最佳的

        现实的场景：
            比如：通过一个线程池实现向用户定时推送消息的业务，我们又该如何设置线程池的数量呢？
                参考公式：
                    线程数 =N（CPU 核数）*（1+WT（线程等待时间）/ST（线程时间运行时间））
                方法：
                    通过 JDK 自带的工具 VisualVM 来查看 WT/ST 比例
                例子：
                    WT（线程等待时间）= 36788ms [线程运行总时间] - 36788ms[ST（线程时间运行时间）]= 0
                    线程数 =N（CPU 核数）*（1+ 0 [WT（线程等待时间）]/36788ms[ST（线程时间运行时间）]）= N（CPU 核数）

            结果跟CPU 密集型的计算公式 N+1 所得出的结果差不多

            结论：
                1。可以根据自己的业务场景，从“N+1”和“2N”两个公式中选出一个适合的
                2。计算出一个大概的线程数量，之后通过实际压测
                3。逐渐往“增大线程数量”和“减小线程数量”这两个方向调整，然后观察整体的处理时间变化，最终确定一个具体的线程数量。

19 | 如何用协程来优化多线程业务？（toDO）
20 | 磨刀不误砍柴工：欲知JVM调优先了解JVM内存模型
    为什么 JVM 在 Java 中如此重要？
        1。运行一个 Java 应用程序，我们必须要先安装 JDK 或者 JRE 包
            原因：
                1。Java 应用在编译后会变成字节码
                2。然后通过字节码运行在 JVM 中，而 JVM 是 JRE 的核心组成部分。
        2。承担了 Java 字节码的分析（JIT compiler）和执行（Runtime）
        3。内置了自动内存分配管理机制
            作用：
                1。大大降低手动分配回收机制可能带来的内存泄露和内存溢出风险
                2。Java 开发人员不需要关注每个对象的内存分配以及回收，从而更专注于业务本身。
            缺点：
                1。容易使 Java 开发人员过度依赖于自动化，弱化对内存的管理能力，这样系统就很容易发生 JVM 的堆内存异常
                2。垃圾回收（GC）的方式不合适以及 GC 次数过于频繁等问题，这些都将直接影响到应用服务的性能。
    JVM 内存模型的具体设计
        1。堆
            概念：
                1。内存中最大的一块内存空间，该内存被所有线程共享
                2。几乎所有对象和数组都被分配到了堆内存中
                3。被划分为新生代和老年代
                        新生代：
                            Eden
                            Survivor
                                From Survivor
                                To Survivor
                        老年代：
                4。永久代：（方法区）
                    java 6：永久代在非堆内存区
                    java 7：永久代的静态变量和运行时常量池被合并到了堆中
                    Java 8：永久代被元空间取代了

        2。程序计数器
            概念：
                1。是一块很小的内存空间
                2。主要用来记录各个线程执行的字节码的地址
            例子：分支、循环、跳转、异常、线程恢复等都依赖于计数器。
                背景：
                    1。当执行的线程数量超过 CPU 数量时
                    2。线程之间会根据时间片轮询争夺 CPU 资源。
                如：
                    1。如果一个线程的时间片用完了，或者是其它原因导致这个线程的 CPU 资源被提前抢夺
                    2。这个退出的线程就需要单独的一个程序计数器，来记录下一条运行的指令
        3。方法区
            概念：
                1。很多开发者都习惯将方法区称为“永久代”，其实这两者并不是等价
                    原因：
                        HotSpot 虚拟机使用永久代来实现方法区，但在其它虚拟机中，例如，Oracle 的 JRockit、IBM 的 J9 就不存在永久代一说。
                2。方法区只是 JVM 中规范的一部分，可以说，在 HotSpot 虚拟机中，设计人员使用了永久代来实现了 JVM 规范的方法区
                3。方法区与堆空间类似，也是一个共享内存区，所以方法区是线程共享的
                    例子：
                        1。假如两个线程都试图访问方法区中的同一个类信息
                        2。而这个类还没有装入 JVM，那么此时就只允许一个线程去加载它，另一个线程必须等待。
                4。不同版本
                    Java7 版本中：将永久代的静态变量和运行时常量池转移到了堆中，其余部分则存储在 JVM 的非堆内存中
                    Java8 版本中：
                        1。已经将方法区中实现的永久代去掉了，并用元空间（class metadata）代替了之前的永久代，并且元空间的存储位置是本地内存
                        2。之前永久代的类的元数据存储在了元空间，永久代的静态变量（class static variables）以及运行时常量池（runtime constant pool）则跟 Java7 一样，转移到了堆中。
                        元空间替代永久代的原因：
                            1。融合 HotSpot JVM 与 JRockit VM，JRockit 没有永久代，所以不需要配置永久代。
                            2。永久代内存经常不够用或发生内存溢出，爆出异常 java.lang.OutOfMemoryError: PermGen。
            作用：
                用来存放已被虚拟机加载的类相关信息
                    包括：类信息、运行时常量池、字符串常量池
                        类信息：包括了类的版本、字段、方法、接口和父类等信息。
            jvm执行某个类的过程：
                流程：
                    1。加载：
                        概念：
                            1。JVM 会先加载 class 文件
                                class文件：
                                    1。有类的版本、字段、方法和接口等描述信息
                                    2。常量池
                                        作用：用于存放编译期间生成的各种字面量和符号引用
                                            字面量：包括字符串（String a=“b”）、基本类型的常量（final 修饰的变量）
                                            符号引用：类和方法的全限定名，字段的名称和描述符以及方法的名称和描述符。
                                                例子：String的全限定名就是 Java/lang/String
                            2。当类加载到内存中后，JVM 就会将 class 文件常量池中的内容存放到运行时的常量池中
                                运行时常量池：
                                    1。全局共享的
                                    2。多个类共用一个运行时常量池
                                    3。class 文件中常量池多个相同的字符串在运行时常量池只会存在一份。

                    2。链接
                        1。验证
                        2。准备
                        3。解析
                            JVM 会把符号引用替换为直接引用（对象的索引值）。
                    3。初始化
                例子：
                    类中的一个字符串常量在 class 文件中时，存放在 class 文件常量池中的
                    具体过程：
                        1。在 JVM 加载完类之后
                        2。JVM 会将这个字符串常量放到运行时常量池中
                        3。在解析阶段，指定该字符串对象的索引值

        4。虚拟机栈
            概念：
                1。线程私有的内存空间，它和 Java 线程一起创建
                2。当创建一个线程时，会在虚拟机栈中申请一个线程栈
                    线程栈的作用：保存方法的局部变量、操作数栈、动态链接方法和返回地址等信息，并参与方法的调用和返回。
                3。每一个方法的调用都伴随着栈帧的入栈操作，方法的返回则是栈帧的出栈操作。
            作用：
                管理 Java 函数的调用，用java实现的
        5。本地方法栈
            概念：
                1。用于管理本地方法的调用
                2。是由 C 语言实现的。
    JVM 的运行原理
        例子：
            com/suixingpay/profit/jikeshijian/xiengnengyouhua/JVMCase20.java:4
            当我们通过 Java 运行以上代码时，JVM 的整个处理过程如下：
                1。JVM 向操作系统申请内存，
                    1。JVM 第一步就是通过配置参数或者默认配置参数向操作系统申请内存空间
                    2。根据内存大小找到具体的内存分配表
                    3。然后把内存段的起始地址和终止地址分配给 JVM，接下来 JVM 就进行内部分配。
                2。JVM 获得内存空间后，会根据配置参数分配堆、栈以及方法区的内存大小。
                3。class 文件加载、验证、准备以及解析，
                    1。准备阶段：会为类的静态变量分配内存，
                    2。初始化：为系统的初始值
                4。完成上一个步骤后，将会进行最后一个初始化阶段。
                    1。JVM 首先会执行构造器 <clinit> 方法
                    2。编译器会在.java 文件被编译成.class 文件时，收集所有类的初始化代码
                        初始化代码：包括静态变量赋值语句、静态代码块、静态方法，收集在一起成为 <clinit>() 方法
                5。执行方法
                    1。启动 main 线程，执行 main 方法，开始执行第一行代码。
                    2。此时堆内存中会创建一个 student 对象，对象引用 student 就存放在栈中。
                6。过程
                    1。此时再次创建一个 JVMCase 对象，调用 sayHello 非静态方法
                    2。sayHello 方法属于对象 JVMCase，此时 sayHello 方法入栈
                    3。并通过栈中的 student 引用调用堆中的 Student 对象
                    4。之后，调用静态方法 print，print 静态方法属于 JVMCase 类，是从静态方法中获取
                    5。之后放入到栈中，也是通过 student 引用调用堆中的 student 对象。

21 | 深入JVM即时编译器JIT，优化Java编译
    背景：
        前端编译： .java 文件被编译成 .class 文件的过程
        运行时编译：JIT 或解释器会将字节码转换成机器码
        C/C++ 编译器：
            概念：
                1。所有优化都是在编译期间完成的
                2。运行期间的性能监控仅作为基础的优化措施则无法进行
            例如：
                调用频率预测、分支频率预测、裁剪未被选择的分支等
        JIT编译器：
            概念： JVM 中运行时编译最重要的部分之一。
            背景：Java 在运行时的再次编译，就可以进行基础的优化措施
    类编译加载执行过程
        1。类编译
            概念：
                1。将 .java 文件编译成 .class 文件
                    目的：在虚拟机上正常运行代码
                2。文件的编译通常是由 JDK 中自带的 Javac 工具完成
            例子：
                1。一个简单的 .java 文件，我们可以通过 javac 命令来生成 .class 文件
                2。使用 javap 反编译命令行查看class文件包含了那些信息
            过程：
                包括词法分析、填充符号表、注解处理、语义分析以及生成 class 文件
            编译后的字节码文件：
                主要包括：常量池和方法表集合
                    1。常量池：记录的是类文件中出现的字面常量以及符号引用
                            1。字面常量：包括字符串常量和声明为 final 的属性以及一些基本类型的属性
                                1。字符串常量：String str=“abc”，其中"abc"就是常量
                                2。一些基本类型：范围在 -127-128 之间的整型
                            2。符号引用：类和接口的全限定名、类引用、方法引用以及成员变量引用
                                    例子： String str=“abc”，其中 str 就是成员变量引用
                    2。方法表集合：
                        1。一些方法的字节码、方法访问权限（public、protect、prviate 等）
                        2。方法名索引（与常量池中的方法引用对应）、描述符索引、JVM 执行指令以及属性集合等
        2。类加载
            概念：
                1。当一个类被创建实例或者被其它对象引用时，虚拟机在没有加载过该类的情况下
                2。会通过类加载器将字节码文件加载到内存中。
            加载器：(双亲委派机制)
                1。根加载器：JDK 中的本地方法类
                2。扩展加载器：JDK 中内部实现的扩展类
                3。系统加载器：程序中的类文件
                4。自定义加载器
            累加载后：
                class 类文件中的常量池信息以及其它数据会被保存到 JVM 内存的方法区中
        3。类连接
            1。验证：验证类符合 Java 规范和 JVM 规范，在保证符合规范的前提下，避免危害虚拟机安全。
            2。准备：为类的静态变量分配内存，初始化为系统的初始值，对于 final static 修饰的变量，直接赋值为用户的定义值
                    例子：
                        1。private final static int value=123，会在准备阶段分配内存，并初始化值为 123
                        2。如果是 private static int value=123，这个阶段 value 的值仍然为 0
            3。解析：将符号引用转为直接引用的过程
                背景：
                    1。在编译时，Java 类并不知道所引用的类的实际地址，因此只能使用符号引用来代替
                    2。类结构文件的常量池中存储了符号引用，包括类和接口的全限定名、类引用、方法引用以及成员变量引用等
                    3。如果要使用这些类和方法，就需要把它们转化为 JVM 可以直接获取的内存地址或指针，即直接引用。

        4。类初始化
            概念：JVM 首先将执行构造器 <clinit> 方法
                背景：
                    1。编译器会在将 .java 文件编译成 .class 文件时，收集所有类初始化代码
                        类初始化代码：静态变量赋值语句、静态代码块、静态方法，
                        收集在一起成为 <clinit>() 方法。
            执行顺序：
                1。同一个类中
                    Java 源码从上到下的顺序一致
                        例子一：
                            1。初始化类的静态变量和静态代码块为用户自定义的值
                            具体代码：
                                private static int i=1；
                                static{
                                    i=0;
                                }
                                public static void main(String [] args){
                                    System.out.println(i);
                                }
                            执行结果是0；
                2。不同的类（子类和父类）
                        子类初始化时会首先调用父类的 <clinit>() 方法，再执行子类的 <clinit>() 方法，
                                概念：
                                    1。先执行父类的静态变量赋值语句、静态代码块、静态方法
                                    2。再执行子类的静态变量赋值语句、静态代码块、静态方法
                3。实例化一个新对象：
                    1。会调用 <init> 方法对实例变量进行初始化
                    2。并执行对应的构造方法内的代码

            安全性：
                JVM 会保证 <clinit>() 方法的线程安全，保证同一时间只有一个线程执行。

    即时编译
        背景：
            1。初始化完成后，类在调用执行过程中，执行引擎会把字节码转为机器码，然后在操作系统中才能执行
            2。在字节码转换为机器码的过程中，虚拟机中还存在着一道编译，那就是即时编译。
        流程：
            1。虚拟机中的字节码是由解释器（ Interpreter ）完成编译的
                注意：当虚拟机发现某个方法或代码块的运行特别频繁的时候，就会把这些代码认定为“热点代码”。
                    发生的时间：
                            即时编译器对些热点代码编译成本地相关的机器码，并进行各层次的优化，然后保存的内存中
                        目的：提高热点代码的执行效率
    即时编译器类型
        C1 编译器：Client Compiler
            概念：
                1。一个简单快速的编译器，主要的关注点在于局部性的优化
            场景：
                适用于执行时间较短或对启动性能有要求的程序
            例子：
                GUI 应用对界面启动速度就有一定要求。
        C2 编译器：Server Compiler
            概念：
                1。是为长期运行的服务器端应用程序做性能调优的编译器
            场景
                适用于执行时间较长或对峰值性能有要求的程序
        java 7之前：
            1。需要根据程序的特性来选择对应的 JIT
            2。虚拟机默认采用解释器和其中一个编译器配合工作。
        Java7 引入了分层编译：
            概念：
                可以通过参数 “-client”“-server” 强制指定虚拟机的即时编译模式
            优点：
                综合了 C1 的启动性能优势和 C2 的峰值性能优势
            分层编译JVM 的执行状态分为了 5 个层次：
                第0层：程序解释执行，默认开启性能监控功能（Profiling），如果不开启，可触发第二层编译；
                第1层：可称为 C1 编译，将字节码编译为本地代码，进行简单、可靠的优化，不开启Profiling）；
                第2层：也称为 C1 编译，开启 Profiling，仅执行带方法调用次数和循环回边执行次数 profiling 的 C1 编译；
                第3层：也称为 C1 编译，执行所有带 Profiling 的 C1 编译；
                第4层：可称为 C2 编译，也是将字节码编译为本地代码，但是会启用一些编译耗时较长的优化，\
                        甚至会根据性能监控信息进行一些不可靠的激进优化。
            注意：
                1。Java8 中，默认开启分层编译，-client 和 -server 的设置已经是无效的了
                2。如果只想开启 C2，可以关闭分层编译（-XX:-TieredCompilation）
                3。如果只想用 C1，可以在打开分层编译的同时，使用参数：-XX:TieredStopAtLevel=1。
                4。“-Xint”参数：强制虚拟机运行于只有解释器的编译模式下，这时 JIT 完全不介入工作
                5。参数“-Xcomp”：强制虚拟机运行于只有 JIT 的编译模式下。
                6。java -versio查看当前系统使用的编译模式。
    热点探测
        概念：
            1。是 JIT 优化的条件
            2。是基于计数器的热点探测
            3。虚拟机会为每个方法建立计数器统计方法的执行次数，如果执行次数超过一定的阈值就认为它是“热点方法” 。
        虚拟机为每个方法准备了两类计数器，
            背景：
                两个计数器都有一个确定的阈值，计数器超过阈值溢出了，就会触发 JIT 编译。
            类型：
                方法调用计数器：用于统计方法被调用的次数
                    默认阈值：
                        在C1模式下：1500次，-XX: CompileThreshold 来设定
                        在C2模式下：10000次，-XX: CompileThreshold 来设定
                        在分层模式下：根据当前待编译的方法数以及编译线程数来动态调整。-XX: CompileThreshold 指定的阈值将失效
                回边计数器：用于统计一个方法中循环体代码执行的次数
                        回边：字节码中遇到控制流向后跳转的指令
                        默认阈值：
                            在不开启分层编译的情况下
                                C1模式下：13995，-XX: OnStackReplacePercentage=N 来设置
                                C2模式下：10700，-XX: OnStackReplacePercentage=N 来设置
                            在分层编译的情况下
                                根据当前待编译的方法数以及编译线程数来动态调整。-XX: OnStackReplacePercentage 指定的阈值同样会失效
                        目的：
                            触发 OSR（On StackReplacement）编译，即栈上编译。
                        具体实现：
                            1。在一些循环周期比较长的代码段中，当循环达到回边计数器阈值时，JVM 会认为这段是热点代码
                            2。JIT 编译器就会将这段代码编译成机器语言并缓存
                            3。在该循环时间段内，会直接将执行代码替换，执行缓存的机器语言。

    编译优化技术
        1。方法内联
            背景：
                1。调用一个方法通常要经历压栈和出栈
                2。调用方法是将程序执行顺序转移到存储该方法的内存地址
                3。将方法的内容执行完后，再返回到执行该方法前的位置。
                    缺点：
                        1。方法调用会产生一定的时间和空间方面的开销。
                        2。对于那些方法体代码不是很大，又频繁调用的方法来说，这个时间和空间的消耗会很大
                    原因：
                        1。要求在执行前保护现场并记忆执行的地址，执行后要恢复现场
                        2。并按原来保存的地址继续执行
                    方法：方法内联

            概念：
                就是把目标方法的代码复制到发起调用的方法之中，避免发生真实的方法调用。
            参数：
                -XX:CompileThreshold：设置热点方法的阈值；
                注意：
                    热点方法不一定会被 JVM 做内联优化
                        1。如果这个方法体太大了，JVM 将不执行内联操作
                            方法体的大小值可以通过设置来优化：
                                1。经常执行的方法：
                                    1。默认情况下，方法体大小小于 325 字节的都会进行内联
                                    2。-XX:MaxFreqInlineSize=N 来设置大小值；
                                2。不是经常执行的方法：
                                    1。默认情况下，方法大小小于 35 字节才会进行内联
                                    2。我们也可以通过 -XX:MaxInlineSize=N 来重置大小值。
            查看内联情况：
                设置 VM 参数：-XX:+PrintCompilation -XX:+UnlockDiagnosticVMOptions -XX:+PrintInlining 之后
                -XX:+PrintCompilation // 在控制台打印编译过程信息
                -XX:+UnlockDiagnosticVMOptions // 解锁对 JVM 进行诊断的选项参数。默认是关闭的，开启后支持一些特定参数对 JVM 进行诊断
                -XX:+PrintInlining // 将内联方法打印出来
            提高方法内联的方式：
                1。设置 JVM 参数来减小热点阈值或增加方法体阈值，以便更多的方法可以进行内联
                    缺点：需要占用更多地内存
                2。在编程中，避免在一个方法中写大量代码，习惯使用小方法体；
                3。尽量使用 final、private、static 关键字修饰方法，编码方法因为继承，会需要额外的类型检查。
        2。逃逸分析（分析技术）
            概念：
                1。判断一个对象是否被外部方法引用或外部线程访问的分析技术
                2。编译器会根据逃逸分析的结果对代码进行优化
            1。栈上分配
                背景：
                    1。Java 中默认创建一个对象是在堆中分配内存的
                    2。而当堆内存中的对象不再使用时，则需要通过垃圾回收机制回收
                        缺点：
                            这个过程相对分配在栈中的对象的创建和销毁来说，更消耗时间和性能
                        方法：栈上分配
                概念：
                    逃逸分析如果发现一个对象只在方法中使用，就会将对象分配在栈上。
                    例子参考代码：profit.jikeshijian.xiengnengyouhua.EscapeAnalysis21
                注意：
                    HotSpot 中暂时没有实现这项优化
                        原因：
                            HotSpot 虚拟机目前的实现导致栈上分配实现比较复杂
            2。锁消除
            非线程安全的情况：
                尽量不要使用线程安全容器，比如：StringBuffer
                原因：
                    StringBuffer 中的 append 方法被 Synchronized 关键字修饰，会使用到锁，从而导致性能下降。
                    但是：StringBuffer 和 StringBuilder 的性能基本没什么区别：
                        原因：
                            1。在局部方法中创建的对象只能被当前线程访问，无法被其它线程访问
                            2。这个变量的读写肯定不会有竞争，这个时候 JIT 编译会对这个对象的方法锁进行锁消除。
            3。标量替换
                概念：
                    1。逃逸分析证明一个对象不会被外部访问
                    2。如果这个对象可以被拆分的话，当程序真正执行的时候可能不创建这个对象，而直接创建它的成员变量来代替
                    3。将对象拆分后，可以分配对象的成员变量在栈或寄存器上，原本的对象就无需分配内存空间了
                通过设置 JVM 参数来开关逃逸分析：还可以单独开关同步消除和标量替换
                    -XX:+DoEscapeAnalysis 开启逃逸分析（jdk1.8 默认开启，其它版本未测试）
                    -XX:-DoEscapeAnalysis 关闭逃逸分析

                    -XX:+EliminateLocks 开启锁消除（jdk1.8 默认开启，其它版本未测试）
                    -XX:-EliminateLocks 关闭锁消除

                    -XX:+EliminateAllocations 开启标量替换（jdk1.8 默认开启，其它版本未测试）
                    -XX:-EliminateAllocations 关闭就可以了

22 | 如何优化垃圾回收机制？
    背景：
        1。开发人员是无需过度关注对象的回收与释放的，JVM 的垃圾回收机制可以减轻不少工作量
        2。但完全交由 JVM 回收对象，也会增加回收性能的不确定性
        3。在一些特殊的业务场景下，不合适的垃圾回收算法以及策略，都有可能导致系统性能下降。
    例子：
        1。在对内存要求苛刻的情况下，需要提高对象的回收效率；
        2。在 CPU 使用率高的情况下，需要降低高并发时垃圾回收的频率
    垃圾回收机制
        三个问题：
            1。回收发生在哪里？
                背景：
                    1。程序计数器，虚拟机栈，本地方法栈这3个区域内存分配和回收都具有确定性。
                        原因：
                            1。随着线程的创建而创建，销毁而销毁
                            2。栈中的栈帧随着方法的进入和退出进行入栈和出栈操作
                            3。每个栈帧中分配多少内存基本是在类结构确定下来的时候就已知的
                概念：
                    堆：主要是对象的回收
                    方法区：主要是废弃常量和无用的类的回收
            2。对象在什么时候可以被回收？
                问题一：
                    什么对象可以回收？
                    答案：
                        一般一个对象不再被引用，就代表该对象可以被回收。
                回收的算法：
                    1。引用计数算法
                        概念：
                            通过一个对象的引用计数器来判断该对象是否被引用了
                        原理：
                            1。每当对象被引用，引用计数器就会加 1
                            2。每当引用失效，计数器就会减 1
                            3。当对象的引用计数器的值为 0 时，就说明该对象不再被引用，可以被回收了
                        优点：
                            引用计数算法的实现简单，判断效率也很高
                        缺点：
                            存在着对象之间相互循环引用的问题，不能回收
                    2。可达性分析算法
                        背景：
                            目前 HotSpot 虚拟机采用的就是这种算法
                        概念：
                            1。GC Roots 是该算法的基础，GC Roots 是所有对象的根对象
                        原理：
                            1。在 JVM 加载时，会创建一些普通对象引用正常对象
                            2。这些对象作为正常对象的起始点，在垃圾回收时，会从这些 GC Roots 开始向下搜索
                            3。当一个对象到 GC Roots 没有任何引用链相连时，就证明此对象是不可用的
                注意：
                    以上两种算法都是通过引用来判断对象是否可以被回收（JDK1.2进行了扩充）
                    引用分类：
                        1。强引用：
                            特点：关联的对象永远不会被垃圾收集器回收掉
                        2。软引用：
                            特点：关联的对象，只有当系统要发生内存溢出时，才会回收软引用引用的对象；
                        3。弱饮用：
                            特点：关联的对象，只要发生垃圾收集事件，就会被回收
                        4。虚引用：
                            特点：关联的对象的唯一作用是能在这个对象被回收器回收时收到一个系统的通知
            3。如何回收这些对象？
                JVM 垃圾回收遵循以下两个特性。
                    自动性：
                        概念：
                            1。Java 提供了一个系统级的线程来跟踪每一块分配出去的内存空间
                            2。当 JVM 处于空闲循环时，垃圾收集器线程会自动检查每一块分配出去的内存空间
                            3。然后自动回收每一块空闲的内存块。
                    不可预期性：
                        问题：
                            一旦一个对象没有被引用了，该对象是否立刻被回收呢？
                        答案：
                            是不可预期的
                        原因：
                            垃圾回收线程在 JVM 中是自动执行的，Java 程序无法强制执行
                        概念：
                            1。我们唯一能做的就是通过调用 System.gc 方法来"建议"执行垃圾收集器
                            2。但是否可执行，什么时候执行？仍然不可预期。
    GC 算法
        1。垃圾收集器的回收算法可以分为以下几种：
            1。标记-清除算法
                优点：
                    不需要移动对象，简单高效
                缺点：
                    标记-清除过程效率低，GC产生内存碎片
            2。复制算法
                优点：
                    简单高效，不会产生内存碎片
                缺点：
                    内存使用率低，且有可能产生频繁复制问题
            3。标记-整理算法
                优点：
                    综合了前两种算法的优点
                缺点：
                    仍需要移动局部对象
            4。分代收集算法(Generational Collection)
                优点：
                    分区回收
                缺点：
                    对于长时间存活对象的场景回收效果不明显，甚至起到反作用
        2。服务端垃圾收集器：内存回收的具体实现
            1。垃圾回收器类型：
                1。Serial New/Serial Old回收器：
                    概念：复制算法/标记-清除算法
                    优点：
                        1。单线程复制回收，简单高效
                    缺点：
                        但会暂停程序导致停顿
                    参数设置：
                        -XX:+UseSerialGC:年轻代，老年代回收器为Serial New，Serial Old
                2。ParNew New(并行)/Parnew Old
                    概念：复制算法/标记-整理算法
                    优点：
                        多线程回收，降低了停顿时间
                    缺点：
                        但容易增加上下文切换
                    参数设置：
                        -XX:+UseParNewGC:年轻代，老年代回收器为ParNew New，ParNew Old，Jdk1.8无效
                        -XX:+UseParallelOldGC:年轻代，老年代回收器为Parallel Scavenge，ParNew Old
                3.Parallel Scavenge回收器
                    概念：复制算法
                    优点：
                        并行回收器，追求高吞吐量，高效利用CPU
                    设置参数：
                        -XX:+UseParallelGC：年轻代，老年代回收器为:Parallel Scavenge,Serial Old
                        -XX:ParallelGCThreads=4:设置并发线程数
                4。CMS回收器
                    概念：标记-清理算法
                    优点：
                        老年代回收器，高并发，低停顿，追求最短GC回收停顿时间，
                        CPU占用比较高，响应时间快，停顿时间短
                    设置参数：
                        -XX:+UseConcMarkSweepGC:年轻代，老年代回收器为：ParNew New,CMS(Serial Old作为备用)
                5。G1回收器
                    概念：标记-整理+复制算法
                    优点：
                        高并发，低停顿，可预测停顿时间
                    设置参数：
                        -XX:+UseG1GC:年轻代，老年代回收器为：G1，G1
                        -XX:MaxGCPauseMillis=200:设置最大暂停时间

            2。查询的垃圾收集器类型
                1。通过 ps 命令查询出经常 ID 
                2。 jmap -heap ID：显示JVM配置信息，包括垃圾收集器的设置类型。

    GC 性能衡量指标
        吞吐量：
            概念：
                应用程序所花费的时间和系统总运行时间的比值
            公式：
                系统总运行时间 = 应用程序耗时 +GC 耗时
            例子：
                1。如果系统运行了 100 分钟，GC 耗时 1 分钟
                2。则系统吞吐量为 99%。GC 的吞吐量一般不能低于 95%。
        停顿时间：
            背景：
                串行回收器：停顿时间可能会比较长；
                并发回收器：程序的停顿时间就会变短，但其效率很可能不如独占垃圾收集器，系统的吞吐量也很可能会降低
                    原因：垃圾收集器和应用程序交替运行
            概念：
                指垃圾收集器正在运行时，应用程序的暂停时间
        垃圾回收频率：
            概念：
                多久发生一次指垃圾回收呢？
            特点：
                越低越好
            方法：
                增大堆内存空间
                缺点：
                    堆积的回收对象越多，最终也会增加回收时的停顿时间
            注意：
                增大堆内存空间，保证正常的垃圾回收频率即可。

    查看 & 分析 GC 日志
        通过JVM 参数预先设置 GC 日志
            -XX:+PrintGC 输出 GC 日志
            -XX:+PrintGCDetails 输出 GC 的详细日志
            -XX:+PrintGCTimeStamps 输出 GC 的时间戳（以基准时间的形式）
            -XX:+PrintGCDateStamps 输出 GC 的时间戳（以日期的形式，如 2013-05-04T21:53:59.234+0800）
            -XX:+PrintHeapAtGC 在进行 GC 的前后打印出堆的信息
            -Xloggc:../logs/gc.log 日志文件的输出路径
        查看Gc日志的工具：
            离线看：
               通过工具GCViewer： https://sourceforge.net/projects/gcviewer/https://sourceforge.net/projects/gcviewer/
            在线看，上传日志文件
                GCeasy：https://www.gceasy.io/index.jsp

    GC 调优策略
        1。降低 Minor GC 频率
            问题：
                由于新生代空间较小，Eden 区很快被填满，就会导致频繁 Minor GC
            方法：
                增大新生代空间(堆内短期存活对象多的情况)
                    缺点：可能不会增加单次 Minor GC 的时间
                        单次 Minor GC 的时间：
                            组成：T1（扫描新生代）和 T2（复制存活对象）
                    例子：
                        扩大新生代前：
                            1。假设一个对象在 Eden 区的存活时间为 500ms
                            2。Minor GC 的时间间隔是 300ms
                            3。那么正常情况下，Minor GC 的时间为 ：T1+T2。
                        扩大新生代后：
                            1。Minor GC 的时间间隔可能会扩大到 600ms，
                            2。此时一个存活 500ms 的对象(短期存活对象)就会在 Eden 区中被回收掉，此时就不存在复制存活对象了
                            3。所以再发生 Minor GC 的时间为：两次扫描新生代，即 2T1。
                    不同的场景：
                       背景：复制对象的成本要远高于扫描成本。
                        场景一：堆内存中存在较多的长期存活的对象，扩容新生代
                                问题：反而会增加 Minor GC 的时间
                                原因：存活的对象多，复制的成本高
                        场景二：堆中的短期对象很多，扩容新生代
                                单次 Minor GC 时间不会显著增加
                                原因：存活的对象少，复制的成本第
                        两个场景结论：
                        单次 Minor GC 时间更多取决于 GC 后存活对象的数量，而非 Eden 区的大小


        2。降低 Full GC 的频率
            Full GC的触发条件：
                堆内存空间不足或老年代对象太多
            问题：
                频繁的 Full GC 会带来上下文切换，增加系统的性能开销
            方法：
                1。减少创建大对象：
                    概念：
                        平常的业务场景中，我们习惯一次性从数据库中查询出一个大对象用于 web 端显示
                    例子：
                        1。我之前碰到过一个一次性查询出 60 个字段的业务操作
                        2。这种大对象如果超过年轻代最大对象阈值，会被直接创建在老年代
                        3。即使被创建在了年轻代，由于年轻代的内存空间有限，通过 Minor GC 之后也会进入到老年代。
                    方法：
                        1。可以将这种大对象拆解出来，首次只查询一些比较重要的字段
                        2。如果还需要其它字段辅助查看，再通过第二次查询显示剩余的字段。
                2。增大堆内存空间：
                    概念：
                        1。在堆内存不足的情况下，增大堆内存空间
                        2。且设置初始化堆内存为最大堆内存，也可以降低 Full GC 的频率。
    选择合适的 GC 回收器
        响应快的需求：
            选择响应速度较快的 GC 回收器，CMS（Concurrent Mark Sweep）回收器和 G1 回收器都是不错的选择。
        吞吐量大的要求：
            就可以选择 Parallel Scavenge 回收器来提高系统的吞吐量。

23 | 如何优化JVM内存分配？
    JVM 内存分配性能问题
        背景：
            代码编程问题：
                应用程序创建对象导致的内存回收对象难，线上的 JVM 内存溢出事故
            平常：
                JVM 内存分配不合理带来的性能表现并不会像内存溢出问题这么突出，不深入，很难发现问题；
        概念：
            JVM 内存分配不合理最直接的表现就是频繁的 GC
        缺点：
            导致上下文切换等性能问题，从而降低系统的吞吐量、增加系统的响应时间
        方法：
            调整 JVM 内存分配了，从而减少 GC 所带来的性能开销。

    对象在堆中的生存周期
        新建一个对象时
            一般情况
                1：对象会被优先分配到新生代的 Eden 区中
                2。这时虚拟机会给对象定义一个对象年龄计数器
                    （通过参数 -XX:MaxTenuringThreshold 设置）
            另一种情况：
                1。当 Eden 空间不足时，虚拟机将会执行一个新生代的垃圾回收（Minor GC）
                2。这时 JVM 会把存活的对象转移到 Survivor 中，并给对象的年龄 +1
                3。对象在 Survivor 中同样也会经历 MinorGC，每经过一次 MinorGC，对象的年龄将会 +1
        内存空间也是有设置阀值：
            -XX:PetenureSizeThreshold ：设置直接被分配到老年代的最大对象
                概念：
                    1。这时如果分配的对象超过了设置的阀值，对象就会直接被分配到老年代
                优点：可以减少新生代的垃圾回收。
    查看 JVM 堆内存分配
        背景：
            默认不配置 JVM 堆内存大小的情况下，JVM 根据默认值来配置当前内存大小
        查看堆内存配置的默认值：
            java -XX:+PrintFlagsFinal -version | grep HeapSize 
            ps -ef|grep java
            jmap -heap 17284
        显示情况：
            1。这台机器上启动的 JVM 默认最大堆内存为 1953MB
            2。初始化大小为 124MB。（NewSize+OldSize）
            JDK1.7：
                –XX:NewRatio ：默认1:2，表示年轻代和老年代的比例
                -XX:SurvivorRatio：8:1:1，表示 Eden 和 To Survivor、From Survivor 的比例
                -XX:+UseAdaptiveSizePolicy：开启
                    1。JVM 将会动态调整 Java 堆中各个区域的大小以及进入老年代的年龄
                    2。–XX:NewRatio 和 -XX:SurvivorRatio 将会失效，
            JDK1.8
                -XX:+UseAdaptiveSizePolicy ：默认开启
                    注意：不要随便关闭这个配置
                        原因：
                            1。JVM 将会分配最小堆内存，年轻代和老年代按照默认比例 1:2 进行分配
                            2。年轻代中的 Eden 和 Survivor 则按照默认比例 8:2 进行分配
                            3。这个内存分配未必是应用服务的最佳配置，因此可能会给应用服务带来严重的性能问题。
                        除非：
                            你已经对初始化堆内存 / 最大堆内存、年轻代 / 老年代以及 Eden 区 /Survivor 区有非常明确的规划了

    JVM 内存分配的调优过程
        案例：
            1。模拟一个抢购接口，假设需要满足一个 5W 的并发请求，且每次请求会产生 20KB 对象
            2。我们可以通过千级并发创建一个 1MB 对象的接口来模拟万级并发请求产生大量对象的场景，
            具体代码：
                @RequestMapping(value = "/test1")
                public String test1(HttpServletRequest request) {
                    List<Byte[]> temp = new ArrayList<Byte[]>();
                    Byte[] b = new Byte[1024*1024];
                    temp.add(b);
                    return "success";
                }
            AB 压测
                分别对应用服务进行压力测试，通过Jmeter测试：
                    显示：
                        并发数量到了一定值时，吞吐量就上不去了，响应时间也迅速增加
            分析GC日志
                1。设置参数，将运行期间的GC日志dump下来
                    -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -Xloggc:/log/heapTest.log
                    参数说明：
                        -XX:PrintGCTimeStamps：打印 GC 具体时间；
                        -XX:PrintGCDetails ：打印出 GC 详细日志；
                        -Xloggc: path：GC 日志生成路径。
                2。收集到GC日志后，通过GCViewer 工具打开它，进而查看到具体的 GC 日志如下：
                    页面显示：
                        1。FullGC 发生了 13 次
                        2。右下角显示年轻代和老年代的内存使用率几乎达到了 100%
                        3。而 FullGC 会导致 stop-the-world 的发生，从而严重影响到应用服务的性能
                    方法：
                        需要调整堆内存的大小来减少 FullGC 的发生
            参考指标
                GC 频率：
                    问题：
                        1。高频的 FullGC 会给系统带来非常大的性能消耗
                        2。虽然 MinorGC 相对 FullGC 来说好了许多，但过多的 MinorGC 仍会给系统带来压力。
                内存：
                    概念：
                        这里指堆内存大小，堆内存又分为年轻代内存和老年代内存
                    问题：
                        1。分析年轻代和老年代的比例是否合适
                        2。如果内存不足或分配不均匀，会增加 FullGC
                        3。严重的将导致 CPU 持续爆满，影响系统性能
                吞吐量：
                    问题：
                        1。频繁的 FullGC 将会引起线程的上下文切换，增加系统的性能开销，
                        2。从而影响每次处理的线程请求，最终导致系统的吞吐量下降。
                延时：
                    问题
                        JVM 的 GC 持续时间也会影响到每次请求的响应时间
            具体调优方法
                调整堆内存空间减少 FullGC：
                    过程：
                        1。通过日志分析，堆内存基本被用完了
                        2。而且存在大量 FullGC，这意味着我们的堆内存严重不足，这个时候我们需要调大堆内存空间。
                    命令：
                        java -jar -Xms4g -Xmx4g heapTest-0.0.1-SNAPSHOT.jar
                        参数：
                            -Xms：堆初始大小；
                            -Xmx：堆最大值。
                    调整之后：
                        1。调大堆内存之后，我们再来测试下性能情况
                        2。发现吞吐量提高了 40% 左右，响应时间也降低了将近 50%。
                        3。再查看 GC 日志，发现 FullGC 频率降低了，老年代的使用率只有 16% 了。

                调整年轻代减少 MinorGC：
                    命令：
                    java -jar -Xms4g -Xmx4g -Xmn3g heapTest-0.0.1-SNAPSHOT.jar
                验证：
                    1。再进行 AB 压测，发现吞吐量上去了。
                    2。再查看 GC 日志，发现 MinorGC 也明显降低了，GC 花费的总时间也减少了。
            设置 Eden、Survivor 区比例：
                背景：
                    1。在 JDK1.8 中，默认是开启 AdaptiveSizePolicy 的，我们可以通过 -XX:-UseAdaptiveSizePolicy 关闭该项配置
                    2。显示运行 -XX:SurvivorRatio=8 将 Eden、Survivor 的比例设置为 8:2
                概念：
                    大部分新对象都是在 Eden 区创建的，我们可以固定 Eden 区的占用比例，来调优 JVM 的内存分配性能。
                验证：
                    1。再进行 AB 性能测试，我们可以看到吞吐量提升了，响应时间降低了


24 | 内存持续上升，我该如何排查问题？
    常用的监控和诊断内存工具
        1。Linux 命令行工具之 top 命令
            top：
                概念：
                    实时显示正在执行进程的 CPU 使用率、内存使用率以及系统负载等信息
                        上半部分：系统的统计信息
                        下半部分：进程的使用率统计信息。
            top -Hp pid：
                查看具体线程使用系统资源情况

        2。Linux 命令行工具之 vmstat 命令
            vmstat：
                1。指定采样周期和次数的功能性监测工具
                2。不仅可以统计内存的使用情况
                3。还可以观测到 CPU 的使用率、swap 的使用情况
                4。vmstat 一般很少用来查看内存的使用情况，而是经常被用来观察进程的上下文切换。
                    r：等待运行的进程数；
                    b：处于非中断睡眠状态的进程数；
                    swpd：虚拟内存使用情况；
                    free：空闲的内存；
                    buff：用来作为缓冲的内存数；
                    si：从磁盘交换到内存的交换页数量；
                    so：从内存交换到磁盘的交换页数量；
                    bi：发送到块设备的块数；
                    bo：从块设备接收到的块数；
                    in：每秒中断数；
                    cs：每秒上下文切换次数；
                    us：用户 CPU 使用时间；
                    sy：内核 CPU 系统使用时间；
                    id：空闲时间；
                    wa：等待 I/O 时间；
                    st：运行虚拟机窃取的时间。

        3。Linux 命令行工具之 pidstat 命令
            pidstat
                1。是 Sysstat 中的一个组件，也是一款功能强大的性能监测工具
                2。我们可以通过命令：yum install sysstat 安装该监控组件
                3。之前的 top 和 vmstat 两个命令都是监测进程的内存、CPU 以及 I/O 使用情况，而 pidstat 命令则是深入到线程级别。
                常用的参数：
                    -u：默认的参数，显示各个进程的 cpu 使用情况；
                    -r：显示各个进程的内存使用情况；
                    -d：显示各个进程的 I/O 使用情况；
                    -w：显示每个进程的上下文切换情况；
                    -p：指定进程号；
                    -t：显示进程中线程的统计信息。

            例子：
                1。jps
                2。pidstat -p 28557 -r 1 3
                    说明： pidstat 的参数 -p 用于指定进程 ID，-r 表示监控内存的使用情况，1 表示每秒的意思，3 则表示采样次数。
                    显示结果的关键指标
                        Minflt/s：任务每秒发生的次要错误，不需要从磁盘中加载页；
                        Majflt/s：任务每秒发生的主要错误，需要从磁盘中加载页；
                        VSZ：虚拟地址大小，虚拟内存使用 KB；
                        RSS：常驻集合大小，非交换区内存使用 KB。
                3。pidstat -p 28557 -r 1 3 -t
                    -t继续查看该进程下的线程内存使用率
        4。JDK 工具之 jstat 命令
            概念：
                可以监测 Java 应用程序的实时运行情况，包括堆内存信息以及垃圾回收信息
            命令：
                jstat -help：查看一些关键参数信息
                jstat -option：查看 jstat 有哪些操作
                        -class：显示 ClassLoad 的相关信息；
                        -compiler：显示 JIT 编译的相关信息；
                        -gc：显示和 gc 相关的堆信息；
                        -gccapacity：显示各个代的容量以及使用情况；
                        -gcmetacapacity：显示 Metaspace 的大小；
                        -gcnew：显示新生代信息；
                        -gcnewcapacity：显示新生代大小和使用情况；
                        -gcold：显示老年代和永久代的信息；
                        -gcoldcapacity ：显示老年代的大小；
                        -gcutil：显示垃圾收集信息；
                        -gccause：显示垃圾回收的相关信息（通 -gcutil），同时显示最后一次或当前正在发生的垃圾回收的诱因；
                        -printcompilation：输出 JIT 编译的方法信息。
            例子：
                jstat -gc pid：使用 jstat 查看堆内存的使用情况
                显示结果：
                    S0C：年轻代中 To Survivor 的容量（单位 KB）；
                    S1C：年轻代中 From Survivor 的容量（单位 KB）；
                    S0U：年轻代中 To Survivor 目前已使用空间（单位 KB）；
                    S1U：年轻代中 From Survivor 目前已使用空间（单位 KB）；
                    EC：年轻代中 Eden 的容量（单位 KB）；
                    EU：年轻代中 Eden 目前已使用空间（单位 KB）；
                    OC：Old 代的容量（单位 KB）；
                    OU：Old 代目前已使用空间（单位 KB）；
                    MC：Metaspace 的容量（单位 KB）；
                    MU：Metaspace 目前已使用空间（单位 KB）；
                    YGC：从应用程序启动到采样时年轻代中 gc 次数；
                    YGCT：从应用程序启动到采样时年轻代中 gc 所用时间 (s)；
                    FGC：从应用程序启动到采样时 old 代（全 gc）gc 次数；
                    FGCT：从应用程序启动到采样时 old 代（全 gc）gc 所用时间 (s)；
                    GCT：从应用程序启动到采样时 gc 用的总时间 (s)。

        5。JDK 工具之 jstack 命令
            jstack
                概念：
                    是一种线程堆栈分析工具
                使用：
                    1。最常用的功能就是使用 jstack pid 命令查看线程的堆栈信息
                    2。通常会结合 top -Hp pid 或 pidstat -p pid -t 一起查看具体线程的状态
                    3。也经常用来排查一些死锁的异常。
            背景：
                每个线程堆栈的信息中，都可以查看到线程 ID、线程的状态（wait、sleep、running 等状态）以及是否持有锁等。

        6。JDK 工具之 jmap 命令
            jmap
                1。查看堆内存初始化配置信息以及堆内存的使用情况
                2。输出堆内存中的对象信息，包括产生了哪些对象，对象数量多少等。
            例子：
                1。jmap -heap 28558
                    查看堆内存初始化配置信息以及堆内存的使用情况
                2。jmap -histo:live 28557|more
                    查看堆内存中的对象数目、大小统计直方图，如果带上 live 则只统计活对象
                3.jmap -dump:format=b,file=/tmp/heap.hprof 28557
                    把堆内存的使用情况 dump 到文件中
                    分析：降文件下载下来，是用mat工具文件进行分析；
    实战操作：
        内存溢出
            原因：
                1。大峰值下没有限流，瞬间创建大量对象
                    方法：使用限流
                2。内存泄漏
                    程序bug引起的，及时找到问题代码
        该项目已经运行在腾讯云上了，通过jmeter模拟发送
            java -jar -Xms256m -Xmx512m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/heapdump.hprof -Xms256m -Xmx512m -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -Xloggc:/tmp/heapTest.log springboot-mybatis-0.0.1-SNAPSHOT.jar 
            第一步：top查看
            第二步：ps -ef|grep java
            第二步：top -Hp 32038
            第三步：jstack 32038 查看堆栈信息
            第四步：jmap -heap 32038 查看Eden，from，to，永久代使用情况（老年代基本使用完了）最重要的一步
            第五步：jstat 32038 查看存活对象的数量
            通过以上堆内存的情况，我们基本可以判断系统发生了内存泄漏。
                接下来是确定哪个对象占用了内存：
                    1。下载dump文件
                    2。然后通过mat工具打开
                    3。可以查看到对象数量排序
                    4。我们可以看到 Byte[] 数组排在了第一位，选中对象后右击选择 with incomming reference 功能，可以查看到具体哪个对象引用了这个对象。

25 | 答疑课堂：模块四热点问题解答（需要多看）
26 | 单例模式：如何创建单一对象优化系统性能？
27 | 原型模式与享元模式：提升系统性能的利器
28 | 如何使用设计模式优化并发编程？
29 | 生产者消费者模式：电商库存设计优化
30 | 装饰器模式：如何优化电商系统中复杂的商品价格策略？
31 | 答疑课堂：模块五思考题集锦
32 | MySQL调优之SQL语句：如何写出高性能SQL语句？
    慢 SQL 语句的几种常见诱因
        1。无索引、索引失效导致慢查询
            例如：
                大数据表以一个索引的列作为查询条件，大部分情况会非常耗时
            方法：
                建立合适的索引来优化查询
        2。锁等待
            InnoDB：支持行锁和表锁
            MyISAM：只支持表锁
                缺点：
                    1。基于表锁的数据库操作，会导致 SQL 阻塞等待，从而影响执行速度
                    2。在一些更新操作（insert\update\delete）大于或等于读操作的情况下
                方法：不建议用MyISAM
            行锁：基于索引加的锁，如果我们在更新操作时，条件索引失效，那么行锁也会升级为表锁。
                缺点：可能会带来死锁
            锁升级：
                1。MySQL 认为如果对一张表使用大量行锁，会导致事务执行效率下降
                2。从而可能造成其它事务长时间锁等待和更多的锁冲突问题发生，致使性能严重下降
                3。MySQL 会将行锁升级为表锁
        3。不恰当的 SQL 语句
            例子：
                1。<SELECT *>，<SELECT COUNT(*)> SQL 语句
                2。在大数据表中使用 <LIMIT M,N> 分页查询
                3。以及对非索引字段进行排序等等。
    优化 SQL 语句的步骤
        背景：
            问题：
                1。SQL 先后查询了哪些表
                2。是否使用了索引
                3。这些数据从哪里获取到
                4。获取到数据遍历了多少行数据等等
            方法：
                通过 EXPLAIN 命令来查看这些执行信息
        1。通过 EXPLAIN 分析 SQL 执行计划
            1。id：每个执行计划都有一个 id，如果是一个联合查询，这里还将有多个 id。
            2。select_type：表示 SELECT 查询类型
                  SIMPLE：普通查询，即没有联合查询、子查询
                  PRIMARY：主查询
                  UNION：UNION 中后面的查询
                  SUBQUERY：子查询等。
            3。table：当前执行计划查询的表，如果给表起别名了，则显示别名信息
            4。partitions：访问的分区表信息。
            5。type：表示从表中查询到行所执行的方式，查询方式是 SQL 优化中一个很重要的指标
                结果值从好到差依次是：system > const > eq_ref > ref > range > index > ALL。
                system/const：
                    概念：
                        1。表中只有一行数据匹配，此时根据索引查询一次就能找到对应的数据
                        2。如果是 B + 树索引，我们知道此时索引构造成了多个层级的树，当查询的索引在树的底层时，查询效率就越低。
                        3。const 表示此时索引在第一层，只需访问一层便能得到数据。
                    例子：
                        explain select *from order wehre id=2;
                eq_ref：
                    概念：
                        使用唯一索引扫描，常见于多表连接中使用主键和唯一索引作为关联条件。
                    例子：
                        explain select *from order a,order_detail b  where a.id=b.order_id;
                ref：
                    概念：
                        非唯一索引扫描，还可见于唯一索引最左原则匹配扫描。
                    例子：
                        explain select *from order wehre order_no=2;

                range：
                    概念：
                        索引范围扫描，比如，<，>，between 等操作。
                    例子：
                        explain select *from order wehre id>2;
                index：
                    概念：
                        索引全表扫描，此时遍历整个索引树。
                    例子
                        explain select ID from order ;

                ALL：
                    概念：
                        表示全表扫描，需要遍历全表来找到对应的行。
                    例子：
                        explain select *from order wehre pay_money=2;
                6.possible_keys：可能使用到的索引。
                7.key：实际使用到的索引。
                8.key_len：当前使用的索引的长度。
                9.ref：关联 id 等信息。
                10.rows：查找到记录所扫描的行数。
                11.filtered：查找到所需记录占总扫描记录数的比例。
                12.Extra：额外的信息。

        2。通过 Show Profile 分析 SQL 执行性能
            背景：
                问题：
                    通过 EXPLAIN 分析执行计划，仅仅是停留在分析 SQL 的外部的执行情况
                    想要深入到内核，执行线程的状态和时间来分析的话，
                方法：
                    profile
            概念：
                1。Profile 除了可以分析执行线程的状态和时间
                2。还支持进一步选择 ALL、CPU、MEMORY、BLOCK IO、CONTEXT SWITCHES 等类型来查询 SQL 语句在不同系统资源上所消耗的时间
                type 参数：
                | ALL：显示所有开销信息
                | BLOCK IO：阻塞的输入输出次数
                | CONTEXT SWITCHES：上下文切换相关开销信息
                | CPU：显示 CPU 的相关开销信息 
                | IPC：接收和发送消息的相关开销信息
                | MEMORY ：显示内存相关的开销，目前无用
                | PAGE FAULTS ：显示页面错误相关开销信息
                | SOURCE ：列出相应操作对应的函数名及其在源码中的调用位置 (行数) 
                | SWAPS：显示 swap 交换次数的相关开销信息
            注意：Show Profile ：5.0.37才支持的
                验证是否支持： select @@have_profiling，显示yes代表支持
                验证是否开启：SELECT @@profiling;
            命令：
                Show Profiles：
                    概念：
                        只显示最近发给服务器的 SQL 语句，默认情况下是记录最近已执行的 15 条记录
                          重新设置 profiling_history_size 增大该存储记录，最大值为 100。
                    显示：query_id
                Show Profile for Query ID 
                        就能够查看到对应 Query_ID 的 SQL 语句在执行过程中线程的每个状态所消耗的时间了：
        3。常用的 SQL 优化
            1。优化分页查询
                分页模式：
                    <LIMIT M,N> + 合适的 order by 来实现分页查询，
                        没有索引：
                            缺点：
                                性能将会非常得糟糕
                            原因：
                                需要做大量的文件排序操作（file sort）
                        有索引：
                            优点：
                                刚开始的分页查询效率会比较理想
                            缺点：
                                但越往后，分页查询的性能就越差。
                            原因：
                                使用 LIMIT 的时候，偏移量 M 在分页越靠后的时候，值就越大，数据库检索的数据也就越多
                                例子：
                                    LIMIT 10000,10 这样的查询
                                    分析：
                                        1。数据库需要查询 10010 条记录，最后返回 10 条记录
                                        2。也就是说将会有 10000 条记录被查询出来没有被使用到。
                案例：
                    前提：order表有10万条数据
                    例子
                      explain  select * from `demo`.`order` order by order_no limit 10000, 20;
                        耗时0.018s，扫描行数10020行
                        原因：
                            查询获取的 10020 行数据结果都返回给我们了
                        思路：
                            1。能否先查询出所需要的 20 行数据中的最小 ID 值
                            2。然后通过偏移量返回所需要的 20 行数据给我们呢？
                        方法：
                            我们可以通过索引覆盖扫描，使用子查询的方式来实现分页查询：
                            具体sql
                                explain select * from `demo`.`order` where id> (select id from `demo`.`order` order by order_no limit 10000, 1)  limit 20;
                                耗时：
                                    虽然主查询扫描了更多的行数，但执行时间却减少了，只有 0.004s
                                原因：
                                    返回行数只有 20 行了

            2。优化 SELECT COUNT(*)
                问题：
                    为什么不带where条件SELECT COUNT(*)，MyISAM 的查询速度要明显快于 InnoDB。
                        原因：
                            MyISAM：
                                原理：
                                    存储引擎记录的是整个表的行数
                                优点：
                                    无需遍历表计算，直接获取该值即可
                            InnoDB：
                                原理：
                                    扫描表来统计具体的行数
                    当带上where条件之后，需要扫描表来进行行数的统计。
                    如果对于一张大表肯定是不明智的；
                方法：
                    1。使用近似值（不需要精确值）
                        有些业务场景并不需要得到一个精确值，
                        方法：
                            使用 EXPLAIN 对表进行估算
                        原因：
                            执行 EXPLAIN 并不会真正去执行查询，而是返回一个估算的近似值。
                    2。增加汇总统计（需要精确值）
                        方法：
                            1。新增一个汇总统计表
                            2。缓存字段来统计需要的 COUNT 值
                        缺点：
                            新增和删除时有一定的成本，但却可以大大提升 COUNT() 的性能。
            3。优化 SELECT *
                概念：
                    聚族索引：
                        概念：
                            在存储数据时，索引是基于 B + 树构成的，具体的行数据则存储在叶子节点。
                        例如：
                            InnoDB的主键索引
                    非聚族索引：
                        概念：
                            在存储数据时，索引是基于 B + 树构成的，而叶子节点存储的是主键值。
                        例如：
                            MyISAM 默认创建的主键索引、二级索引以及 InnoDB 的二级索引
                案例：
                    前提： (order_no、status)为组合索引 
                        select * from order where order_no='xxx’
                            执行过程：
                                1。先会查询组合索引
                                2。通过组合索引获取到主键 ID
                                3。再通过主键 ID 去主键索引中获取对应行所有列的值。
                        select order_no, status from order where order_no='xxx’
                            执行过程：
                                只会查询组合索引
33 | MySQL调优之事务：高并发场景下的数据库事务调优
    事务：
        概念：
            数据库系统执行过程中的一个逻辑处理单元
        作用：
            保证一个数据库操作要么成功，要么失败
        特性：
            原子性（Atomicity）
            一致性（Consistent）
            隔离性（Isolation）
            持久性（Durable）
    背景：
       1。MyISAM 存储引擎不支持事务，InnoDB支持
            问题一：
                并发变成带来了线程安全的问题；
            问题二：
                并发事务，也存在这安全问题；
                例子：
                    修改数据丢失，读取数据不一致等。
                方法：
                    事务隔离
    并发事务带来的问题
        1。数据丢失
                事务A：
                    第一步：开启事务：
                    第二步：账户余额为0，为该账户新增1000元（与B事务同时提交）
                    第三步：提交失败，回滚（在B事务后提交）
                事务B：
                    第一步：开启事务
                    第二步：账户余额为0，该账户新增2000元（与A事务同时提交）
                    第三步：提交后，账户余额为2000元，（先提交成功）
                问题：导致B事务提交数据覆盖掉，造成数据丢失了
                方法一：
                    基于数据库中的悲观锁来避免发生，保证在该事务结束之前其他事务无法更新该数据。
                    如：select xx for update 语句来实现一个排他锁
                方法二：
                    基于乐观锁来避免
                    原理：
                        1。即将某一字段作为版本号
                        2。如果更新时的版本号跟之前的版本一致，则更新，否则更新失败
        2。脏读
                事务A：
                    第一步：开启事务：
                    第二步：账户余额为1000，为该账户新增1000元（与B事务同时提交）
                    第三步：提交失败，回滚，余额为1000元

                事务B：
                    第一步：开启事务
                    第二步：查询余额，余额为1000元（在事务A更新同时查询）
                    第三步：再次查询，账户余额为2000元，（与A更新事务后查询）
                问题：事务A更新失败回滚，导致事务B读取的数据是脏数据
                方法：数据库提供一定的事务隔离机制来解决
                原因：数据库读一致性造成的

        3。不可重复读
                事务A：
                    第一步：开启事务：
                    第二步：账户余额为1000，为该账户新增1000元（与B事务同时提交）
                    第三步：提交成功

                事务B：
                    第一步：开启事务
                    第二步：查询余额，余额为1000元（在事务A更新同时查询）
                    第三步：再次查询，账户余额为2000元，（与A提交成功后查询）
                问题：由于事务B第一次读取账户为1000元，而后一次则变为2000元
                方法：数据库提供一定的事务隔离机制来解决
                原因：数据库读一致性造成的
        4。幻读
                事务A：
                    第一步：开启事务：
                    第二步：查询10个账户，余额为0（与B事务同时提交）
                    第三步：新增一个账户，余额为0，提交成功

                事务B：
                    第一步：开启事务
                    第二步：查询10个账户，账户余额为0，修改账户金额为1000元（在事务A更新同时查询）
                    第三步：再次查询余额账户，发现有一个账户的余额仍然为0，（与A提交成功后查询）
                问题：由于事务A新增一个账户，并提交，而事务B此时正在更新所有账户余1000元
                    更新完成后，再次查看数据库数据，发现还有一个账户的余额为0
                方法：数据库提供一定的事务隔离机制来解决
                原因：数据库读一致性造成的

    事务隔离解决并发问题
        锁机制：
            1。共享锁（S）:
                概念：
                    1。允许一个事务读数据，不允许修改数据
                    2。如果其他事务要再对该行加锁，只能加共享锁
            2。排他锁（X）:
                概念：
                    1。修改数据时加的锁，可以读取和修改数据
                    2。一旦一个事务对该行数据加锁，其他事务将不能再对该数据加任务锁。
        隔离级别：
            背景：
                1。不同的锁机制会产生以下几种不同的事务隔离级别
                2。不同的隔离级别分别可以解决并发事务产生的几个问题
            1。未提交读(Read Uncommitted）
                    实现原理：
                        在事务 A 读取数据时，事务 B 读取和修改数据加了共享锁。
                    问题：
                        会导致脏读、不可重复读以及幻读。
                    解决了
                        数据丢失的问题的问题
            2。已提交读（Read Committed）
                    实现原理：
                        1。在事务 A 读取数据时增加了共享锁，一旦读取，立即释放锁
                        2。事务 B 读取修改数据时增加了行级排他锁，直到事务结束才释放锁
                    现象：
                        1。事务 A 在读取数据时，事务 B 只能读取数据，不能修改
                        2。当事务 A 读取到数据后，事务 B 才能修改。
                    优点：
                        可以避免脏读
                    问题：
                        存在不可重复读以及幻读的问题。
            3。可重复读（Repeatable Read）
                    实现原理：
                        1。在事务 A 读取数据时增加了共享锁，事务结束，才释放锁
                        2。事务 B 读取修改数据时增加了行级排他锁，直到事务结束才释放锁。
                    现象：
                        1。事务 A 在没有结束事务时，事务 B 只能读取数据，不能修改
                        2。当事务 A 结束事务，事务 B 才能修改。
                    优点：
                        可以避免脏读、不可重复读
                    问题：
                        依然存在幻读的问题
            4。可序列化（Serializable）
                    实现原理：
                        1。在事务 A 读取数据时增加了共享锁，事务结束，才释放锁
                        2。事务 B 读取修改数据时增加了表级排他锁，直到事务结束才释放锁
                    优点：
                        解决了脏读、不可重复读、幻读等问题
                    缺点：
                        并发性会越来越低
            注意：
                InnoDB 中的 RC 和 RR 隔离事务是基于多版本并发控制（MVVC）实现高性能事务
                  用MVCC原因：
                        1。一旦数据被加上排他锁，其他事务将无法加入共享锁
                        2。且处于阻塞等待状态，如果一张表有大量的请求，这样的性能将是无法支持的。
                MVVC的：
                    原理：
                        1。对普通的 Select 不加锁，如果读取的数据正在执行 Delete 或 Update 操作（更新操作在查询之后）
                        2。这时读取操作不会等待排它锁的释放，而是直接利用 MVVC 读取该行的数据快照
                            数据快照：该行的之前版本的数据
                                数据快照的版本：是基于 undo 实现的
                                    undo ：是用来做事务回滚的，记录了回滚的不同版本的行记录
                    优点：
                        避免了对数据重复加锁的过程，大大提高了读操作的性能。
    锁具体实现算法
        背景：
            InnoDB 既实现了行锁，也实现了表锁
            行锁：通过索引实现的
                具体实现算法：
                    1。record lock：是专门对索引项加锁
                    2。gap lock：是对索引项之间的间隙加锁比如(1,2)
                    3。next-key lock：则是前面两种的组合，对索引项以其之间的间隙加锁。比如(1,2]
                注意：
                    1。在可重复读或以上隔离级别下的特定操作才会取得 gap lock 或 next-key lock
                    2.在 Select 、Update 和 Delete 时，除了基于唯一索引的查询之外
                    3.其他索引查询时都会获取 gap lock 或 next-key lock，即锁住其扫描的范围。
                具体参考：mysql日记里面的21讲加锁规则：两个原则，两个优化，和一个bug
            表锁：如果不通过索引条件检索数据，那么 InnoDB 将对表中所有的记录进行加锁，升级为表锁了

    优化高并发事务
        1。结合业务场景，使用低级别事务隔离
            场景一：
                1。在修改用户最后登录时间的业务场景中，这里对查询用户的登录时间没有特别严格的准确性要求
                    原因：
                        1。而修改用户登录信息只有用户自己登录时才会修改
                        2。不存在一个事务提交的信息被覆盖的可能。
            场景二：
                1。如果是账户中的余额或积分的消费，就存在多个客户端同时消费一个账户的情况
                2。此时我们应该选择 RR 级别来保证一旦有一个客户端在对账户进行消费
                3。其他客户端就不可能对该账户同时进行消费了。

        2。避免行锁升级表锁
            背景：
                1。在 InnoDB 中，行锁是通过索引实现的
                2。如果不通过索引条件检索数据，行锁将会升级到表锁
            缺点：
                表锁是会严重影响到整张表的操作性能的

        3。控制事务的大小，减少锁定的资源量和锁定时间长度
            案例一：
                异常：
                    MySQLQueryInterruptedException: Query execution was interrupted
                    具体流程
                        1。抢购提交订单中开启了事务
                        2。在高并发时对一条记录进行更新的情况下，由于更新记录所在的事务还可能存在其他操作，导致一个事务比较长，
                        3。当有大量请求进入时，就可能导致一些请求同时进入到事务中。
                        分析：
                            1。因为锁的竞争是不公平的，当多个事务同时对一条记录进行更新时
                            2。极端情况下，一个更新操作进去排队系统后，可能会一直拿不到锁
                            3。最后因超时被系统打断踢出
            案例二：
                1。在用户购买商品时，首先我们需要查询库存余额
                2。再新建一个订单，并扣除相应的库存
                注意：
                    1。这一系列操作是处于同一个事务的。
                    2。在两种不同的执行顺序下，其结果都是一样的，事务性能方面却不一样
                    执行顺序1：
                        1。开启事务
                        2。查询库存，判断库存是否满足
                        3。新建订单
                        4。扣除库存
                        5。提交或回滚
                    执行顺序2：
                        1。开启事务
                        2。查询库存，判断库存是否满足
                        3。扣除库存
                        4。新建订单
                        5。提交或回滚

                分析：
                    1。虽然这些操作在同一个事务，但锁的申请在不同时间
                    2。只有当其他操作都执行完，才会释放所有锁
                    3。因为扣除库存是更新操作，属于行锁，这将会影响到其他操作该数据的事务
                    4。所以我们应该尽量避免长时间地持有该锁，尽快释放该锁。
                    5。又因为先新建订单和先扣除库存都不会影响业务
                    6。所以我们可以将扣除库存操作放到最后
                结论：
                    使用执行顺序 1，以此尽量减小锁的持有时间

34 | MySQL调优之索引：索引的失效与优化
    背景：
        MyIsam主键索引：叶子结点存储的是行指针
        Innodb辅助索引：叶子结点存储的是主键值
            优点：
                当行发生移动或者数据分裂时，不用再维护索引的变更
        聚簇索引和非聚簇索引区别参考31讲
        例子一：
            select * from merchandise where id=7;
            分析：
                1。使用主键索引查询商品
                2。则会按照 B+ 树的索引找到对应的叶子节点
                3。直接获取到行数据
        例子二：
            select * from merchandise where mecrchant_no=7;
            分析：
                1。使用商品编码查询商品，即使用辅助索引进行查询
                2。会先检索辅助索引中的 B+ 树的 serial_no，找到对应的叶子节点，获取主键值
                3。然后再通过聚族索引中的 B+ 树检索到对应的叶子节点
                4。然后获取整行数据。这个过程叫做回表。

    1。覆盖索引优化查询
        需求：
            只需要查询商品的名称、价格信息
        问题：
            我们有什么方式来避免回表呢？
        思路：
            可以建立一个组合索引，即商品编码、名称、价格作为一个组合索引
        原因：
            如果索引中存在这些数据，查询将不会再次检索主键索引，从而避免回表。
        覆盖索引：
            概念：
                从辅助索引中查询得到记录，而不需要通过聚族索引查询获得
            优点：
                减少大量的 I/O 操作
                    原因：
                        不需要查询出包含整行记录的所有信息
            例子：
                SELECT COUNT(*) 
                思路：
                    1。如果不存在辅助索引，此时会通过查询聚族索引来统计行数
                    2。如果此时正好存在一个辅助索引，则会通过查询辅助索引来统计行数，减少 I/O 操作。

    2。自增字段作主键优化查询
        背景：
            1。InnoDB 创建主键索引默认为聚族索引，数据被存放在了 B+ 树的叶子节点上
            2。同一个叶子节点内的各个数据是按主键顺序存放的
            3。每当有一条新的数据插入时，数据库会根据主键将其插入到对应的叶子节点中。
        自增主键
            优点：
                不需要重新移动数据，因此这种插入数据的方法效率非常高。
            原因：
                1。使用自增主键，那么每次插入的新数据就会按顺序添加到当前索引节点的位置
                2。不需要移动已有的数据，当页面写满，就会自动开辟一个新页面
        非自增主键：
            插入过程：
                1。由于每次插入主键的索引值都是随机的
                2。因此每次插入新的数据时，就可能会插入到现有数据页中间的某个位置
                3。这将不得不移动其它数据来满足新数据的插入，甚至需要从一个页面复制数据到另外一个页面（页分裂）
            缺点：
                页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率。
        建议：
            在使用 InnoDB 存储引擎时，如果没有特别的业务需求，建议使用自增字段作为主键。

    3。前缀索引优化
        背景：
            1。索引文件是存储在磁盘中的，而磁盘中最小分配单元是页，通常一个页的默认大小为 16KB，
            2。假设我们建立的索引的每个索引值大小为 2KB，则在一个页中，我们能记录 8 个索引值
            3。假设我们有 8000 行记录，则需要 1000 个页来存储索引
        问题：
            如果我们使用该索引查询数据，可能需要遍历大量页，这显然会降低查询效率。
        方法：
            在一些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小。
            原因：
                减小索引字段大小，可以增加一个页中存储的索引项，有效提高索引的查询速度

        局限性：
            order by 就无法使用前缀索引，无法把前缀索引用作覆盖索引。
        参考：
            前缀索引的选取也要考虑：区分度；参考mysql的笔记

    4。防止索引失效
        例子一：
            Memory 引擎：hash索引：
                如果使用范围查询，会失效，一般，Hash 索引只有在“=”的查询条件下，索引才会生效
        例子二：
            如果是以 % 开头的 LIKE 查询将无法利用节点查询数据
        例子三：
            概念：
                最左匹配原则：
                    在使用复合索引时，需要使用索引中的最左边的列进行查询，才能使用到复合索引
            流程：
                1。在 order 表中建立一个复合索引 idx_user_order_status(order_no, status, user_id)；
                2。如果我们使用 order_no、order_no+status、order_no+status+user_id 以及 order_no+user_id 组合查询，则能利用到索引
                3。而如果我们用 status、status+user_id 查询，将无法使用到索引
        例子四：
            如果查询条件中使用 or，且 or 的前后条件中有一个列没有索引，那么涉及的索引都不会被使用到。

35 | 记一次线上SQL死锁事故：如何避免死锁？
    背景：
        1。MYSQL 数据库和 Oracle 提交事务不太一样，MySQL 数据库默认情况下是自动提交事务
        查看自动提交事务是否开启：
            show variables like 'autocommit';
        设置关闭：
            set autocommit=0;

    案例一：
        1.订单在做幂等性校验时，先是通过订单号检查订单是否存在
        2.如果不存在则新增订单记录
        模拟：
            事务A：
                begin;
                第一步：
                    select id from order_record where order_no=4 for update;//检查是否存在order_no等于4的订单
                第三步：
                    insert into order_record(orderr_no,status,create_date) values(4,1,'2019-07-13 10:57:03');
                    此时：锁等待中
                第五步：
                    commit;
            事务B：
                    begin
                第二步：
                    select id from order_record where order_no=5 for update //检查是否存在order_no=5的订单
                第四步：
                    insert into order_record(orderr_no,status,create_date) values(5,1,'2019-07-13 10:57:03');
                    此时：锁等待中
                第五步：
                    commit;
        结果：
            两个事务已经进入死锁状态。
        查询死锁情况：
            information_schema：数据库显示死锁情况
            过程：
                use information_schema;
                select * from innodb_locks;
        问题：
            为什么 SELECT 要加 for update 排他锁，而不是使用共享锁呢？
                原因：
                    1。如果是两个订单号一样的请求同时进来，就有可能出现幻读。
                    2。一开始事务 A 中的查询没有该订单号，后来事务 B 新增了一个该订单号的记录
                    3。此时事务 A 再新增一条该订单号记录，就会创建重复的订单记录
                作用：
                    使用锁间隙算法来防止幻读
        分析：
            1。当我们执行以下查询 SQL 时，由于 order_no 列为非唯一索引
            2。此时又是 RR 事务隔离级别，所以 SELECT 的加锁类型为 gap lock，这里的 gap 范围是 (4,+∞）
                SELECT id FROM demo.order_record where order_no = 4 for update;
            3。执行查询 SQL 语句获取的 gap lock 并不会导致阻塞
            4。而当我们执行以下插入 SQL 时，会在插入间隙上再次获取插入意向锁。
            5。插入意向锁其实也是一种 gap 锁，它与 gap lock 是冲突的
            6。所以当其它事务持有该间隙的 gap lock 时，需要等待其它事务释放 gap lock 之后，才能获取到插入意向锁。
        推论：
            1。以上事务 A 和事务 B 都持有间隙 (4,+∞）的 gap 锁
            2。下来的插入操作为了获取到插入意向锁，都在等待对方事务的 gap 锁释放，于是就造成了循环等待，导致死锁。
                INSERT INTO demo.order_record(order_no, status, create_date) VALUES (5, 1, ‘2019-07-13 10:57:03’);
        方法：
            方法一：
                1。在两个事务相互等待时，当一个事务的等待时间超过设置的某一阈值
                2。就对这个事务进行回滚，另一个事务就可以继续执行了。
                具体实现：
                    innodb_lock_wait_timeout参数来设置超时时间；
            方法二：
                将 order_no 列设置为唯一索引列，虽然不能防止幻读，但我们可以利用它的唯一性来保证订单记录不重复创建
                原因：
                    锁住的行锁
                缺点：
                    当遇到重复创建订单时会抛出异常。
            方法三：
                用 Redis 以及 ZooKeeper 来实现
                优点：
                    运行效率比数据库更佳

    其它常见的 SQL 死锁问题
        背景：
            死锁的四个条件：
                1。互斥
                2。占有且等待
                3。不可强占用
                4。循环等待
            只要系统发生死锁，这些条件必然成立
        案例：
            条件
                InnoDB 存储引擎的主键索引为聚簇索引，其它索引为辅助索引
            过程：
                1。如果使用辅助索引来更新数据库，就需要使用聚簇索引来更新数据库字段
                2。如果两个更新事务使用了不同的辅助索引，或一个使用了辅助索引
                3。一个使用了聚簇索引，就都有可能导致锁资源的循环等待。
            具体过程
                事务A
                    Begin
                    update order_record set status =1 where order_no=4;
                事务B
                    begin;
                    update order_record set status =1 where id =4;
            出现死锁的步骤：
                事务A
                    第一步：
                        首先获取idx_order_status非聚簇索引
                    第三步：
                        根据非聚簇索引的主键，获取主键索引的行锁
                事务B
                    第二步
                        获取主键索引的行锁
                    第四步
                        更新status 列时，需要获取idx_order_status非聚簇索引
        建议：
            更新操作时，我们应该尽量使用主键来更新表字段，这样可以有效避免一些不必要的死锁发生。

36 | 什么时候需要分表分库？
    背景：
        1。对于一个日活用户在百万数量级的商城来说，每天产生的订单数量可能在百万级
        2。特别在一些活动促销期间，甚至上千万。
        3。假设我们基于单表来实现，每天产生上百万的数据量，不到一个月的时间就要承受上亿的数据，这时单表的性能将会严重下降。
        4。因为 MySQL 在 InnoDB 存储引擎下创建的索引都是基于 B+ 树实现的，所以查询时的 I/O 次数很大程度取决于树的高度
        5。随着 B+ 树的树高增高，I/O 次数增加，查询性能也就越差。
    方法：
        分区、NoSQL 存储、分表分库等
    分区：
        概念：
            1。也是基于分表的原理实现的，即有多个底层表实现
            2。但分区依然是在单库下进行的，在一些需要提高并发的场景中的优化空间非常有限
            3。且一个表最多只能支持 1024 个分区。面对日益增长的海量数据，优化存储能力有限
        场景：
            1。在一些非海量数据的大表中，我们可以考虑使用分区来优化表性能。
            2。如果数据量比较大，且热点数据比较集中、历史数据很少访问，表分区
    NoSQL：
        概念：
            基于键值对存储
        优点：
            查询性能非常高
        缺点
            不是关系型数据库，不支持事务以及稳定性方面相对 RDBMS 差一些
            注意：有些 NoSQL 数据库也实现了事务，宣传具有可靠的稳定性，目前 NoSQL 还是主要用作辅助存储
    分表分库
    1。什么时候要分表分库？
        前提：能不分表分库就不要分表分库
            问题一：
                单表的情况下，当业务正常时，我们使用单表即可，而当业务出现了性能瓶颈时
            思路一：
                考虑用分区的方式来优化
                问题二：
                    如果分区优化之后仍然存在后遗症
                思路
                    考虑分表分库
        案例：
            1。如果在单表单库的情况下，当数据库表的数据量逐渐累积到一定的数量时（5000W 行或 100G 以上）
            2。操作数据库的性能会出现明显下降，即使我们使用索引优化或读写库分离，性能依然存在瓶颈
            方法：
                如果每日数据增长量非常大，我们就应该考虑分表
            作用：
                避免单表数据量过大，造成数据库操作性能下降。
        场景：大数据量且高并发
            单表单库的缺点：
                1。性能比较差
                2。数据库连接数、磁盘 I/O 以及网络吞吐等资源都是有限的
                3。并发能力也是有限的
            方法：
                分表分库
                优点
                    提升数据库的并发处理能力，从而提升应用的整体性能。
    2。如何分表分库？
        1。垂直分库
            概念：
                是指根据业务来分库，不同的业务使用不同的数据库。
            例子：
                订单和消费券在抢购业务中都存在着高并发，如果同时使用一个库，会占用一定的连接数
                方法：
                    将数据库分为订单库和促销活动库。
            原理：
                1。而垂直分表则是指根据一张表中的字段，将一张表划分为两张表
                2。规则就是将一些不经常使用的字段拆分到另一张表中
                例子：
                    一张订单详情表有一百多个字段，显然这张表的字段太多了
                缺点：
                    1。不方便我们开发维护
                    2。可能引起跨页问题
                方法：
                    拆分该表字段
        2。水平分表
            原理：
                1。将表中的某一列作为切分的条件
                2。按照某种规则（Range 或 Hash 取模）来切分为更小的表。
            概念：
                1。只是在一个库中，如果存在连接数、I/O 读写以及网络吞吐等瓶颈
                2。我们就需要考虑将水平切换的表分布到不同机器的库中
        3。数据库划分：
            前提：基于垂直切分和水平切分
            单库单表：
                场景：
                    平时的业务开发中，我们应该优先考虑单库单表
            单库多表：
                场景：
                    如果访问热点数据分散，基本上所有的数据都会访问到，我们可以考虑单库多表
            多库多表：
                场景：
                    如果并发量比较高、海量数据以及每日新增数据量巨大
        分表的问题：
            涉及到多表的分页查询、多表的 JOIN 查询，从而增加业务的复杂度
        分库的问题：
            跨库分页查询、跨库 JOIN 查询，还会存在跨库事务的问题，增加系统的开发的复杂度

    3。分表分库之后面临的问题
        案例：
            1。假设我们有一张订单表以及一张订单详情表，每天的数据增长量在 60W 单
            2。平时还会有一些促销类活动，订单增长量在千万单
        思路一：
            考虑将订单表和订单详情表做分库分表
        思路二：
            因为用户一般查询的是最近的订单信息，所以热点数据比较集中，我们还可以考虑用表分区来优化单表查询。
            目的：
                提高系统的并发能力
        具体实现方式：
            方式一：基于订单号 Hash 取模实现
                优点：
                    订单号 Hash 取模的好处是数据能均匀分布到各个表中
                缺点：
                    则是一个用户查询所有订单时，需要去多个表中查询。
            方式二：根据用户 ID Hash 取模实现
                前提：
                    订单表用户查询比较多，对订单表进行水平分表
                优点：
                    提高高并发时的订单处理能力
        1。分布式事务问题
            案例：
                提交订单时，除了创建订单之外，我们还需要扣除相应的库存。
                订单表和库存表由于垂直分库，不在一个库中
            问题：
                带来事务的不一致性
            方法：
                分布式事务
            具体实现：
                1。两阶事务提交（2PC
                2。补偿事务提交（TCC）
                3。 Spring 实现的 JTA
            4。目前阿里开源的分布式事务中间件 Fescar

        2。跨节点 JOIN 查询问题
            案例：
                用户在查询订单时，我们往往需要通过表连接获取到商品信息，
                而商品信息表可能在另外一个库中，这就涉及到了跨库 JOIN 查询。
            方法一：
                我们会冗余表或冗余字段来优化跨库 JOIN 查询
                冗余表例子：
                    例如商品信息表，我们可以在每一个订单分库中复制一张基础表，
                    优点：
                        避免跨库 JOIN 查询
                冗余字段例子
                    而对于一两个字段的查询，我们也可以将少量字段冗余在表中
                    优点：
                        从而避免 JOIN 查询，也就避免了跨库 JOIN 查询

        3。跨节点分页查询问题
            案例：
                1。当用户在订单列表中查询所有订单时，可以通过用户 ID 的 Hash 值来快速查询到订单信息
                2。而运营人员在后台对订单表进行查询时，则是通过订单付款时间来进行查询的
            问题：
                这些数据都分布在不同的库以及表中，此时就存在一个跨节点分页查询的问题了。
            方法一：中间件
                原理：
                    1。通过在每个表中先查询出一定的数据
                    2。然后在缓存中排序后，获取到对应的分页数据
                缺点：
                    越往后面的查询，就越消耗性能。
            方法二：使用两套数据
                具体过程：
                    1。一套是基于分库分表的用户单条或多条查询数据
                    2。一套则是基于 Elasticsearch、Solr 存储的订单数据，主要用于运营人员根据其它字段进行分页查询。
                    3。为了不影响提交订单的业务性能，我们一般使用异步消息来实现 Elasticsearch、Solr 订单数据的新增和修改。
        4。全局主键 ID 问题
            问题：
                在分库分表后，主键将无法使用自增长来实现了
            思路：
                单独设计全局主键，避免不同表和库中的主键重复问题
            方法一：uuid
                使用 UUID 实现全局 ID 
                具体过程：
                    随机生成一个 32 位 16 进制数字
                优点：
                    可以保证一个 UUID 的唯一性，水平扩展能力以及性能都比较高
                缺点：
                    它是一个比较长的字符串，连续性差，如果作为主键使用，性能相对来说会比较差。
            方法二：redis
                基于 Redis 分布式锁实现一个递增的主键 ID
                优点：
                    保证主键是一个整数且有一定的连续性
                缺点：
                    分布式锁存在一定的性能消耗。
            方法三：
                基于 Twitter 开源的分布式 ID 生产算法，snowflake 解决全局主键 ID 问题，
                原理：
                    是通过分别截取时间、机器标识、顺序计数的位数组成一个 long 类型的主键 ID
                优点：
                    可以满足每秒上万个全局 ID 生成，不仅性能好，而且低延时
        5。扩容问题
            案例：
                随着用户的订单量增加，根据用户 ID Hash 取模的分表中，数据量也在逐渐累积。
            问题：
                需要考虑动态增加表，一旦动态增加表了，就会涉及到数据迁移问题
            思路：
            1。在最开始设计表数据量时，尽量使用 2 的倍数来设置表数量
            2。当我们需要扩容时，也同样按照 2 的倍数来扩容，这种方式可以减少数据的迁移量。

37 | 电商系统表设计优化案例分析
    背景：
        问题一：
            表结构没有设计好，那么后期随着业务以及数据量的增多，系统就很容易出现瓶颈。
        问题二：
            表结构扩展性差，业务耦合度将会越来越高，系统的复杂度也将随之增加
        思考点：
            设计表时，都需要考虑哪些因素，又是如何通过表设计来优化系统性能。
    核心业务
        案例：电商系统为例
            背景：
                电商系统一般分为平台型和自营型电商系统
                    平台型电商系统：
                        1。指有第三方商家入驻的电商平台
                        2。第三方商家自己开设店铺来维护商品信息、库存信息、促销活动、客服售后等
                        例子：淘宝、天猫
                    自营型电商系统：
                        1。是指没有第三方商家入驻，而是公司自己运营的电商平台
                        例子：京东自营、苹果商城
                    区别：
                        平台型电商系统的复杂度要远远高于自营型电商系统
    下面的案例以苹果商城为例，讨论表结构设计
    核心业务：肯定就是销售商品了，围绕销售商品
        1。商品模块
            包括
                商品分类：
                比如：
                    手机
                    电视
                    配件
                        耳机
                        充电宝
                商品信息管理：
            分类查询之后，就到了商品页面：
                一个商品item包含了若干商品SKU。
                商品item是指一种商品，例如IPhone9；
                商品SKU是指商品属性的商品，例如金色128G内存的iphone9
        2。购物车模块
            作用：
                用于用户临时存放欲购买的商品，并可以在购物车中统一下单结算
            分类：
                离线购物车：用户选择放入到购物车的商品只保存在本地缓存中
                    例子：
                        1。当用户没有登录商城时，主要是离线购物车在记录用户的商品信息
                        2。当用户登录商城之后，用户放入到购物车中的商品都会同步到服务端
                        3。以后在手机和电脑等不同平台以及不同时间都能查看到自己放入购物车的商品。

                在线购物车：是会同步这些商品信息到服务端
        3。订单模块
            作用：
                订单是盘活整个商城的核心功能模块，如果没有订单的产出，平台将难以维持下去
            概念：
                1。管理着用户在平台的交易记录，是用户和商家交流购买商品状态的渠道
                2。用户可以随时更改一个订单的状态，商家则必须按照业务流程及时订单的更新状态
                3。告知用户已购买商品的具体状态。
            状态：
                通常一个订单分为以下几个状态：
                待付款、待发货、待收货、待评价、交易完成、用户取消、仅退款、退货退款状态。
            流程：
                第一步：提交订单(待付款状态)
                第二步：取消订单:(用户取消)     或者支付成功(待发货)
                第三步：支付成功后退款(仅退款)     已发货还未收到货退款(退货退款状态) 收到货退款(退货退款状态)
                第四步：售后
                第五步：交易完成
        4。库存模块
            作用：
                这里主要记录的是商品 SKU 的具体库存信息
            库存信息分为：
                商品 SKU
                仓区
                    原因：
                        1。现在大部分电商都实现了华南华北的库存分区
                        2。所以可能存在同一个商品 SKU 在华北没有库存，而在华南存在库存的情况
                    作用：
                        区分不同地区仓库的同一个商品 SKU。
                实时库存：指商品的实时库存
                锁定库存：则表示用户已经提交订单到实际扣除库存或订单失效的这段时间里锁定的库存
                待退货库存：表示订单退款时的库存数量
                活动库存：每次活动时的库存数量。
                库存状态：虚拟库存状态、实物库存状态。
                        虚拟库存状态：
                            概念：
                                1。如果一个商品不需要设定库存，可以任由用户购买
                                2。我们则不需要在每次用户购买商品时都去查询库存、扣除库存，只需要设定商品的库存状态为虚拟库存即可
            功能：
                库存交易：
                    指用户购买商品时实时消费库存
                库存管理：
                    主要包括运营人员对商品的生产或采购入库、调拨。

        5。促销活动模块
            概念：
                是指消费券、红包以及满减等促销功能；
            包括：
                活动管理：负责管理每次发放的消费券及红包有效期、金额、满足条件、数量等信息
                交易管理：主要负责管理用户领取红包、消费券等信息。

    业务难点
        1。不同商品类别存在差异，如何设计商品表结构？
            案例：
                一个手机商品：手机的 SKU 包括了颜色、运行内存、存储内存等
                一件衣服：一件衣服则包含了尺码、颜色。
            问题：
                我们需要将这些商品都存放在一张表中
                思路一：要么就使用相同字段来存储不同的信息
                   缺点：
                        1。会导致程序设计复杂化、表宽度大，
                        2。从而减少磁盘单页存储行数，影响查询性能，且维护成本高
                思路二：要么就新增字段来维护各自的信息。
                    缺点：
                        1。会导致一张表中字段过多
                        2。如果有新的商品类型出现，又需要动态添加字段。
                思路三：通过一个公共表字段来存储一些具有共性的字段
                    具体实现：
                        创建单独的商品类型表，例如手机商品一个表、服饰商品一个表
                    缺点：
                        1。可能会导致表非常多，查询商品信息的时候不够灵活，不好实现全文搜索。
                思路四：
                    过程：
                        1。基于一个公共表来存储商品的公共信息，
                        2。同时结合搜索引擎，将商品详细信息存储到键值对数据库
                        3。例子： ElasticSearch、Solr 
        2。双十一购物车商品数量大增，购物车系统出现性能瓶颈怎么办？
            背景：
                1。在用户没有登录系统的情况下，我们是通过 cookie 来保存购物车的商品信息
                2。在用户登录系统之后，购物车的信息会保存到数据库中。
            过程：
                1。在双十一期间，大部分用户都会提前将商品加入到购物车中
                2。在加入商品到购物车的这段操作中，由于时间比较长，操作会比较分散，所以对数据库的写入并不会造成太大的压力
                3。但在购买时，由于多数属于抢购商品，用户对购物车的访问则会比较集中了
                4。如果都去数据库中读取，那么数据库的压力就可想而知了。
            方案：
                我们应该考虑冷热数据方案来存储购物车的商品信息
                    热数据：用户一般都会首选最近放入购物车的商品的这些信息
                    冷数据：较久之前放入购物车中的商品信息则是
            具体做法：
                我们需要提前将热数据存放在 Redis 缓存中
            优点：
                1。以便提高系统在活动期间的并发性能
                2。可以大大降低数据库的压力。
                原因：
                    当在缓存中没有查找到购物车信息时，再去数据库中查询
            例子：
                可以将购物车中近一个月的商品信息都存放到 Redis 中，且至少为一个分页的信息。

        3。订单表海量数据，如何设计订单表结构？
            背景：
                1。我们的订单表是系统数据累计最快的一张表，无论订单是否真正付款
                2。只要订单提交了就会在订单表中创建订单。
                3。如果公司的业务发展非常迅速，那么订单表的分表分库就只是迟早的事儿了。
            建议：
                如果确定后期表做升级，建议使用 snowflake 来生成主键 ID，不要使用自增id
            原因：
                1。会存在主键 ID 与业务耦合的情况
                2。新自增 ID 与之前的 ID 也可能会发生冲突
            使用自增ID缺点：
                后期做表升级的时候我们将会面临巨大的工作量
            问题一：
                如果订单表要实现水平分表，那我们基于哪个字段来实现分表呢？
                方法：
                    基于用户 ID 字段来实现。
                优点：
                    优化用户购买端对订单的操作性能
            问题二：
                在分表分库之后，对于我们的后台订单管理系统来说，查询订单就是一个挑战了？
                原因：
                    1。通常后台都是根据订单状态、创建订单时间进行查询的
                    2。且需要支持分页查询以及部分字段的 JOIN 查询
                方法：
                    对于join从查询的解决方法：
                        我们一般可以通过冗余一些不常修改的配置表来实现
                        具体案例：
                            1。商品的基础信息，我们录入之后很少修改，可以在每个分库中冗余该表
                            2。如果字段信息比较少，我们可以直接在订单表中冗余这些字段。
                    对于分页查询的解决方法：
                        通常我们建议冗余订单信息到大数据中
                        实现过程：
                            1。通过大数据来查询订单信息，用户在提交订单并且付款之后，后台将会同步这条订单到大数据
                            2。用户在 C 端修改或运营人员在后台修改订单时，会通过异步方式通知大数据更新该订单数据

        4。抢购业务，如何解决库存表的性能瓶颈？
            背景：
                1。平时购买商品时，我们一般是直接去数据库检查、锁定库存
                2。但如果是在促销活动期间抢购商品，我们还是直接去数据库检查、更新库存的话，面对高并发，系统无疑会产生性能瓶颈。
            方法：
                1。将促销活动的库存更新到缓存中，通过缓存来查询商品的实时库存
                2。并且通过分布式锁来实现库存扣减、锁定库存
        5。促销活动也存在抢购场景，如何设计表？
            背景：
                促销活动中的优惠券和红包交易，很多时候跟抢购活动有些类似。
            案例：
                1。在一些大型促销活动之前，我们一般都会定时发放各种商品的优惠券和红包
                2。用户需要点击领取才能使用。所以在一定数量的优惠券和红包放出的同时
                3。也会存在同一时间抢购这些优惠券和红包的情况，特别是一些热销商品。
            方法：
                1。用缓存和分布式锁来查询、更新优惠券和红包的数量
                2。通过缓存获取数量成功以后，再通过异步方式更新数据库中优惠券和红包的数量。

38 | 数据库参数设置优化，失之毫厘差之千里
    背景：
        数据库系统主要的性能瓶颈就是 I/O 读写的瓶颈了
        原因：
            数据库主要是用来存取数据的，而存取数据涉及到了磁盘 I/O 的读写操作
        方法：
            内存管理来优化数据库操作
            比如：
                内存优化查询、排序以及写入操作
        问题：
            内存设置得越大越好，数据刷新到磁盘越快越好，不就对了吗？
        答案：
         不对，内存设置过大，同样会带来新的问题
        原因：
            1。InnoDB 中的数据和索引缓存，如果设置过大，就会引发 SWAP 页交换。
                SWAP 页交换：
                    概念：
                        1。SWAP 分区在系统的物理内存不够用的时候，就会把物理内存中的一部分空间释放出来，以供当前运行的程序使用
                        2。等到那些程序要运行时，再从 SWAP 分区中恢复保存的数据到内存中。
                            被释放的空间：可能来自一些很长时间没有什么操作的程序
                            被释放的空间的数据：被临时保存到 SWAP 分区中
    概念：
        1。MySQL 是一个高定制化的数据库
        2。我们可以根据需求来调整参数，定制性能最优的数据库。

    MySQL 体系结构
        MySQL 的结构分为四层
            第一层：客户端连接器
                包括了数据库连接、授权认证、安全管理等，该层引用了线程池，为接入的连接请求提高线程处理效率。
            第二层：Server 层
                主要实现 SQL 的一些基础功能，包括 SQL 解析、优化、执行以及缓存等，
            第三层：各种存储引擎
                各种存储引擎，主要负责数据的存取，涉及到 Buffer 缓存
            第四层：数据存储层
                主要负责将数据存储在文件系统中，并完成与存储引擎的交互
    数据接收到一个sql怎么处理的：
        1。查询语句
            1。连接和授权认证，再将 SQL 请求发送至 SQL 接口
            2。SQL 接口接收到请求之后，会先检查查询 SQL 是否命中 Cache 缓存中的数据
                1。命中：直接返回缓存中的结果
                2。不命中：需要进入解析器。
                    1。解析器主要对 SQL 进行语法以及词法分析
                    2。之后，便会进入到优化器中，优化器会生成多种执行计划方案，并选择最优方案执行。
                    3。确定了最优执行计划方案之后，执行器会检查连接用户是否有该表的执行权限
                        1。有：查看 Buffer 中是否存在该缓存，存在则获取锁，查询表数据
                        2。没有：
                            1。重新打开表文件，通过接口调用相应的存储引擎处理
                            2。这时存储引擎就会进入到存储文件系统中获取相应的数据，并返回结果集。


        2。更新语句
            背景：
                1。数据库更新 SQL 的执行流程其实跟查询 SQL 差不多
                2。执行更新操作的时候多了记录日志的步骤
                比如：
                    1。在执行更新操作时 MySQL 会将操作的日志记录到 binlog（归档日志）中，这个步骤所有的存储引擎都有
                    2。而 InnoDB 除了要记录 binlog 之外，还需要多记录一个 redo log（重做日志）。
                        redo log：
                            作用：
                                为了解决 crash-safe 问题而引入
                                    crash-safe：
                                        概念：
                                            1。当数据库在存储数据时发生异常重启，我们需要保证存储的数据要么存储成功，要么存储失败
                                            2。也就是不会出现数据丢失的情况
            流程：
                1。首先会查询相关的数据，之后通过执行器执行更新操作
                2。并将执行结果写入到内存中，同时记录更新操作到 redo log 的缓存中
                3。此时 redo log 中的记录状态为 prepare，并通知执行器更新完成，随时可以提交事务。
                4。执行器收到通知后会执行 binlog 的写入操作，此时的 binlog 是记录在缓存中
                5。写入成功后会调用引擎的提交事务接口，更新记录状态为 commit
                6。之后，内存中的 redo log 以及 binlog 都会刷新到磁盘文件中。

    内存调优
        背景：
            在执行查询 SQL 语句时，会涉及到两个缓存。
            第一个缓存：
                Query Cache：
                    概念：
                        1。缓存的是 SQL 语句和对应的结果集，以查询 SQL 的 Hash 值为 key，返回结果集为 value 的键值对
                        2，判断一条 SQL 是否命中缓存，是通过匹配查询 SQL 的 Hash 值来实现的。
                    优点：
                        1。优化查询 SQL 语句，减少大量工作
                        2。减少了 I/O 读取操作
                    缺点：
                        仅限于不常修改的数据，
                    问题：
                        1。如果一张表数据经常进行新增、更新和删除操作
                        2。则会造成 Query Cache 的失效率非常高，从而导致频繁地清除 Cache 中的数据，给系统增加额外的性能开销。
                        3。这也会导致缓存命中率非常低
                    主要的参数：
                        1。have_query_cache:表示是否支持Query Cache
                        2。query_cache_limit:表示存放的单条Query最大结果集，默认值是1M，结果集大小超过该值的Query不会被cache
                        3。query_cache_min_res_unit:表示每个结果哦集存放的最小内存大小，默认为4k
                        4。query_cache_size:表示系统用于Query Cache的大小
                        5。query_cache_type:表示是否打开了Query Cache的功能；ON，OFF，DEMAND（表示只有在查询语句中使用SQL_CACHE和SQL_NO_CACHE来控制是否需要缓存）
                        6。query_cache_min_res_unit：减少碎片，最合适的大小和应用程序查询结果的平均大小直接相关，
                            （query_cache_size - Qcache_free_memory）/ Qcache_queries_in_cache
                        7。Qcache_free_memory 和 Qcache_queries_in_cache 的值可以通过以下命令查询：
                                show status like 'Qcache%'
                        8。Qcache_hits：值表示缓存命中率，如果缓存命中率特别低的话
                            我们还可以通过 query_cache_size = 0 或者 query_cache_type 来关闭查询缓存。

            第二个缓存：
                存储引擎中的 Buffer 缓存
                背景：
                    不同的存储引擎，使用的 Buffer 也是不一样的
                    1。MyISAM 存储引擎参数设置调优
                        概念：
                            1。MyISAM 存储引擎使用 key buffer 缓存索引块
                            2。MyISAM 表的数据块则没有缓存，它是直接存储在磁盘文件中的。
                        参数：
                            key_buffer_size：设置key buffer 缓存的大小
                                如何评估合理：通过缓存使用率公式来计算：
                            1-((key_blocks_unused*key_cache_block_size)/key_buffer_size)
                                key_blocks_unused ：表示未使用的缓存簇（blocks）数
                                key_cache_block_size ：表示 key_buffer_size 被分割的区域大小
                                key_blocks_unused*key_cache_block_size： 则表示剩余的可用缓存空间（一般来说，缓存使用率在 80% 作用比较合适）。

                2。InnoDB 存储引擎参数设置调优
                    概念：
                        1。InnoDB Buffer Pool（简称 IBP）是 InnoDB 存储引擎的一个缓冲池
                        2。与 MyISAM 存储引擎使用 key buffer 缓存不同，它不仅存储了表索引块，还存储了表数据
                    原理：
                        查询数据时，IBP 允许快速返回频繁访问的数据，而无需访问磁盘文件。
                    InnoDB 表空间缓存越多好处：
                        MySQL 访问物理磁盘的频率就越低，这表示查询响应时间更快，系统的整体性能也有所提高。
                    通过多个设置参数来调整 IBP，优化 InnoDB 表性能。
                    1。innodb_buffer_pool_size
                        概念：
                            1。IBP 默认的内存大小是 128M
                            2。IBP 设置得越大，InnoDB 表性能就越好
                            3。表示 IBP 的大小
                        问题：
                            将 IBP 大小设置得过大也不好，可能会导致系统发生 SWAP 页交换
                        方法一：
                            需要在 IBP 大小和其它系统服务所需内存大小之间取得平衡
                            建议：
                                MySQL 推荐配置 IBP 的大小为服务器物理内存的 80%。
                        方法二：
                            通过计算 InnoDB 缓冲池的命中率来调整 IBP 大小：
                            公式：
                                (1-innodb_buffer_pool_reads/innodb_buffer_pool_read_request)*100
                            思路：
                                1。如果我们将 IBP 的大小设置为物理内存的 80% 以后
                                2。发现命中率还是很低，此时我们就应该考虑扩充内存来增加 IBP 的大小。

                    2。innodb_buffer_pool_instances
                        作用：
                            提高系统的并发性。
                                原因：
                                    将缓冲池划分为单独的实例可以减少不同线程读取和写入缓存页面时的争用
                        背景：
                            该参数项仅在将 innodb_buffer_pool_size 设置为 1GB 或更大时才会生效
                        例子一：
                            1。在 windows 32 位操作系统中，如果 innodb_buffer_pool_size 的大小超过 1.3GB
                            2。该值默认大小就为 innodb_buffer_pool_size/128MB
                            3。否则，默认为 1。
                        例子二：
                            1。而在其它操作系统中，如果 innodb_buffer_pool_size 大小超过 1GB，
                            2。该值就默认为 8；否则，默认为 1。
                        操作方案：
                            为了获取最佳效率，建议指定该值的大小，并保证每个缓冲池实例至少有 1GB 内存
                            建议：
                                该值的大小不超过 innodb_read_io_threads + innodb_write_io_threads 之和，建议实例和线程数量比例为 1:1。

                    3。innodb_read_io_threads / innodb_write_io_threads
                        背景：
                            1。在默认情况下，MySQL 后台线程包括了主线程、IO 线程、锁线程以及监控线程等，
                            2。其中读写线程属于 IO 线程，主要负责数据库的读取和写入操作
                            3。这些线程分别读取和写入 innodb_buffer_pool_instances 创建的各个内存页面
                                innodb_read_io_threads：设置读线程数量，默认为4
                                innodb_write_io_threads：设置写线程数量，默认为4
                            两者的数量与innodb_buffer_pool_instances有关，两者的协同优化是提高系统性能的一个关键因素。
                                ( innodb_read_io_threads + innodb_write_io_threads ) = innodb_buffe_pool_instances
                        问题：对半分一般不合理
                        原因：
                            例如我们的应用服务读取数据库的数据多于写入数据库的数据，那么增加写入线程反而没有优化效果
                        方法：我们可以通过以下查询来确定读写比率：
                            1。如果读大于写，我们应该考虑将读线程的数量设置得大一些，写线程数量小一些
                            2。否则，反之。
                            SHOW GLOBAL STATUS LIKE 'Com_select';// 读取数量
                            SHOW GLOBAL STATUS WHERE Variable_name IN ('Com_insert', 'Com_update', 'Com_replace', 'Com_delete');// 写入数量
            注意：
                1。除了以上 InnoDB 缓存等因素之外，
                2。InnoDB 的日志缓存大小、日志文件大小以及日志文件持久化到磁盘的策略都影响着 InnnoDB 的性能

            第三个：InnoDB 的日志文件大小
                innodb_log_file_size
                    概念：
                        1。InnoDB 中有一个 redo log 文件，InnoDB 用它来存储服务器处理的每个写请求的重做活动。
                        2。执行的每个写入查询都会在日志文件中获得重做条目
                    作用：
                        以便在发生崩溃时可以恢复更改。
                    问题：
                        当日志文件大小已经超过我们参数设置的日志文件大小时
                    执行过程：
                    1。InnoDB 会自动切换到另外一个日志文件，由于重做日志是一个循环使用的环
                    2。在切换时，就需要将新的日志文件脏页的缓存数据刷新到磁盘中（触发检查点）。
                        假设：
                            innodb_log_file_size 设置得越大，缓冲池中需要的检查点刷新活动就越少，从而节省磁盘 I/O。
                            那是不是将这个日志文件设置得越大越好呢？
                        缺点：
                            如果日志文件设置得太大，恢复时间就会变长，这样不便于 DBA 管理
                    实际：
                        在大多数情况下，我们将日志文件大小设置为 1GB 就足够了。

            第四个：日志缓存大小
                innodb_log_buffer_size
                    概念：
                        决定了 InnoDB 重做日志缓冲池的大小，默认值为 8MB
                    作用：
                        可以通过增大该参数来减少写入磁盘操作，从而提高并发时的事务性能。
                    该值太小的缺点：
                        如果高并发中存在大量的事务，该值设置得太小，就会增加写入磁盘的 I/O 操作

            第五个：日志文件持久化到磁盘的策略
                innodb_flush_log_at_trx_commit
                    概念：
                        1。可以控制重做日志从缓存写入文件刷新到磁盘中的策略，默认值为 1。
                        2。参数为0时，InnoDB 每秒种就会触发一次缓存日志写入到文件中并刷新到磁盘的操作
                            问题：
                                有可能在数据库崩溃后，丢失 1s 的数据。
                        3。当设置该参数为 1 时，则表示每次事务的 redo log 都会直接持久化到磁盘中
                            优点：可以保证 MySQL 异常重启之后数据不会丢失。
                        4。当设置该参数为 2 时，每次事务的 redo log 都会直接写入到文件中，再将文件刷新到磁盘。
                    场景：
                    1。设置为1，在一些对数据安全性要求比较高
                    2。设置为 0 或 2，在一些可以容忍数据库崩溃时丢失 1s 数据的场景中
                        优点：减少日志同步到磁盘的 I/O 操作。




39 | 答疑课堂：MySQL中InnoDB的知识点串讲
    InnoDB 体系架构
        1。内存池
            概念：
                1。是由多个内存块组成的，主要包括缓存磁盘数据、redo log 缓冲等
                2。理论上来说，缓冲池的内存越大越好，参考38讲
                3。不仅缓存索引页和数据页，还包括了 undo 页，插入缓存、自适应哈希索引以及 InnoDB 的锁信息
                4。允许多个缓冲池实例，从而减少数据库内部资源的竞争，增强数据库的并发处理能力，参考38讲
                5。InnoDB 存储引擎会先将重做日志信息放入到缓冲区中，然后再刷新到重做日志文件中。
            执行过程：
                1。客户端读取数据时，如果数据存在于缓冲池中，客户端就会直接读取缓冲池中的数据，否则再去磁盘中读取
                2。对于数据库中的修改数据，首先是修改在缓冲池中的数据
                3。然后再通过 Master Thread 线程刷新到磁盘上。
            作用：
                提高整个数据库的读写性能。
            原因：
                如果客户端从数据库中读取数据是直接从磁盘读取的话，无疑会带来一定的性能瓶颈
        2。后台线程
            概念：
                包括了 Master Thread、IO Thread 以及 Purge Thread 等；
            Master Thread：
                作用：
                    负责将缓冲池中的数据异步刷新到磁盘中，除此之外还包括插入缓存、undo 页的回收等
            IO Thread：
                作用：
                    是负责读写 IO 的线程
            Purge Thread：
                作用：
                    用于回收事务已经提交了的 undo log
            Pager Cleaner Thread：
                概念：
                    新引入的一个用于协助 Master Thread 刷新脏页到磁盘的线程，
                作用：
                    可以减轻 Master Thread 的工作压力，减少阻塞。
        3。存储文件：
            背景：
                在 MySQL 中建立一张表都会生成一个.frm 文件
                    说明：.frm文件：该文件是用来保存每个表的元数据信息的，主要包含表结构定义。
            概念：
                1。包括表结构文件（.frm）、共享表空间文件（ibdata1）、独占表空间文件（ibd）以及日志文件（redo 文件等）
                2。在 InnoDB 中，存储数据都是按表空间进行存放的，默认为共享表空间
                    共享表空间文件（ibdata1）：存储的文件。
                    独占表空间：将存储的数据、索引等信息单独存储，在这里，因此也会产生一个独占表空间文件（ibd）
                    日志文件：主要是重做日志文件，主要记录事务产生的重做日志，保证事务的一致性
    InnoDB 逻辑存储结构：
        1。表空间（Tablespace）
            背景：
                1。InnoDB 提供了两种表空间存储数据的方式
                    1。共享表空间
                    2。独占表空间
                2。 InnoDB 默认会将其所有的表数据存储在一个共享表空间中，即 ibdata1
            参数设置：
                innodb_file_per_table=1：开启独占表空间
                    作用：
                        方便备份以及恢复数据
                    原因：
                        1。每个表都有自己独立的表空间物理文件
                        2。所有的数据以及索引都会存储在该文件中

        2。段 (Segment)
            背景：
                InnoDB 默认是基于 B + 树实现的数据存储
            分类：
                1。数据段：是指的 B + 树的非叶子节点
                2。索引段：是 B + 树的叶子节点
                3。回滚段：指的是回滚数据
                    例子：
                        我们在讲事务隔离的时候就介绍到了 MVCC 利用了回滚段实现了多版本查询数据。

        3。区 (Extent) / 页（Page）
            区：表空间的单元结构，每个区的大小为 1MB
            页：组成区的最小单元，页也是 InnoDB 存储引擎磁盘管理的最小单元，每个页的大小默认为 16KB
            注意：
                为了保证页的连续性，InnoDB 存储引擎每次从磁盘申请 4-5 个区。

        4。行（Row）
            概念：
                1。InnoDB 存储引擎是面向列的（row-oriented)，也就是说数据是按行进行存放的
                2。每个页存放的行记录也是有硬性定义的，最多允许存放 16KB/2-200 行，即 7992 行记录。

    InnoDB 事务之 redo log 工作原理
        背景：
            InnoDB 是一个事务性的存储引擎
        如何实现事务的：
            基于事务日志 redo log 和 undo log 实现的
                redo log：重做日志，提供再写入操作，实现事务的持久性
                    1。内存中的日志缓冲redo log buffer：存储在内存中，容易丢失
                    2。重做日志文件redo log file：持久化在磁盘中，不会丢失
                undo log：是回滚日志，提供回滚操作，保证事务的一致性。

        InnoDB 的更新操作
            背景：
                采用的是 Write Ahead Log 策略
            原理：
                先写日志，再写入磁盘
                    innodb_flush_log_at_trx_commit参数设置：
                        作用：
                            如何将 redo log buffer 中的日志刷新到 redo log file 中。
            例子：
                1。当一条记录更新时，InnoDB 会先把记录写入到 redo log buffer 中
                2。并更新内存数据
        InnoDB 的 redo log的原理：
            可以参考mysql的笔记
            原理：
                1。大小是固定的，分别有多个日志文件采用循环方式组成一个循环闭环
                2。当写到结尾时，会回到开头循环写日志
            参数设置：
                1。innodb_log_files_in_group：日志文件数量
                2。innodb_log_file_size：每个日志文件的大小。
            脏页：
                Buffer Pool 中更新的数据未刷新到磁盘中的内存页
                注意：
                    1。最终脏页的数据会刷新到磁盘中，将磁盘中的数据覆盖
                    2。这个过程与 redo log 不一定有关系。
            触发脏页刷新到磁盘的条件：
                1。redo log 日志满了的情况下
                2。系统内存不足时，需要将一部分数据页淘汰掉，如果淘汰的是脏页，需要先将脏页同步到磁盘；
                3。MySQL 认为空闲的时间，这种情况没有性能问题；
                4。MySQL 正常关闭之前，会把所有的脏页刷入到磁盘，这种情况也没有性能问题。
            案例：
                生产环境中，如果我们开启了慢 SQL 监控，你会发现偶尔会出现一些用时稍长的 SQL
                原因：
                    脏页在刷新到磁盘时可能会给数据库带来性能开销，导致数据库操作抖动。

    LRU 淘汰策略：
        背景：
            1。InnoDB 存储引擎是基于集合索引实现的数据存储
                集合索引：
                    1。索引列：存储在 B + 树
                    2。主键：存储在 B + 树
                    3。其它列数据：存储在 B + 树的叶子节点
            2。索引页和数据页都会缓存在缓冲池中
                目的：
                    提高数据库的查询性能
                原因：
                    查询数据时，只要在缓冲池中存在该数据，InnoDB 就不用每次都去磁盘中读取页
                缺点：
                    缓冲池是一个很大的内存区域，但无法将所有的数据都存储在其中
                原因：
                    由于存放了各种类型的数据，加上存储数据量之大
                方法：
                    通过LRU算法淘汰
        LRU算法原理：
            1。将最近且经常查询的数据缓存在其中
            2。而不常查询的数据就淘汰出去。
        注意：
            InnoDB 对 LRU 做了一些优化，
                普通的LRU算法：
                    将最近查询的数据放到 LRU 列表的首部
                InnoDB 对 LRU算法：
                    将数据放在一个 midpoint 位置，通常这个 midpoint 为列表长度的 5/8。
                    目的：
                        1。为了避免一些不常查询的操作突然将热点数据淘汰出去
                        2。热点数据被再次查询时，需要再次从磁盘中获取，从而影响数据库的查询性能。
                    问题：
                        我们的热点数据比较多的时候，怎么办：
                    方法：
                        可以通过调整 midpoint 值来增加热点数据的存储量，从而降低热点数据的淘汰率。

41 | 如何设计更优的分布式锁？
    案例：
        1。去年双十一，我们的游戏商城也搞了一波活动，那时候我就发现在数据库操作日志中
        2。出现最多的一个异常就是 Interrupted Exception 了，几乎所有的异常都是来自一个校验订单幂等性的 SQL。
        分析：
            1。因为校验订单幂等性是提交订单业务中第一个操作数据库的，所以幂等性校验也就承受了比较大的请求量
            2。再加上我们还是基于一个数据库表来实现幂等性校验的，所以出现了一些请求事务超时，事务被中断的情况
            3。其实基于数据库实现的幂等性校验就是一种分布式锁的实现。
    问题：
        那什么是分布式锁呢，它又是用来解决哪些问题的呢？
    答案：
        在JVM中：
            1。在多线程并发的情况下，我们可以使用同步锁或 Lock 锁
            2。保证在同一时间内，只能有一个线程修改共享变量或执行代码块
            分析：
                1。基于分布式集群来实现部署的，对于一些共享资源
                2。例如我们之前讨论过的库存，在分布式环境下使用 Java 锁的方式就失去作用了。
            方法：
                分布式锁
                作用：
                    1。来保证共享资源的原子性
                    2。分布式锁也经常用来避免分布式中的不同节点执行重复性的工作
                例子：
                    1。一个定时发短信的任务，在分布式集群中，我们只需要保证一个服务节点发送短信即可
                    2。一定要避免多个节点重复发送短信给同一个用户。
    方法一
    数据库实现分布式锁
        具体实现：
            第一步：
                首先，我们应该创建一个锁表，通过创建和查询数据来保证一个数据的原子性：
                CREATE TABLE `order`  (
                    `id` int(11) NOT NULL AUTO_INCREMENT,
                    `order_no` int(11) DEFAULT NULL,
                    `pay_money` decimal(10, 2) DEFAULT NULL,
                    `status` int(4) DEFAULT NULL,
                    `create_date` datetime(0) DEFAULT NULL,
                    `delete_flag` int(4) DEFAULT NULL,
                    PRIMARY KEY (`id`) USING BTREE,
                    INDEX `idx_status`(`status`) USING BTREE,
                    INDEX `idx_order`(`order_no`) USING BTREE
                ) ENGINE = InnoDB
            第二步：
                1。如果是校验订单的幂等性，就要先查询该记录是否存在数据库中
                2。查询的时候要防止幻读，如果不存在，就插入到数据库，否则，放弃操作。
                select id from `order` where `order_no`= 'xxxx' for update
            第三步：
                1。最后注意下，除了查询时防止幻读，我们还需要保证查询和插入是在同一个事务中
                2。因此我们需要申明事务，具体的实现代码如下：
                @Transactional
                public int addOrderRecord(Order order) {
                    if(orderDao.selectOrderRecord(order)==null){
                        int result = orderDao.addOrderRecord(order);
                        if(result>0){
                            return 1;
                        }
                    }
                    return 0;
                }
            分析：
                1。在 RR 事务级别，select 的 for update 操作是基于间隙锁 gap lock 实现的
                2。这是一种悲观锁的实现方式，所以存在阻塞问题。
                情形一：
                    1。因此在高并发情况下，当有大量的请求进来时，大部分的请求都会进行排队等待
                    方法：
                        为了保证数据库的稳定性，事务的超时时间往往又设置得很小
                    缺点：
                        就会出现大量事务被中断的情况
                情形二：
                    1。除了阻塞等待之外，因为订单没有删除操作，所以这张锁表的数据将会逐渐累积
                        方法：
                            我们需要设置另外一个线程，隔一段时间就去删除该表中的过期订单
                        缺点：
                            这就增加了业务的复杂度。
        注意：
            1。有一些单纯基于数据库实现的分布式锁代码块或对象，是需要在锁释放时，删除或修改数据的。
            2。如果在获取锁之后，锁一直没有获得释放，即数据没有被删除或修改，这将会引发死锁问题。
        优点：
            1。简单易懂
            2。不需要再引入第三方中间件
        缺点：
            存在性能瓶颈

    Zookeeper 实现分布式锁
        概念：
            是一种提供“分布式服务协调“的中心化服务
        特性：
            1。顺序临时节点：
                背景：
                    1。Zookeeper 提供一个多层级的节点命名空间（节点称为 Znode）
                    2。每个节点都用一个以斜杠（/）分隔的路径来表示，而且每个节点都有父节点（根节点除外），非常类似于文件系统。
                    节点类型：
                        1。持久节点（PERSISTENT ）
                        2。临时节点（EPHEMERAL）
                    节点特点：
                        1。每个节点还能被标记为有序性（SEQUENTIAL），一旦节点被标记为有序性
                        2。那么整个节点就具有顺序自增的特点。
                    应用：
                        一般我们可以组合这几类节点来创建我们所需要的节点
                        例子：
                            1。创建一个持久节点作为父节点
                            2。在父节点下面创建临时节点，并标记该临时节点为有序
            2。Watch 机制：
                概念：
                    1。ZooKeeper 允许用户在指定节点上注册一些 Watcher
                    2。并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知给用户。
        实现过程：
            第一步：
                1。我们需要建立一个父节点，节点类型为持久节点（PERSISTENT） ，每当需要访问共享资源时
                2。就会在父节点下建立相应的顺序子节点，节点类型为临时节点（EPHEMERAL）
                3。且标记为有序性（SEQUENTIAL），并且以临时节点名称 + 父节点名称 + 顺序号组成特定的名字。
            第二步：
                1。在建立子节点后，对父节点下面的所有以临时节点名称 name 开头的子节点进行排序
                2。判断刚刚建立的子节点顺序号是否是最小的节点，如果是最小节点，则获得锁。
            第三步：
                1。如果不是最小节点，则阻塞等待锁，并且获得该节点的上一顺序节点
                2。为其注册监听事件，等待节点对应的操作获得锁。
            第四步：
                当调用完共享资源后，删除该节点，关闭 zk，进而可以触发监听事件，释放该锁。
            第五步：
                1。以上实现的分布式锁是严格按照顺序访问的并发锁
                2。一般我们还可以直接引用 Curator 框架来实现 Zookeeper 分布式锁，代码如下：
            参考代码：
                InterProcessMutex lock = new InterProcessMutex(client, lockPath);
                if ( lock.acquire(maxWait, waitUnit) ) 
                {
                    try
                    {
                        // do some work inside of the critical section here
                    }
                    finally
                    {
                        lock.release();
                    }
                }
        优点：
            1。是集群实现，可以避免单点问题
            2。且能保证每次操作都可以有效地释放锁
            原因：
                因为一旦应用服务挂掉了，临时节点会因为 session 连接断开而自动删除掉。
        缺点：
            由于频繁地创建和删除结点，加上大量的 Watch 事件，对 Zookeeper 集群来说，压力非常大

    Redis 实现分布式锁
        缺点：
            基于 Redis 实现的分布式锁是最为复杂的
        优点：
            性能是最佳的。
        Redis 2.6.12 版本之前：
            大部分开发人员利用 Redis 实现分布式锁的方式，都是使用 SETNX+EXPIRE 组合来实现
            参考代码：
                public static boolean tryGetDistributedLock(Jedis jedis, String lockKey, String requestId, int expireTime) {

                    Long result = jedis.setnx(lockKey, requestId);// 设置锁
                    if (result == 1) {// 获取锁成功
                        // 若在这里程序突然崩溃，则无法设置过期时间，将发生死锁
                        jedis.expire(lockKey, expireTime);// 通过过期时间删除锁
                        return true;
                    }
                    return false;
                }
            说明：
                1。这种方式实现的分布式锁，是通过 setnx() 方法设置锁，如果 lockKey 存在，则返回失败，否则返回成功。
                2。设置成功之后，为了能在完成同步代码之后成功释放锁
                3。方法中还需要使用 expire() 方法给 lockKey 值设置一个过期时间，确认 key 值删除
                4。避免出现锁无法释放，导致下一个线程无法获取到锁，即死锁问题。
            缺点：
                1。如果程序在设置过期时间之前、设置锁之后出现崩溃
                2。此时如果 lockKey 没有设置过期时间，将会出现死锁问题。

        在 Redis 2.6.12 版本后 SETNX 增加了过期时间参数：
            参考代码：
                private static final String LOCK_SUCCESS = "OK";
                private static final String SET_IF_NOT_EXIST = "NX";
                private static final String SET_WITH_EXPIRE_TIME = "PX";

                /**
                * 尝试获取分布式锁
                * @param jedis Redis 客户端
                * @param lockKey 锁
                * @param requestId 请求标识
                * @param expireTime 超期时间
                * @return 是否获取成功
                */
                public static boolean tryGetDistributedLock(Jedis jedis, String lockKey, String requestId, int expireTime) {

                    String result = jedis.set(lockKey, requestId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime);

                    if (LOCK_SUCCESS.equals(result)) {
                        return true;
                    }
                    return false;

                }
            扩展：
                1。我们也可以通过 Lua 脚本来实现锁的设置和过期时间的原子性
                2。再通过 jedis.eval() 方法运行该脚本：
                参考代码：
                    // 加锁脚本
                    private static final String SCRIPT_LOCK = "if redis.call('setnx', KEYS[1], ARGV[1]) == 1 then redis.call('pexpire', KEYS[1], ARGV[2]) return 1 else return 0 end";
                    // 解锁脚本
                    private static final String SCRIPT_UNLOCK = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end";

        注意：
            1。虽然 SETNX 方法保证了设置锁和过期时间的原子性
            2。但如果我们设置的过期时间比较短，而执行业务时间比较长，就会存在锁代码块失效的问题
            3。我们需要将过期时间设置得足够长，来保证以上问题不会出现。
        缺点：
            1。这个方案是目前最优的分布式锁方案，但如果是在 Redis 集群环境下，依然存在问题
            2。由于 Redis 集群数据同步到各个节点时是异步的，如果在 Master 节点获取到锁后
            3。在没有同步到其它节点时，Master 节点崩溃了，此时新的 Master 节点依然可以获取锁，
            4。所以多个应用服务可以同时获取到锁。

    Redlock 算法
        Redisson：
            概念：
                1。由 Redis 官方推出，它是一个在 Redis 的基础上实现的 Java 驻内存数据网格（In-Memory Data Grid）
                2。它不仅提供了一系列的分布式的 Java 常用对象，还提供了许多分布式服务
                3。Redisson 是基于 netty 通信框架实现的，所以支持非阻塞通信，性能相对于我们熟悉的 Jedis 会好一些。
                4。Redisson 中实现了 Redis 分布式锁，且支持单点模式和集群模式
            集群模式下：
                1。Redisson 使用了 Redlock 算法，避免在 Master 节点崩溃切换到另外一个 Master 时，多个应用同时获得锁
                2。我们可以通过一个应用服务获取分布式锁的流程，了解下 Redlock 算法的实现：
                    1。在不同的节点上使用单个实例获取锁的方式去获得锁，且每次获取锁都有超时时间
                    2。如果请求超时，则认为该节点不可用
                    3。当应用服务成功获取锁的 Redis 节点超过半数（N/2+1，N 为节点数) 时
                    4。并且获取锁消耗的实际时间不超过锁的过期时间，则获取锁成功。
                    5。一旦获取锁成功，就会重新计算释放锁的时间，该时间是由原来释放锁的时间减去获取锁所消耗的时间；
                    6。而如果获取锁失败，客户端依然会释放获取锁成功的节点。
                具体实现：
                    1。首先引入 jar 包：
                        <dependency>
                            <groupId>org.redisson</groupId>
                            <artifactId>redisson</artifactId>
                            <version>3.8.2</version>
                        </dependency>
                    2。实现 Redisson 的配置文件：
                        @Bean
                        public RedissonClient redissonClient() {
                            Config config = new Config();
                            config.useClusterServers()
                            .setScanInterval(2000) // 集群状态扫描间隔时间，单位是毫秒
                            .addNodeAddress("redis://127.0.0.1:7000).setPassword("1")
                            .addNodeAddress("redis://127.0.0.1:7001").setPassword("1")
                            .addNodeAddress("redis://127.0.0.1:7002")
                            .setPassword("1");
                            return Redisson.create(config);
                        }
                    3。获取锁操作：
                        long waitTimeout = 10;
                        long leaseTime = 1;
                        RLock lock1 = redissonClient1.getLock("lock1");
                        RLock lock2 = redissonClient2.getLock("lock2");
                        RLock lock3 = redissonClient3.getLock("lock3");

                        RedissonRedLock redLock = new RedissonRedLock(lock1, lock2, lock3);
                        // 同时加锁：lock1 lock2 lock3
                        // 红锁在大部分节点上加锁成功就算成功，且设置总超时时间以及单个节点超时时间
                        redLock.trylock(waitTimeout,leaseTime,TimeUnit.SECONDS);
                        ...
                        redLock.unlock();

42 | 电商系统的分布式事务调优
    案例：
        1。我们团队曾经遇到过一个非常严重的线上事故，在一次 DBA 完成单台数据库线上补丁后
        2。系统偶尔会出现异常报警，我们的开发工程师很快就定位到了数据库异常问题。
        具体情况：
            当玩家购买道具之后，扣除通宝时出现了异常
        分析：
            1。这种异常在正常情况下发生之后，应该是整个购买操作都需要撤销
            2。然而这次异常的严重性就是在于玩家购买道具成功后，没有扣除通宝。
        原因：
            1。是由于购买的道具更新的是游戏数据库，而通宝是在用户账户中心数据库，
            2。在一次购买道具时，存在同时操作两个数据库的情况，属于一种分布式事务
            3。而我们的工程师在完成玩家获得道具和扣除余额的操作时，没有做到事务的一致性
            4。即在扣除通宝失败时，应该回滚已经购买的游戏道具。
    背景：
        1。大部分公司的服务基本都实现了微服务化，首先是业务需求，为了解耦业务
        2。其次是为了减少业务与业务之间的相互影响。
        例子：
            1。电商系统亦是如此，大部分公司的电商系统都是分为了不同服务模块
            2。例如商品模块、订单模块、库存模块等等
            分解服务：
                优点：
                    可以带来一些开发、性能以及运维上的优势
                缺点：
                    1。增加业务开发的逻辑复杂度
                    2。最为突出的就是分布式事务了。
    分布式服务部署的服务架构有以下两种：
        一种是：
            同服务不同数据库
            参考：第42讲同种服务不同数据库.png
        一种是：
            不同服务不同数据库
            参考：
                第42讲不同服务不同数据库.png
        注意：
            我们都是基于第二种架构部署实现的

    分布式事务解决方案
        单个数据库：
            数据事务操作具有 ACID 四个特性
        多个数据库：
            但如果在一个事务中操作多个数据库，则无法使用数据库事务来保证一致性。
            解释：
                1。当两个数据库操作数据时，可能存在一个数据库操作成功
                2。而另一个数据库操作失败的情况，我们无法通过单个数据库事务来回滚两个数据操作。
        目的：
            1。为了解决在同一个事务下，不同节点的数据库操作数据不一致的问题
            2。在一个事务操作请求多个服务或多个数据库节点时
            3。要么所有请求成功，要么所有请求都失败回滚回去。
        实现方式：
            方式一：
                XA 协议实现的二阶提交（2PC）
                XA 规范：
                    背景：
                        在 XA 规范之前，存在着一个 DTP 模型，该模型规范了分布式事务的模型设计
                        DTP 模型：
                            组成：
                                1。AP：
                                    概念：
                                        应用程序，是事务发起和结束的地方
                                2。RM：
                                    概念：
                                        是资源管理器，主要负责管理每个数据库的连接数据源
                                3。TM：
                                    概念：
                                        事务管理器，负责事务的全局管理，包括事务的生命周期管理和资源的分配协调等
                            联系：
                                1。XA 则规范了 TM 与 RM 之间的通信接口
                                2。在 TM 与多个 RM 之间形成一个双向通信桥梁，
                                3。从而在多个数据库资源下保证 ACID 四个特性
                    注意：
                        JTA 是基于 XA 规范实现的一套 Java 事务编程接口，是一种两阶段提交事务。

                二阶提交事务：
                    概念：
                        1。XA 规范实现的分布式事务属于二阶提交事务
                        2。顾名思义就是通过两个阶段来实现事务的提交。
                    阶段：
                        第一阶段：
                            1。应用程序向事务管理器（TM）发起事务请求
                            2。而事务管理器则会分别向参与的各个资源管理器（RM）发送事务预处理请求（Prepare）
                            3。此时这些资源管理器会打开本地数据库事务，然后开始执行数据库事务
                            4。但执行完成后并不会立刻提交事务，而是向事务管理器返回已就绪（Ready）或未就绪（Not Ready）状态
                        第二阶段：
                            如果各个参与节点都返回状态了，就会进入第二阶段。
                                情形一：
                                    1。如果资源管理器返回的都是就绪状态，事务管理器则会向各个资源管理器发送提交（Commit）通知
                                    2。资源管理器则会完成本地数据库的事务提交，最终返回提交结果给事务管理器。
                                情形二：
                                    1。如果任意资源管理器返回了未就绪状态，此时事务管理器会向所有资源管理器发送事务回滚（Rollback）通知
                                    2。此时各个资源管理器就会回滚本地数据库事务，释放资源，并返回结果通知。
                    缺点：
                        第一：
                            1。在整个流程中，我们会发现各个资源管理器节点存在阻塞，只有当所有的节点都准备完成之后
                            2。事务管理器才会发出进行全局事务提交的通知，这个过程如果很长
                            3。则会有很多节点长时间占用资源，从而影响整个节点的性能。
                            现象：
                                1。一旦资源管理器挂了，就会出现一直阻塞等待的情况
                                2。类似问题，我们可以通过设置事务超时时间来解决。
                        第二：
                            仍然存在数据不一致的可能性
                            例子：
                                1。在最后通知提交全局事务时，由于网络故障
                                2。部分节点有可能收不到通知，由于这部分节点没有提交事务，就会导致数据不一致的情况出现。

            方式二：
                三阶提交 (3PC)
                背景：
                    为了减少二阶段此类问题的发生。
                区别：
                    3PC 把 2PC 的准备阶段分为了准备阶段和预处理阶段
                流程：
                    第一阶段：
                        只是询问各个资源节点是否可以执行事务
                    第二阶段：
                        所有的节点反馈可以执行事务，才开始执行事务操作
                    第三阶段：
                        执行提交或回滚操作
                        注意：
                            1。并且在事务管理器和资源管理器中都引入了超时机制
                            2。如果在第三阶段，资源节点一直无法收到来自资源管理器的提交或回滚请求，它就会在超时之后，继续提交事务。
                优点：
                    3PC 可以通过超时机制，避免管理器挂掉所造成的长时间阻塞问题
                缺点：
                    1。其实这样还是无法解决在最后提交全局事务时，由于网络故障无法通知到一些节点的问题
                    2。特别是回滚通知，这样会导致事务等待超时从而默认提交。

            方式三：
                TCC 补偿性事务
                背景：
                    1。以上这种基于 XA 规范实现的事务提交，由于阻塞等性能问题，有着比较明显的低性能、低吞吐的特性
                    2。所以在抢购活动中使用该事务，很难满足系统的并发性能。
                    3。JTA 只能解决同一服务下操作多数据源的分布式事务问题
                        微服务架构下：
                            可能存在同一个事务操作，分别在不同服务上连接数据源，提交数据库操作。
                    4。TCC 正是为了解决以上问题而出现的一种分布式事务解决方案
                概念：
                    1。TCC 采用最终一致性的方式实现了一种柔性分布式事务
                    2。与 XA 规范实现的二阶事务不同的是，TCC 的实现是基于服务层实现的一种二阶事务提交。

                TCC 分为三个阶段，即 Try、Confirm、Cancel 三个阶段。
                    Try 阶段：
                        概念：
                            主要尝试执行业务，执行各个服务中的 Try 方法，主要包括预留操作；
                    Confirm阶段：
                        概念：
                            1。确认 Try 中的各个方法执行成功
                            2。然后通过 TM 调用各个服务的 Confirm 方法，这个阶段是提交阶段；
                    Cancel阶段：
                        概念：
                            当在 Try 阶段发现其中一个 Try 方法失败
                            例子：
                                1。例如预留资源失败、代码异常等，则会触发 TM 调用各个服务的 Cancel 方法
                                2。对全局事务进行回滚，取消执行业务。
                问题：
                    如果在 Confirm 和 Cancel 阶段出现异常情况，那 TCC 该如何处理呢？
                    方法：
                        此时 TCC 会不停地重试调用失败的 Confirm 或 Cancel 方法，直到成功为止。
                缺点：
                    TCC 补偿性事务也有比较明显的缺点，那就是对业务的侵入性非常大。
                    分析：
                        1。首先，我们需要在业务设计的时候考虑预留资源
                        2。然后，我们需要编写大量业务性代码，例如 Try、Confirm、Cancel 方法；
                        3。最后，我们还需要为每个方法考虑幂等性。
                    实践：
                        这种事务的实现和维护成本非常高，但综合来看，这种实现是目前大家最常用的分布式事务解决方案
            方法四：
                业务无侵入方案——Seata(Fescar)
                概念：
                    1。Seata 是阿里去年开源的一套分布式事务解决方案
                    2。开源一年多已经有一万多 star 了，可见受欢迎程度非常之高。
                特点：
                    1。Seata 的基础建模和 DTP 模型类似，只不过前者是将事务管理器分得更细了
                    2。抽出一个事务协调器（Transaction Coordinator 简称 TC），主要维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚
                    3。而 TM 则负责开启一个全局事务，并最终发起全局提交或全局回滚的决议

                事务流程：
                    1。TM 向 TC 申请开启一个全局事务，全局事务创建成功并生成一个全局唯一的 XID
                    2。XID 在微服务调用链路的上下文中传播；
                    3。RM 向 TC 注册分支事务，将其纳入 XID 对应全局事务的管辖；
                    4。TM 向 TC 发起针对 XID 的全局提交或回滚决议；
                    5。TC 调度 XID 下管辖的全部分支事务完成提交或回滚请求。
                主要区别：
                    1。它在第一提交阶段就已经将各个事务操作 commit 了
                    2。Seata 认为在一个正常的业务下，各个服务提交事务的大概率是成功的
                    3。这种事务提交操作可以节约两个阶段持有锁的时间，从而提高整体的执行效率。
                    问题：
                        那如果在第一阶段就已经提交了事务，那我们还谈何回滚呢？
                    答案：
                        情形一
                            1。Seata 将 RM 提升到了服务层，通过 JDBC 数据源代理解析 SQL
                            2。把业务数据在更新前后的数据镜像组织成回滚日志，利用本地事务的 ACID 特性
                            3。将业务数据的更新和回滚日志的写入在同一个本地事务中提交。
                        情形二：
                            1。如果 RM 决议要全局回滚，会通知 RM 进行回滚操作
                            2。通过 XID 找到对应的回滚日志记录，通过回滚记录生成反向更新 SQL，进行更新回滚操作。
                    问题：
                        以上我们可以保证一个事务的原子性和一致性，但隔离性如何保证呢？
                        答案：
                            1。Seata 设计通过事务协调器维护的全局写排它锁，来保证事务间的写隔离
                            2。而读写隔离级别则默认为未提交读的隔离级别。

43 | 如何使用缓存优化系统性能？
    背景：
        缓存是我们提高系统性能的一项必不可少的技术，无论是前端、还是后端，都应用到了缓存技术。
        前端：
            使用缓存，可以降低多次请求服务的压力
        后端：
            使用缓存，可以降低数据库操作的压力，提升读取数据的性能。
    前端缓存技术：
        作用：
            可以缓解服务端的压力，减少带宽的占用，同时也可以提升前端的查询性能
        1。本地缓存：
            概念：
                1。浏览器常用的一种缓存就是这种基于 304 响应状态实现的本地缓存了
                2。通常这种缓存被称为协商缓存。
                协商缓存：
                    概念：
                        顾名思义就是与服务端协商之后，通过协商结果来判断是否使用本地缓存。
                    实现方式
                        实现方式一：
                            可以基于请求头部中的 If-Modified-Since 字段与返回头部中的 Last-Modified 字段实现
                            特点：
                                基于时间实现的
                        实现方式二：
                            也可以基于请求头部中的 If-None-Match 字段与返回头部中的 ETag 字段来实现。
                            特点：
                                基于一个唯一标识实现的
                                优点：
                                    更加准确地判断文件内容是否被修改，避免由于时间篡改导致的不可靠问题
                    协商缓存实现流程：
                        第一步：
                            1。当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时
                            2。在 Response 头部加上 ETag 唯一标识，这个唯一标识的值是根据当前请求的资源生成的；
                        第二步：
                            1。当浏览器再次请求访问服务器中的该资源时，会在 Request 头部加上 If-None-Match 字段
                            2。该字段的值就是 Response 头部加上 ETag 唯一标识；
                        第三步：
                            1。服务器再次收到请求后，会根据请求中的 If-None-Match 值与当前请求的资源生成的唯一标识进行比较
                            2。如果值相等，则返回 304 Not Modified
                            3。如果不相等，则在 Response 头部加上新的 ETag 唯一标识，并返回资源；
                        第四步：
                            如果浏览器收到 304 的请求响应状态码，则会从本地缓存中加载资源，否则更新资源。

                强缓存
                    概念：
                        只要判断缓存没有过期，则直接使用浏览器的本地缓存。
                    原理：
                        1。利用 Expires 或者 Cache-Control 这两个 HTTP Response Header 实现的
                        2。它们都用来表示资源在客户端缓存的有效期。
                    Expires：
                        是一个绝对时间
                        缺点：
                            基于 Expires 实现的强缓存也会因为时间问题导致缓存管理出现问题
                    Cache-Control：
                        1。是一个相对时间，即一个过期时间大小，与协商缓存一样
                        2。建议使用 Cache-Control 来实现强缓存，具体流程如下：
                    第一步：
                        1。当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时
                        2。在 Response 头部加上 Cache-Control，Cache-Control 中设置了过期时间大小；
                    第二步：
                        1。浏览器再次请求访问服务器中的该资源时，会先通过请求资源的时间与 Cache-Control 中设置的过期时间大小
                        2。来计算出该资源是否过期，如果没有，则使用该缓存，否则请求服务器；
                    第三步：
                        服务器再次收到请求后，会再次更新 Response 头部的 Cache-Control。
        2。网关缓存
            概念：
                还可以在网关中设置缓存，也就是我们熟悉的 CDN
            原理：
                1。CDN 缓存是通过不同地点的缓存节点缓存资源副本，当用户访问相应的资源时
                2。会调用最近的 CDN 节点返回请求资源
            应用
                视频资源的缓存。

    服务层缓存技术
        背景：
            1。前端缓存一般用于缓存一些不常修改的常量数据或一些资源文件
            2。大部分接口请求的数据都缓存在了服务端，方便统一管理缓存数据。
        目的：
            为了提升系统性能
            例子：
                1。数据库由于并发查询压力过大，可以使用缓存减轻数据库压力
                2。在后台管理中的一些报表计算类数据，每次请求都需要大量计算，消耗系统 CPU 资源，我们可以使用缓存来保存计算结果。
        分类：
            1。进程缓存：
                概念：
                    在 Java 中进程缓存就是 JVM 实现的缓存
                例子：
                    经常使用的容器类，ArrayList、ConcurrentHashMap 等
                优点：
                    数据的存取会更加高效
                缺点：
                    JVM 的堆内存数量是有限的，且在分布式环境下很难同步各个服务间的缓存更新
                应用：
                    一般缓存一些数据量不大、更新频率较低的数据
                实现方式：
                    // 静态常量
                    public final staticS String url = "https://time.geekbang.org";
                    //list 容器
                    public static List<String> cacheList = new Vector<String>();
                    //map 容器   
                    private static final Map<String, Object> cacheMap= new ConcurrentHashMap<String, Object>();
                扩展：
                    1。除了 Java 自带的容器可以实现进程缓存
                    2。我们还可以基于 Google 实现的一套内存缓存组件 Guava Cache 来实现。
                    Guava Cache ：
                        应用：
                            适用于高并发的多线程缓存，它和 ConcurrentHashMap 一样，都是基于分段锁实现的并发缓存。
                        特点：
                            同时也实现了数据淘汰机制
                            例子
                                当我们设置了缓存的最大值后，当存储的数据超过了最大值时，它就会使用 LRU 算法淘汰数据
                            代码实现：
                                public class GuavaCacheDemo {
                                    public static void main(String[] args) {
                                        Cache<String,String> cache = CacheBuilder.newBuilder()
                                            .maximumSize(2)
                                            .build();
                                        cache.put("key1","value1");
                                        cache.put("key2","value2");
                                        cache.put("key3","value3");
                                        System.out.println(" 第一个值：" + cache.getIfPresent("key1"));
                                        System.out.println(" 第二个值：" + cache.getIfPresent("key2"));
                                        System.out.println(" 第三个值：" + cache.getIfPresent("key3"));
                                    }
                                }
                问题：
                    1。如果我们的数据量比较大，且数据更新频繁，又是在分布式部署的情况下
                    2。想要使用 JVM 堆内存作为缓存，这时我们又该如何去实现呢？
                    答案：
                        1。Ehcache 是一个不错的选择，Ehcache 经常在 Hibernate 中出现，主要用来缓存查询数据结果
                        2。Ehcache 是 Apache 开源的一套缓存管理类库，是基于 JVM 堆内存实现的缓存
                        3。同时具备多种缓存失效策略，支持磁盘持久化以及分布式缓存机制。

            2。分布式缓存：
                背景：
                    1。由于高并发对数据一致性的要求比较严格，我一般不建议使用 Ehcache 缓存有一致性要求的数据。
                    2。对于分布式缓存，我们建议使用 Redis 来实现
                        原因：
                            1。Redis 相当于一个内存数据库，由于是纯内存操作
                            2。又是基于单线程串行实现，查询性能极高，读速度超过了 10W 次 / 秒。
                            3。还支持不同类型的数据结构
                                常见的有 string、list、set、hash 等
                            4。还支持数据淘汰策略、数据持久化以及事务等。
        数据库与缓存数据一致性问题
            查询数据时：
                1。我们会先读取缓存，如果缓存中没有该数据，则会去数据库中查询
                2。之后再放入到缓存中。
            修改或删除数据时：
                1。当我们的数据被缓存之后，一旦数据被修改（修改时也是删除缓存中的数据）或删除
                2。我们就需要同时操作缓存和数据库。这时，就会存在一个数据不一致的问题。
            例子：(针对的是删除操作，并发系统40问14讲缓存的使用姿势（二）：缓存如何做到高可用？针对的是更新操作)
                先删除缓存，再删除数据库方式
                    并发条件下：
                        请求一：
                            1。当 A 操作使得数据发生删除变更，那么该操作会先删除缓存中的数据
                            2。之后再去删除数据库中的数据，此时若是还没有删除成功
                        请求二：
                            1。另外一个请求查询操作 B 进来了，发现缓存中已经没有了数据，
                            2。则会去数据库中查询，此时发现有数据
                            3。B 操作获取之后又将数据存放在了缓存中，随后数据库的数据又被删除了
                        现象：
                            此时就出现了数据不一致的情况。
                如果先删除数据库，再删除缓存呢？
                    并发条件下：
                        请求一：
                            1。当 A 操作使得数据发生删除变更，那么该操作会先删除了数据库的操作
                            2。接下来删除缓存，失败了，那么缓存中的数据没有被删除
                            3。而数据库的数据已经被删除了，同样会存在数据不一致的问题。
                问题：
                    又该如何避免高并发下，数据更新删除操作所带来的数据不一致的问题呢？
                    解决方案：
                       如果我们需要使用一个线程安全队列来缓存更新或删除的数据
                        请求一
                            1。当 A 操作变更数据时，会先删除一个缓存数据
                            2。此时通过线程安全的方式将缓存数据放入到队列中，并通过一个线程进行数据库的数据删除操作。
                        请求二：
                            1。当有另一个查询请求 B 进来时，如果发现缓存中没有该值，则会先去队列中查看该数据是否正在被更新或删除
                            2。如果队列中有该数据，则阻塞等待，直到 A 操作数据库成功之后，唤醒该阻塞线程，再去数据库中查询该数据。
                    缺点：
                        可能存在读请求被长时间阻塞，高并发时低吞吐量等问题
            注意：
                在考虑缓存时，如果数据更新比较频繁且对数据有一定的一致性要求，我通常不建议使用缓存。

        缓存穿透（参考参考14，15 | 缓存的使用姿势（二）：缓存如何做到高可用？）
            概念：
                指大量查询没有命中缓存，直接去到数据库中查询，如果查询量比较大，会导致数据库的查询流量大，对数据库造成压力。
            两种解决方案：
                一种：
                    1。将第一次查询的空值缓存起来
                    2。同时设置一个比较短的过期时间。
                    缺点：
                        存在一个安全漏洞，就是当黑客利用大量没有缓存的 key 攻击系统时，缓存的内存会被占满溢出。
                一种：
                    布隆过滤算法（BloomFilter）
            流程：
                1。该算法可以用于检查一个元素是否存在，返回结果有两种：可能存在或一定不存在。
                2。在最初缓存数据时也将 key 值缓存在布隆过滤器的 BitArray 中
                3。当有 key 值查询时，对于一定不存在的 key 值，我们可以直接返回空值
                4。对于可能存在的 key 值，我们会去缓存中查询，如果没有值，再去数据库中查询。
            场景：
                很适合用来解决故意攻击系统的缓存穿透问题
            实现原理：
                1。BloomFilter 的实现原理与 Redis 中的 BitMap 类似
                2。首先初始化一个 m 长度的数组，并且每个 bit 初始化值都是 0
                3。当插入一个元素时，会使用 n 个 hash 函数来计算出 n 个不同的值
                4。数来计算出 n 个不同的值，
                例子：
                    1。假设我们插入两个 key 值分别为 20,28 的元素
                    2。通过两次哈希函数取模后的值分别为 4,9 以及 14,19，因此 4,9 以及 14,19 都被设置为 1。
            问题：
                那为什么说 BloomFilter 返回的结果是可能存在和一定不存在呢？
                例子：
                    1。假设我们查找一个元素 25，通过 n 次哈希函数取模后的值为 1,9,14，此时在 BitArray 中肯定是不存在的
                    2。而当我们查找一个元素 21 的时候，n 次哈希函数取模后的值为 9,14，此时会返回可能存在的结果，但实际上是不存在的。
            注意：
                BloomFilter 是不允许删除任何元素的
                原因：
                    1。这样会导致已经删除的元素依然返回可能存在的结果
                    2。也会影响 BloomFilter 判断的准确率
                解决办法：
                    则是重建一个 BitArray。
        缓存击穿：
            概念：
                1。在高并发情况下，同时查询一个 key 时，key 值由于某种原因突然失效
                2。设置过期时间或缓存服务宕机），就会导致同一时间，这些请求都去查询数据库了
            场景：
                经常出现在查询热点数据的场景中
            方法：
                在查询数据库时，使用排斥锁来实现有序地请求数据库，减少数据库的并发压力。
        缓存雪崩
            概念：
                1。缓存雪崩则与缓存击穿差不多，区别就是失效缓存的规模
                2。雪崩一般是指发生大规模的缓存失效情况
            例子：
                对于大量缓存的过期时间同一时间过期的问题，我们可以采用分散过期时间来解决
                问题一：
                    大量缓存的过期时间同一时间过期的问题
                    方案：
                        采用分散过期时间来解决
                问题二：
                    针对缓存服务宕机的情况
                方案：
                    我们可以采用分布式集群来实现缓存服务。
44 | 记一次双十一抢购性能瓶颈调优










加餐 | 什么是数据的强、弱一致性？